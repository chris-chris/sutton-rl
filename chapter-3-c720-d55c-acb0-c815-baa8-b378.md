제 3 장

  


유한 마르코프 결정 과정

  


이 장에서는 나머지 책에서 우리가 해결하려고하는 문제를 소개합니다. 이 문제는 강화학습 분야를 정의하는 것으로 간주 될 수 있습니다.이 문제를 해결하는 데 적합한 방법으로 강화학습 방법으로 간주합니다.

  


이 장에서 우리의 목표는 강화학습 문제를 넓은 의미에서 기술하는 것이다. 우리는 강화 학습 과제로 구성 될 수있는 광범위한 응용 프로그램을 전달하려고합니다. 정확한 이론적 진술을 할 수있는 수학적으로 이상화 된 강화 학습 문제에 대해서도 설명합니다. 우리는 가치 함수 및 벨만 방정식과 같은 문제의 수학적 구조의 주요 요소를 소개합니다. 모든 인공 지능과 마찬가지로 적용 범위와 수학적 편의성 사이에는 긴장감이 있습니다. 이 장에서 우리는이 긴장을 소개하고 이것이 의미하는 장단점과 문제점을 논의합니다.

  


3.1 에이전트 - 환경 인터페이스

  


강화학습 문제는 상호 작용에서 학습 목표를 달성하기위한 문제의 직접적인 골격을 의미합니다. 학습자와 의사 결정자는 에이전트라고합니다. 에이전트와 상호 작용하며 에이전트 외부의 모든 것을 포함하는 것을 환경이라고합니다. 에이전트는 행동과 환경을 선택하여 해당 행동에 응답하고 새로운 상황을 에이전트에 제공합니다 .1 환경은 또한 상담원이 시간이 지남에 따라 최대화하려고 시도하는 특별한 수치를 보상합니다. 보상이 결정되는 방법을 포함하여 환경에 대한 완전한 명세는 강화학습 문제의 한 사례 인 과제를 정의합니다.

  


1 엔지니어의 용어 컨트롤러, 제어 시스템 \(또는 플랜트\) 및 제어 신호 대신 에이전트, 환경 및 조치라는 용어는 더 많은 청중에게 의미가 있기 때문에 사용합니다.

  


  


48

  


제 3 장 유한 마르코프 결정 프로세스

  


상태

  


세인트

  


보상

  


있는 걸의

  


행동

  


에서

  


에이전트

  


환경

  


강화 학습의 에이전트 환경의 상호 작용 그림 3.1.

  


보다 구체적으로, 에이전트 및 환경은 t = 0, 1, 2, 3 ,. . 각 시간 단계 t에서, 에이전트는 환경 상태의 표현 인 St∈ S를 수신하고, 여기서 S는 가능한 상태의 집합이며, 그 근거에 따라 동작을 선택한다. A \(St\), A \( St\)는 상태 St.에서 한 번에 사용할 수있는 일련의 행동으로, 행동의 결과로 부분적으로 에이전트는 숫자 보상 Rt + 1 ∈ R ⊂ R을 받고 새로운 상태 St +1.3 그림 3.1은 에이전트 - 환경 상호 작용을 도식화 한 것이다.

  


각 시간 단계에서 에이전트는 가능한 모든 동작을 선택하는 상태에서 상태로 매핑을 구현합니다. 이 매핑은 에이전트의 정책이라고 불리며 πt로 표시됩니다. 여기서 πt \(a \| s\)는 St = s 인 경우 At = a의 확률입니다. 강화 학습 방법은 에이전트가 경험의 결과로 정책을 변경하는 방법을 지정합니다. 에이전트의 목표는 대략 말하면 장기적으로받는 보상의 총액을 최대화하는 것입니다.

  


이 프레임 워크는 추상적이고 유연하며 여러 가지 다른 여러 가지 문제로 여러 가지 방식으로 적용 할 수 있습니다. 예를 들어, 시간 단계는 실시간의 고정 간격을 참조하지 않아도됩니다. 그들은 의사 결정과 행동의 임의의 연속적인 단계들을 지칭 할 수있다. 동작은 로봇 팔의 모터에 적용되는 전압과 같은 저수준 제어 또는 점심 식사를 할 것인지 또는 대학원에 진학 할 것인지와 같은 높은 수준의 결정 일 수 있습니다. 마찬가지로, 주에서는 다양한 형태를 취할 수 있습니다. 이들은 직접 센서 판독 값과 같은 저레벨의 감각에 의해 완전히 결정될 수 있거나, 방의 대상에 대한 상징적 인 설명과 같이보다 고차원 적이거나 추상적 일 수 있습니다. 국가를 구성하는 요소 중 일부는 과거의 감각을 기억하거나 전적으로 정신적 또는 주관적 일 수 있습니다. 예를 들어, 에이전트는 객체가 어디에 있는지, 또는 명확하게 정의 된 의미에서 놀랐다는 것을 확신하지 못하는 상태 일 수 있습니다. 마찬가지로 일부 행동은 전적으로 정신적이거나 계산적 일 수 있습니다. 예를 들어 어떤 행동은 상담원이 생각하는 것을 제어하거나 관심을 집중시키는 부분을 제어 할 수 있습니다. 일반적으로 행동은 우리가 만드는 법을 배우고 자하는 모든 결정이 될 수 있으며, 주정부는 그 결정을 내리는 데 유용 할 수있는 모든 것이 될 수 있습니다.

  


2 아이디어의 많은 부분이 연속 시간의 경우까지 확장 될 수 있음에도 불구하고 가능한 한 단순하게 유지하기 위해 이산 시간에주의를 제한한다 \(Bertsekas and Tsitsiklis, 1996; Werbos, 1992; Doya, 1996\).

  


3 Rt 대신에 Rt + 1을 사용하여 At로 인한 보상을 나타냅니다. 왜냐하면 다음 보상과 다음 상태, Rt + 1과 St + 1이 공동으로 결정된다는 것을 강조하기 때문입니다. 불행히도 두 관례는 문헌에서 널리 사용됩니다.

  


Rt + 1 St + 1

  


  


3.1. 대리인 환경 인터페이스 49

  


특히, 에이전트와 환경 간의 경계는 로봇이나 동물의 신체의 물리적 경계와 종종 동일하지 않습니다. 일반적으로 경계는 에이전트보다 더 가깝게 그려집니다. 예를 들어, 로봇과 그 감지 하드웨어의 모터와 기계적 연결은 대개 에이전트의 일부가 아닌 환경의 일부로 간주되어야합니다. 마찬가지로 사람이나 동물에게 프레임 워크를 적용하면 근육, 골격 및 감각 기관을 환경의 일부로 간주해야합니다. 리워드 또한 아마도 자연 및 인공 학습 시스템의 물리적 본문 내부에서 계산되지만 에이전트의 외부로 간주됩니다.

  


우리가 따라야 할 일반적인 규칙은 에이전트가 임의로 변경할 수없는 모든 것이 에이전트 외부에서, 따라서 환경의 일부로 간주된다는 것입니다. 우리는 환경의 모든 것이 에이전트에게 알려지지 않았다고 가정하지 않습니다. 예를 들어, 상담원은 자신의 보상 및 조치 상태에 따라 보상이 계산되는 방법에 대해 알고 있습니다. 그러나 우리는 보상 계산이 상담원이 직면하는 업무를 정의하고 따라서 임의로 변경할 수있는 능력 이상이어야하므로 항상 상담원 외부에있는 보상 계산을 고려합니다. 사실, 어떤 경우에는 에이전트가 환경이 어떻게 작동하는지에 대한 모든 것을 알고있을 수도 있고 루빅 큐브와 같은 퍼즐이 어떻게 작동하는지 정확히 알 수있을 것입니다. 그러나 여전히 해결할 수는 없습니다.

  


에이전트 - 환경 경계는 다른 목적을 위해 다른 장소에 위치 할 수 있습니다. 복잡한 로봇에서 많은 다른 에이전트가 한 번에 작동 할 수 있으며 각 에이전트는 자체 경계를 가지고 있습니다. 예를 들어, 한 대리인은 고위 결정을 이행하는 하위 요원이 직면 한 국가의 일부를 구성하는 고위 결정을 내릴 수 있습니다. 실제로, 에이전트 - 환경 경계는 특정 상태, 동작 및 보상을 선택하고 결정되므로 관심있는 특정 의사 결정 작업을 식별합니다.

  


강화 학습 프레임 워크는 상호 작용에서 목표 지향 학습의 문제를 상당히 추상화 한 것입니다. 그것은 감각, 기억, 제어 장치의 세부 사항이 무엇이든, 목표를 달성하려고 시도하는 것이 무엇이든 관계없이 목표 지향적 행동을 학습하는 모든 문제는 에이전트와 그 환경 사이를 오가는 세 신호로 줄일 수 있다고 제안합니다. 상담원이 선택한 선택을 나타내는 신호 \(동작\), 선택이 이루어진 기준을 나타내는 신호 \(상태\) 및 상담원의 목표 \(보상\)를 정의하는 신호. 이 틀은 모든 의사 결정 문제를 유용하게 표현하기에 충분하지 않을 수도 있지만, 널리 유용하고 적용 가능하다는 것이 입증되었습니다.

  


물론 특정 상태와 동작은 작업마다 크게 다르며 표현 방법이 성능에 크게 영향을 줄 수 있습니다. 강화학습에서 다른 학습과 마찬가지로 그러한 표현 선택은 현재 과학보다 예술이다. 이 책에서 우리는 주와 행동을 표현하는 좋은 방법에 관한 몇 가지 조언과 사례를 제시하지만, 주된 초점은 표현이 선택되면 어떻게 행동해야하는지 배우는 일반적인 원칙에 있습니다.

  


예제 3.1 : 생물 반응기 \(Bioreactor\) 생물 반응기의 순간 온도와 교반 속도를 결정하기 위해 강화학습이 적용된다고 가정 해 보자 \(

  


50 장 3. 최종 마르코프 결정 과정

  


유용한 화학 물질을 생산하는 데 사용되는 영양소와 박테리아의 큰 통\). 이러한 적용에서의 동작은 목표 온도를 달성하기 위해 가열 요소 및 모터를 직접 활성화하는 하위 수준 제어 시스템으로 전달되는 목표 온도 및 목표 교반 속도 일 수 있습니다. 미국은 열전대와 다른 감각 판독 값, 아마도 필터링되고 지연된 것, 그리고 부가 가치세 및 대상 화학 물질의 성분을 나타내는 기호 입력이 될 것입니다. 보상은 유용한 화학 물질이 생물 반응기에 의해 생성되는 속도의 순간적인 측정 일 수 있습니다. 여기서 각 상태는 센서 판독 값 및 기호 입력의 목록 또는 벡터이며 각 동작은 목표 온도 및 교반 속도로 구성된 벡터입니다. 그러한 구조화 된 표현으로 주와 행동을 취하는 것이 강화학습 과제의 전형입니다. 반면에 보상은 항상 단일 숫자입니다.

  


Example 3.2 : Pick-and-Place 로봇 반복적 인 pick-and-place 작업에서 강화학습을 사용하여 로봇 팔의 동작을 제어하는 ​​것을 고려하십시오. 빠르고 부드러운 움직임을 배우고 싶다면 학습 에이전트는 모터를 직접 제어하고 기계 링크의 현재 위치와 속도에 대한 대기 시간이 짧은 정보를 가지고 있어야합니다. 이 경우의 동작은 각 관절에서 각 모터에 적용되는 전압 일 수 있으며, 상태는 관절 각 및 속도의 최신 판독 값 일 수 있습니다. 성공적으로 선택되어 배치 된 각 오브젝트에 대해 보상이 +1이 될 수 있습니다. 매끄러운 움직임을 장려하기 위해 각 시간 단계마다 작고 부정적인 보상이 동작의 순간 순간적 움직임에 따라 제공 될 수 있습니다.

  


Example 3.3 : 재활용 로봇 모바일 로봇은 사무실 환경에서 빈 탄산 음료 캔 수집 작업을 수행합니다. 캔을 감지 할 수있는 센서와 캔을 집어 내고 선반에 넣을 수있는 팔과 그리퍼; 충전 용 배터리로 작동합니다. 로봇의 제어 시스템에는 감각 정보를 해석하고, 탐색하고, 팔과 그리퍼를 제어하는 ​​구성 요소가 있습니다. 캔을 검색하는 방법에 대한 높은 수준의 결정은 배터리의 현재 충전 수준을 기반으로 한 강화 학습 에이전트에 의해 결정됩니다. 이 에이전트는 로봇이 \(1\) 특정 시간 동안 적극적으로 캔을 검색해야하는지, \(2\) 정지 상태를 유지하고 다른 사람이 캔을 가져 오기를 기다릴 지 또는 \(3\) 배터리를 충전하십시오. 이 결정은 정기적으로 또는 특정 사건이 발생할 때마다 이루어져야하며, 빈 캔을 찾는 것과 같은 따라서 에이전트에는 세 가지 조치가 있으며 상태는 배터리의 상태에 따라 결정됩니다. 보상은 대부분 0 일 수 있지만 로봇이 빈 캔을 확보하면 양수가되고, 배터리가 완전히 떨어지면 크고 부정적이됩니다. 이 예에서 강화학습 에이전트는 전체 로봇이 아닙니다. 모니터링 상태는 로봇의 외부 환경 조건이 아니라 로봇 자체의 상태를 나타냅니다. 에이전트의 환경에는 로봇의 외부 환경뿐만 아니라 다른 복잡한 의사 결정 시스템이 포함될 수있는 나머지 로봇도 포함됩니다. 로봇이 빈 캔을 확보하면 양수가되고, 배터리가 완전히 떨어지면 크고 부정적이됩니다. 이 예에서 강화학습 에이전트는 전체 로봇이 아닙니다. 모니터링 상태는 로봇의 외부 환경 조건이 아니라 로봇 자체의 상태를 나타냅니다. 에이전트의 환경에는 로봇의 외부 환경뿐만 아니라 다른 복잡한 의사 결정 시스템이 포함될 수있는 나머지 로봇도 포함됩니다. 로봇이 빈 캔을 확보하면 양수가되고, 배터리가 완전히 떨어지면 크고 부정적이됩니다. 이 예에서 강화학습 에이전트는 전체 로봇이 아닙니다. 모니터링 상태는 로봇의 외부 환경 조건이 아니라 로봇 자체의 상태를 나타냅니다. 에이전트의 환경에는 로봇의 외부 환경뿐만 아니라 다른 복잡한 의사 결정 시스템이 포함될 수있는 나머지 로봇도 포함됩니다.

  


연습 3.1 강화학습 프레임 워크에 맞는 자신의 세 가지 예제 작업을 작성하여 각 주, 동작 및 보상을 식별하십시오. 가능한 세 가지 예를 서로 다른 것으로 만드십시오. 프레임 워크는 추상

  


  


3.2입니다. 목표와 보상 51. 융통성 있고 다양한 방법으로 적용될 수 있습니다.

  


적어도 하나의 예제 에서 어떤 식 으로든 한계를 확장 하십시오.

  


연습 3.2

  


모든 목표 지향 학습 과제 를 유용하게 표현하기 위해 강화 학습 프레임 워크가 적절 합니까? 명확한 예외를 생각해 볼 수 있습니까?

  


운동 3.3 운전의 문제를 고려하십시오. 운전자는 가속기, 스티어링 휠 및 브레이크, 즉 신체가 기계와 만나는 지점의 동작을 정의 할 수 있습니다. 또는 고무가 도로를 만나는 곳에서 타이어 토크라고 생각하면 더 멀리 말할 수 있습니다. 또는 당신의 두뇌가 당신의 몸과 만나는 곳에, 당신의 팔다리를 제어하기 위해 근육이 꼬이는 행동을 더 멀리 정의 할 수 있습니다. 아니면 당신이 정말로 높은 수준으로 가서 당신의 행동이 당신이 운전할 곳을 선택할 수 있다고 말할 수 있습니다. 에이전트와 환경 사이에 선을 그릴 수있는 적합한 수준은 무엇입니까? 라인의 한 위치가 다른 라인보다 선호되는 기준은 무엇입니까? 한 위치를 다른 위치보다 선호하는 근본적인 이유가 있습니까? 아니면 자유로운 선택입니까?

  


3.2 목표 및 보상

  


강화 학습에서 에이전트의 목적 또는 목표는 환경에서 에이전트로 전달되는 특수 보상 신호의 측면에서 공식화됩니다. 매 시간 단계에서 보상은 Rt ∈ R이라는 단순한 수입니다. 비공식적으로 상담원의 목표는받는 보상의 총액을 최대화하는 것입니다. 이것은 즉각적인 보상이 아니라 장기적으로 누적 된 보상을 최대화하는 것을 의미합니다. 우리는이 비공식적 인 생각을 보상 가설로 분명히

  


말할 수 있습니다. 목표와 목적에 의해 의미하는 것이 모두 수신 된 스칼라 신호의 누적 합계 \(보상이라고 함\)의 기대 값을 최대화하는 것으로 생각할 수 있습니다.

  


목표에 대한 아이디어를 공식화하기위한 보상 신호의 사용은 강화학습의 가장 두드러진 특징 중 하나입니다.

  


보상 신호 측면에서 목표를 공식화하는 것은 처음에는 제한적으로 보일 수 있지만 실제로는 유연하고 폭넓게 적용될 수 있음이 입증되었습니다. 이를 확인하는 가장 좋은 방법은 그것이 어떻게 사용되었거나 사용될 수 있는지에 대한 예를 고려하는 것입니다. 예를 들어, 로봇이 걷는 것을 배우도록하기 위해 연구원은 로봇의 전방 동작에 비례하여 각 시간 단계에 대한 보상을 제공했습니다. 로봇이 미로에서 벗어나는 법을 배우게 할 때, 탈출하기 전에 지나가는 모든 시간 단계에 대해 보상은 종종 -1이됩니다. 이것은 에이전트가 가능한 한 빨리 탈출하도록 장려합니다. 로봇이 재활용을 위해 빈 탄산 음료 캔을 찾아서 수집하는 것을 배우려면 대부분의 시간에 0의 보상을 줄 수 있으며 각각에 대해 +1의 보상을 수집 할 수 있습니다. 사람이 물건에 부딪 힐 때 또는 누군가가 그것에 소리를 지르면 부정적인 보상을주기를 원할 수도 있습니다.

  


이 모든 예에서 어떤 일이 일어나고 있는지 확인할 수 있습니다. 에이전트는 항상 보상을 최대화하는 법을 배웁니다. 우리가 그것이 우리를 위해 무엇인가를하기를 원한다면, 우리는 다음을 제공해야한다.

  


제 3 장 FINITE MARKOV DECISION PROCESSES

  


그것을 극대화 할 때 에이전트가 우리의 목표를 달성 할 수있는 방식으로 보상을 제공합니다. 따라서 우리가 설정 한 보상이 우리가 성취하고자하는 것을 분명히 나타내는 것이 중요합니다. 특히, 보상 신호는 우리가 원하는 것을 달성하는 방법에 대한 사전 지식을 에이전트에게 전할 수있는 장소가 아닙니다 .4 예를 들어, 체스 게임 에이전트는 실제로 이기기 위해서만 보상을받으며, 부고를 달성하기 위해 보상해서는 안됩니다 적의 조각을 가져 가거나 이사회의 중심을 장악하는 것으로. 이러한 종류의 부 목적을 달성하면 보상을 받으면 에이전트는 실제 목표를 달성하지 않고이를 달성 할 수있는 방법을 찾을 수 있습니다. 예를 들어, 게임을 잃어 버릴지라도 상대방의 조각을 가져갈 수있는 방법을 찾을 수 있습니다. 보상 신호는 로봇이 달성하고자하는 바를 로봇에 전달하는 방법이며, 원하는대로 달성 할 방법이 아닙니다.

  


강화학습에 대한 새 소식은 때로는 학습 목표를 정의하는 보상이 상담원이 아닌 환경에서 계산된다는 사실에 놀라는 경우가 있습니다. 물론 동물의 궁극적 인 목표는 음식, 기아, 고통, 즐거움을 인식하는 센서와 같이 신체 내부에서 발생하는 계산에 의해 인식됩니다. 그럼에도 불구하고 이전 섹션에서 논의했듯이 본문 부분이 에이전트 외부 \(즉 에이전트 환경의 일부\)로 간주되는 방식으로 에이전트 환경 인터페이스를 다시 그릴 수 있습니다. 예를 들어 목표가 로봇의 내부 에너지 저장고와 관련된 경우 환경의 일부로 간주됩니다. 로봇의 팔다리의 위치와 관련된 목표는 환경의 일부로 간주됩니다. 즉, 에이전트의 경계는 팔다리와 그들의 제어 시스템 사이의 경계면에 그려진다. 이러한 것들은 로봇 내부이지만 학습 에이전트 외부에있는 것으로 간주됩니다. 우리의 목적을 위해 학습 에이전트의 경계를 물리적 인 신체의 한계가 아닌 통제의 한계에 두는 것이 편리합니다.

  


우리가하는 이유는 에이전트의 궁극적 인 목표가 불완전한 통제를해야한다는 것입니다. 예를 들어, 보상이 임의로 행동을 바꿀 수있는 것과 같은 방식으로 접수되었다고 단순히 명령 할 수는 없습니다 . 따라서 우리는 상담원 외부에서 보상 소스를 배치합니다. 이것은 에이전트가 내부 보상의 종류 또는 일련의 내부 보상을 정의하는 것을 배제하지 않습니다. 사실, 이것은 정확히 많은 강화학습 방법이하는 것입니다.

  


3.3 반환

  


지금까지 우리는 비공식적으로 배우는 목적을 논의했습니다. 에이전트의 목표는 장기적으로 누적 된 보상을 최대화하는 것입니다. 이것이 형식적으로 어떻게 정의 될 수 있습니까? 시간 단계 t 이후에 수신 된 보상 시퀀스가 ​​Rt + 1, Rt + 2, Rt + 3, ...이라고 표시되면이 시퀀스의 정확한 측면을 극대화하기를 원합니까? 일반적으로, 우리는 기대 수익을 극대화하려고 노력한다. 여기서 Gt는 보상 시퀀스의 특정 함수로 정의된다.

  


이러한 종류의 사전 지식을 전달하기위한 가장 단순한 장소는 초기 정책 또는 가치 기능 또는 이것들에 대한 영향입니다. Lin \(1992\), Maclin and Shavlik \(1994\), Clouse \(1996\) 참조.

  


  


3.3. RETURNS 53

  


반환 값은 보상의 합계입니다 :

  


Gt =. Rt + 1 + Rt + 2 + Rt + 3 + ... + RT, \(3.1\)

  


여기서 T는 마지막 시간 단계입니다. 이 접근법은 마지막 시간 단계의 자연스러운 개념, 즉 에이전트 - 환경 상호 작용이 자연스럽게 게임의 연극, 미로를 통한 여행 등과 같은 에피소드라고 부르는 하위 ​​시퀀스로 분리되는 자연스러운 개념이있는 응용 프로그램에서 의미가 있습니다. 또는 반복적 인 상호 작용의 어떤 종류. 각 에피소드는 터미널 상태라고하는 특수한 상태로 끝나고 표준 시작 상태로 재설정되거나 시작 상태의 표준 배포에서 샘플로 리셋됩니다. 에피소드가 게임 우승 및 패배와 같이 다른 방식으로 끝나는 것으로 생각하는 경우에도 다음 에피소드는 이전 이벤트가 종료 된 방법과는 별도로 시작됩니다. 따라서 에피소드는 모두 동일한 터미널 상태에서 끝나는 것으로 간주 될 수 있으며 다른 결과에 대해 다른 보상을 제공합니다. 이러한 종류의 에피소드가있는 작업을 일회성 작업이라고합니다.

  


다른 한편, 많은 경우에있어 에이전트 - 환경 상호 작용은 자연적으로 식별 가능한 에피소드로 분리되지 않지만 계속 제한없이 계속됩니다. 예를 들어, 이것은 긴 수명을 가진 로봇에 대한 지속적인 프로세스 제어 작업 또는 응용 프로그램을 공식화하는 자연스러운 방법입니다. 우리는 이러한 지속적인 과제를 부릅니다. 최종 타임 스텝이 T = ∞ 일 것이고, 우리가 최대화하려고 시도하는 수익률 자체가 쉽게 무한 할 수 있기 때문에 수익률 공식 \(3.1\)은 계속 작업에 문제가됩니다. 예를 들어, 상담원이 매 시간 단계마다 +1의 보상을받는다고 가정합니다. 따라서이 책에서 개념적으로 약간 더 복잡하지만 수학적으로 훨씬 단순한 수익의 정의를 사용합니다.

  


우리가 필요로하는 추가 개념은 할인의 개념입니다. 이 접근법에 따르면, 에이전트는 미래에 받게되는 할인 보상의 합이 최대가되도록 조치를 선택하려고합니다. 특히, 예상 할인 수익을 극대화하기 위해 At를 선택합니다 :

  


Gt =.

  


여기서 γ는 0 ≤ γ ≤ 1 인 할인율이라 불리는 변수이다. Rt + 1 + γRt + 2 + γ2Rt + 3 + ... = ∞ γkRt + k + 1 .

  


할인율은 미래의 보상의 현재 가치를 결정합니다 : 미래의 k 시간 단계를받은 보상은 즉시받은 경우 가치가있는 것만 큼 γk-1 배 가치가 있습니다. γ 

&lt;

1 인 경우 보상 시퀀스 {Rk}가 한정되어있는 한 무한 합계는 유한 값을가집니다. γ = 0 인 경우 에이전트는 즉각적인 보상을 극대화하는 데에만 관심이 있다는 점에서 근시입니다.이 경우의 목표는 At를 선택하여 Rt + 1 만 최대화하는 방법을 배우는 것입니다. 에이전트의 각각의 행동이 미래의 보상이 아닌 즉각적인 보상에만 영향을 미쳤다면, 근시 환자는 각 즉각적인 보상을 개별적으로 최대화함으로써 \(3.2\) 최대화 할 수 있습니다. 그러나 일반적으로 즉각적인 보상을 극대화하기 위해 행동하면 미래의 보상에 대한 접근이 줄어들어 수익이 실제로 감소 될 수 있습니다. γ가 1에 접근함에 따라,

  


5 에피소드는 때때로 문헌에서 

"

시험

"

이라고 불립니다.

  


제 3 장 최종 마르코프 결정 과정

  


그림 3.2 : 극 균형 조정 작업.

  


Example 3.4 : Pole-Balancing 그림 3.2는 강화 학습의 초기 그림을 제공하는 작업을 보여줍니다. 여기에있는 목표는 트랙을 따라 움직이는 카트에 힘을 적용하여 카트가 넘어지는 것을 방지합니다. 극점이 수직에서 주어진 각도를 넘어 서거나 카트가 트랙에서 벗어나면 실패가 발생한다고합니다. 각 실패 후에 기둥이 수직으로 재설정됩니다. 이 작업은 일시적인 것으로 간주 될 수 있습니다. 자연스러운 에피소드는 장대감을 극대화하려는 반복적 인 시도입니다. 이 경우 보상은 실패가 발생하지 않은 모든 시간 단계마다 +1이 될 수 있으므로 매번 반환 할 때까지 실패까지의 단계 수입니다. 또는 폴 밸런싱을 할인을 사용하여 계속적인 작업으로 처리 할 수 ​​있습니다. 이 경우 보상은 각 실패마다 -1이되고 다른 모든 시간은 0이됩니다. 각 시간에서의 복귀는 -γK와 관련 될 것이다. 여기서 K는 실패 이전의 시간 간격의 수이다. 어느 경우 든 극은 가능한 한 오랫동안 균형을 유지하여 수익을 극대화합니다.

  


Exercise 3.4 당신이 일방적 인 작업으로 극 밸런싱을 처리했다고 가정하고, 실패시 -1을 제외한 모든 보상이 0 인 할인을 사용한다고 가정 해보십시오. 그렇다면 매번 돌아 오는 것이 무엇이겠습니까? 이 수익은이 작업의 할인 된 계속 공식에서의 수익과 어떻게 다른가요?

  


연습 3.5 미로를 돌리는 로봇을 설계한다고 상상해보십시오. 당신은 미로에서 벗어날 때 +1의 보상을주고 다른 모든 때는 0의 보상을 주려고합니다. 이 작업은 자연스럽게 에피소드 \(즉, 미로를 통한 연속 실행\)로 분류되어 예상되는 총 보상 \(3.1\)을 최대화하는 일회용 작업으로 처리하기로 결정했습니다. 학습 에이전트를 잠시 실행 한 후에는 미로에서 벗어나는 데 아무런 개선이 없음을 알게됩니다. 무슨 일 이니? 상담원이 원하는 것을 효과적으로 전달 했습니까?

  


3.4 에피소드 및 계속 작업을위한 통합 표기법

  


이전 섹션에서는 두 가지 유형의 강화 학습 학습을 설명했습니다. 하나는 에이전트 - 환경 상호 작용이 자연스럽게 별도의 에피소드 \(일시적인 작업\)로 나누어지고 다른 하나는 실행되지 않는 작업 \(계속 작업\)입니다. 전자의 경우는 수학적으로 더 쉽습니다. 왜냐하면 각 액션은 에피소드 동안 연속적으로받은 보상의 유한 수에만 영향을 미치기 때문입니다. 이 책에서 우리는

  


  


\* 3.5를 고려한다 . 마르코프 재산 55

  


때때로 한 종류의 문제 및 때로는 다른 종류, 그러나 수시로. 따라서 두 가지 사례를 동시에 정확하게 이야기 할 수있는 하나의 표기법을 만드는 것이 유용합니다.

  


에피소드 작업에 대해 정확하게하려면 몇 가지 추가 표기법이 필요합니다. 일련의 긴 시간 간격보다는 시간 간격의 유한 순서로 구성된 일련의 시간을 고려해야합니다. 우리는 0에서 새로 시작하는 각 에피소드의 시간 간격을 지정합니다. 그러므로, 우리는 시간 t에서의 상태 표현 인 St뿐만 아니라 에피소드 i의 시간 t에서의 상태 표현 인 St, i \(그리고 At, i, Rt, i, πt, i, Ti , 등\). 그러나 에피소드 작업에 관해 논의 할 때 거의 다른 에피소드를 구분할 필요가 없다는 것이 밝혀졌습니다. 우리는 거의 항상 특정 단일 에피소드를 고려하거나 모든 에피소드에 대해 진실을 말합니다. 따라서 실제로는 에피소드 번호에 대한 명시적인 참조를 삭제하여 표기법을 약간 남용합니다. 그건,

  


에피소드 및 계속 작업 모두를 다루는 단일 표기법을 얻으려면 다른 협약이 필요합니다. 우리는 하나의 경우 \(3.1\)의 유한 수에 대한 합계와 다른 하나의 무한 수 \(3.2\)에 대한 합계로 반환을 정의했습니다. 이것들은 에피소드 종결을 고려하여 통합 될 수 있으며, 이는 오직 그 자체로 전이하고 0의 보상만을 생성하는 특별한 흡수 상태에 들어가는 것으로 간주됩니다. 예를 들어, 상태 전이 다이어그램

  


여기에서 솔리드 사각형은 에피소드의 끝 부분에 해당하는 특수 흡수 상태를 나타냅니다. S0에서 시작해서, 우리는 보상 순서 +1, + 1, + 1,0,0,0, ...을 얻습니다. 이것들을 요약하면 첫 번째 T 보상 \(여기서 T = 3\) 또는 전체 무한 시퀀스를 통해. 할인을 도입하더라도 사실입니다. 따라서 우리는 에피소드 번호가 필요하지 않을 때 생략하는 관례를 사용하여 \(3.2\)에 따라 반환을 정의 할 수 있으며, 합계가 정의 된 경우 \(예 : 모든 에피소드가 종료되기 때문에\) γ = 1 일 가능성을 포함하여 반환 값을 정의 할 수 있습니다 \). 또는 Google에서 수익을 쓸 수도 있습니다

  


. T -t-

  


1Gt = γkRt + k + 1, \(3.3\)

  


k = 0

  


T = ∞ 또는 γ = 1 \(그러나 둘 다 아닌\) 가능성을 포함하여. 표기법을 간소화하고 에피소드와 계속되는 작업간에 긴밀한 유사점을 표현하기 위해이 책의 나머지 부분에서이 동의어를 사용합니다. \(나중에, 10 장에서 우리는 계속되는 것과 버려지지 않은 것 모두를 공식화 할 것이다.\)

  


S R1 = +1 S R2 = +1 S R3 = +1 R4 = 0 0 1 2 R5 = 0

  


. ..

  


56 제 3 장 최종 마르코프 결정 과정 \* 3.5 마르코프 속성

  


강화학습 프레임 워크에서 에이전트는 환경 상태라고하는 환경의 신호에 따라 의사 결정을 내립니다. 이 섹션에서는 상태 신호에 대해 요구되는 사항과 제공해야 할 것으로 예상되는 것과하지 말아야 할 정보의 종류에 대해 논의합니다. 특히 우리는 Markov 속성이라고 불리는 특정 관심 대상 환경과 그 상태 신호의 속성을 공식적으로 정의합니다.

  


이 책에서 

"

국가

"

란 상담원이 사용할 수있는 정보를 의미합니다. 국가는 명목상 환경의 일부인 전처리 시스템에 의해 주어진다고 가정합니다. 이 책에서 주 신호를 구성, 변경 또는 학습하는 문제는 다루지 않습니다. 우리는 국가 대표가 중요하지 않다고 생각하기 때문에가 아니라 의사 결정 문제에 전적으로 집중하기 위해이 접근법을 취합니다. 다시 말해, 주 신호는 상태 신호를 설계하는 것이 아니라 상태 신호가 무엇이든지간에 어떤 동작을 취할 지 결정하는 것입니다. 관습에 따라 보상 신호는 국가의 일부가 아니지만 그 신호의 사본이 될 수 있습니다.

  


물론 상태 신호는 감각 측정과 같은 즉각적인 감각을 포함해야하지만, 그 이상의 것을 포함 할 수 있습니다. 상태 표현은 오리지날 센세이션의 고도로 처리 된 버전 일 수도 있고, 일련의 감각으로부터 시간이 지남에 따라 복잡한 구조가 될 수도 있습니다. 예를 들어 한 번에 자세하게 볼 수있는 중심점에 해당하는 작은 지점 만 있으면서 장면을 풍부하게 묘사 할 수 있습니다. 또는 더 분명히, 우리는 물체를보고, 멀리보고, 그것이 여전히 존재한다는 것을 알 수 있습니다. 우리는 

"

예

"

라는 말을들을 수 있고 이전에 온 질문과 더 이상 들리지 않는 질문에 따라 완전히 다른 상태에 있다고 생각할 수 있습니다. 평범한 수준에서, 제어 시스템은 두 개의 서로 다른 시간에 위치를 측정하여 속도에 대한 정보를 포함하는 상태 표현을 생성 할 수 있습니다. 이 모든 경우에 국가는 이전 국가 또는 과거 감각의 다른 기억과 함께 즉각적인 감각을 기반으로 건설되고 유지됩니다. 이 책에서, 우리는 그것이 어떻게 행해지는지 탐구하지는 않지만 확실히 그것은 할 수 있고 끝났습니다. 상태 표현을 즉각적인 감각으로 제한 할 이유가 없습니다. 전형적인 응용 프로그램에서는 주 대표가 그 이상을 에이전트에게 알릴 수 있어야합니다. 우리는 그것이 어떻게 행해지는지 탐구하지는 못하지만, 분명히 할 수 있고 행해질 수 있습니다. 상태 표현을 즉각적인 감각으로 제한 할 이유가 없습니다. 전형적인 응용 프로그램에서는 주 대표가 그 이상을 에이전트에게 알릴 수 있어야합니다. 우리는 그것이 어떻게 행해지는지 탐구하지는 못하지만, 분명히 할 수 있고 행해질 수 있습니다. 상태 표현을 즉각적인 감각으로 제한 할 이유가 없습니다. 전형적인 응용 프로그램에서는 주 대표가 그 이상을 에이전트에게 알릴 수 있어야합니다.

  


반면에 상태 신호는 환경에 대한 모든 것을 에이전트에게 알리거나 의사 결정에 유용 할 수있는 모든 것을 알려주지 않아야합니다. 에이전트가 블랙 잭을 사용하고 있다면, 갑판의 다음 카드가 무엇인지 알 필요가 없습니다. 상담원이 전화에 응답하는 경우 발신자가 누구인지를 미리 알 필요가 없습니다. 대리인이 도로 사고로 불리는 구급 요원 인 경우, 의식을 잃은 피해자의 내부 상해를 즉각적으로 알아야합니다. 이 모든 경우에 환경에 숨겨진 상태 정보가 있으며 에이전트가 알았지 만 해당 정보가 유용 할 수 있지만 관련 감각을받은 적이 없으므로 에이전트가 알 수 없습니다. 요컨대, 우리는 중요한 것을 알지 못하는 에이전트를 잘못하지 않으며,

  


\* 3.5. MARKOV PROPERTY 57은 뭔가를 알고 잊어 버렸습니다!

  


우리가 원하는 것은, 이상적으로 모든 과거의 감각을 조밀하게 요약하면서 모든 관련 정보가 유지되는 상태 신호입니다. 이것은 보통 즉각적인 감각 이상을 필요로하지만, 과거의 모든 감각의 완전한 역사 이상을 요구하지는 않습니다. 모든 관련 정보를 유지하는 데 성공한 상태 신호는 Markov이거나 Markov 속성을가집니다. 예를 들어 체커 위치 \(보드의 모든 조각의 현재 구성\)는 마코프 \(Markov\) 상태로 작용할 것입니다. 시퀀스에 대한 많은 정보가 손실되지만 게임의 미래에 중요한 요소는 모두 유지됩니다. 비슷하게, 포탄의 현재 위치와 속도는 장래 비행에 중요한 것입니다. 그 위치와 속도가 어떻게 발생했는지는 중요하지 않습니다. 문제의 모든 것이 현재 상태 신호에 있기 때문에이를 

"

경로 독립

"

속성이라고도합니다. 그 의미는 그것에 이르는 신호의 

"

경로

"

또는 역사와는 무관합니다.

  


이제 강화학습 문제에 대한 Markov 속성을 공식적으로 정의합니다. 수학을 간단하게 유지하기 위해, 여기에는 한정된 수의 주 및 보상 값이 있다고 가정합니다. 이것은 우리가 적분과 확률 밀도보다는 합계와 확률의 관점에서 작업 할 수있게하지만, 논증은 연속적인 상태와 보상 \(또는 무한한 불연속 공간\)을 포함하도록 쉽게 확장 될 수 있습니다. 시간 t에서 취해진 행동에 대해 일반적인 환경이 시간 t + 1에 어떻게 반응 하는지를 고려하십시오. 가장 일반적인 인과 관계의 경우,이 반응은 이전에 일어난 모든 것에 달려있을 수 있습니다. 이 경우 동역학은 완전 접합 확률 분포를 지정하여 정의 할 수 있습니다.

  


Pr St + 1 = s ', Rt + 1 = r \| S0, A0, R1, ..., St-1, At-1, Rt, St, At, \(3.4\)

  


S0, A0, R1, ..., St-1, At-1, Rt, St, At의 모든 r, s '및 모든 가능한 값을 나타냅니다. 반면에 상태 신호가 Markov 속성을 가지고 있다면 t + 1에서의 환경 응답은 t에서의 상태 및 동작 표현에만 의존하며,이 경우 환경의 동력학은

  


p \(s ', r \| s, a\) =. Pr St + 1 = s ', Rt + 1 = r \|

  


모든 r, s ', s 및 a에 대해 St = s, At = a, \(3.5\) . 즉, 상태 신호는 Markov 속성을 가지며, 모든 s ', r 및 이력에 대해 \(3.4\)가 p \(s', r \| St, At\)와 동일한 경우에만 Markov 상태이며 S0 , A0, R1, ..., St-1, At-1, Rt, St, At. 이 경우, 환경 및 작업 전체가 Markov 속성을 가지고 있다고합니다.

  


환경에 Markov 속성이 있으면 원스텝 역학 \(3.5\)을 통해 현재 상태와 동작에 따라 다음 상태와 다음 번 예상 보상을 예측할 수 있습니다. 이 방정식을 반복함으로써 현재 상태에 대한 지식만으로 모든 미래의 상태와 예상되는 보상을 예측할 수있을뿐만 아니라 현재까지의 전체적인 역사가 가능할 것이라고 예측할 수 있습니다. 또한 마르코프 주 \(州\)가 행동 선택을위한 최선의 기반을 제공합니다. 즉, 행동을 마르코프 국가의 함수로 선택하는 최선의 방책은 완전한 역사의 함수로서 행동을 선택하는 최선의 정책만큼이나 좋습니다.

  


제 3 장 최종 마르크스 결정 과정

  


상태 신호가 Markov가 아닌 경우에도 강화 학습의 상태를 Markov 상태의 근사치로 생각하는 것이 적절합니다. 특히, 우리는 국가가 미래의 보상을 예측하고 행동을 선택하기위한 좋은 기반이되기를 항상 원합니다. 환경 모델이 학습 된 경우 \(8 장 참조\) 우리는 또한 국가가 후속 주를 예측하기위한 좋은 기초가되기를 바랍니다. 마르코프주는 이러한 모든 일을하는 탁월한 기반을 제공합니다. 국가가 이러한 방식으로 마르코프 주립대의 능력에 접근하는 정도까지 강화학습 시스템에서 더 나은 성과를 얻을 수 있습니다. 이러한 모든 이유들로 인해 각 시간 단계의 상태를 마르코프 국가에 대한 근사치로 생각하는 것이 유용하지만 마르코프 재산을 완전히 만족시키지 못할 수도 있다는 것을 기억해야합니다.

  


Markov 속성은 결정과 값이 현재 상태의 함수로만 간주되기 때문에 강화 학습에서 중요합니다. 이것들이 효과적이고 유익한 정보를 얻으려면 주정부의 정보가 유익해야합니다. 이 책에 제시된 모든 이론은 마르코프 상태 신호를 가정합니다. 이것은 마르코프 속성이 엄격하게 적용되지 않는 경우에 모든 이론이 엄격하게 적용되는 것은 아니라는 것을 의미합니다. 그러나 마르코프 사건에 대해 개발 된 이론은 여전히 ​​알고리즘의 동작을 이해하는 데 도움이되며 알고리즘은 마르코프가 아닌 국가의 여러 작업에 성공적으로 적용될 수 있습니다. 마르코프 사건의 이론에 대한 완전한 이해는 마르코프 사건을보다 복잡하고 현실적인 비 마르코프 사건으로 확대하기위한 필수 기반이다. 마지막으로,

  


Example 3.5 : pole-balancing state 앞서 소개 한 pole-balancing task에서 상태 신호는 트랙을 따라 카트의 위치와 속도를 정확하게 지정하거나 정확히 재구성 할 수 있다면 Markov가 될 것이고, 카트와 폴, 그리고이 각도가 변화하는 속도 \(각속도\). 이상적인 장바구니 시스템에서이 정보는 컨트롤러가 취한 조치를 고려할 때 장바구니와 장대의 향후 작동을 정확하게 예측하는 데 충분합니다. 그러나 실제로는 실제 센서가 측정에 약간의 왜곡과 지연을 초래하기 때문에이 정보를 정확하게 알 수 없습니다. 게다가 어떤 실제 카트 - 폴 시스템에서도 항상 폴 \(pole\)의 휘어짐, 휠과 폴 베어링의 온도, 다양한 형태의 백래시가 시스템의 동작에 약간 영향을 미칩니다. 이러한 요인은 상태 신호가 장바구니와 장대의 위치와 속도 만인 경우 Markov 속성을 위반하게됩니다.

  


그러나 위치와 속도는 종종 국가로 잘 전달됩니다. 기둥 균형 조정 과제를 해결하기위한 초기 연구에서는 카트 위치를 오른쪽, 왼쪽 및 중간 \(다른 3 개의 고유 한 상태 변수의 유사한 거친 양자화\)의 세 영역으로 나눈 굵은 상태 신호를 사용했습니다. 마르코프가 아닌이 명백한 상태는 강화 학습 방법으로 과제를 쉽게 해결할 수 있도록 충분했습니다. 사실,이 거친 표현은 학습 에이전트가 작업을 해결하는 데 유용하지 않은 훌륭한 구별을 무시하도록하여 신속한 학습을 ​​촉진 할 수 있습니다.

  


  


\* 3.5. 마르코프 프로퍼티 59

  


Example 3.6 : Draw Poker Draw Poker에서 각 플레이어는 5 장의 카드를받습니다. 각 플레이어가 새 카드를 교환하기 위해 카드를 교환 한 다음 최종 내기가 있습니다. 각 라운드에서 각 플레이어는 다른 플레이어의 가장 높은 베팅과 일치하거나 초과해야하며, 그렇지 않으면 드롭 아웃 \(폴드\)해야합니다. 두 번째 베팅 라운드가 끝나면 최고의 핸드를 가진 플레이어가 폴드하지 않고 모든 베팅을받습니다.

  


드로우 포커의 상태 신호는 플레이어마다 다릅니다. 각 플레이어는 자신의 손에 든 카드를 알고 있지만 다른 플레이어의 손에있는 카드는 추측 할 수 있습니다. 일반적인 실수는 Markov 상태 신호가 모든 플레이어의 손과 덱에 남아있는 카드의 내용을 포함해야한다고 생각하는 것입니다. 그러나 공정한 게임에서, 우리는 플레이어가 원칙적으로 과거의 관찰로부터 이러한 것을 결정할 수 없다고 가정합니다. 플레이어가 그 사실을 알고 있다면 과거의 모든 관측치를 기억하는 것보다 미래의 사건 \(예 : 교환 할 수있는 카드\)을 예측할 수 있습니다.

  


자신의 카드에 대한 지식 외에도, 드로 포커에서 국가는 다른 플레이어가 베팅 한 베팅과 카드 수를 포함해야합니다. 예를 들어, 다른 플레이어 중 한 명이 새 카드를 3 ​​장 뽑은 경우, 그는 한 쌍의 카드를 가지고 있다고 생각할 수 있으며 이에 따라 손의 힘을 추측 할 수 있습니다. 선수의 베팅은 또한 자신의 손에 대한 평가에 영향을 미칩니다. 사실,이 특정 선수들의 과거 역사의 대부분은 마르코프 \(Markov\) 국가의 일부입니다. Ellen은 허세 부리는 것을 좋아합니까? 아니면 그녀가 보수적으로 플레이합니까? 그녀의 얼굴이나 태도가 그녀의 손의 힘에 단서를 제공합니까? 늦은 밤이나 이미 많은 돈을 벌었을 때 Joe의 플레이가 어떻게 바뀌 었습니까?

  


다른 플레이어에 대해 지금까지 관찰 된 모든 것이 다양한 종류의 손을 가지고있는 확률에 영향을 미칠 수 있지만, 실제로 이것은 기억하고 분석하기에는 너무 많으며, 대부분은 예측과 결정에 명확한 영향을주지 않습니다 . 아주 좋은 포커 플레이어는 핵심 단서만을 기억하고 새로운 플레이어를 빠르게 정리하는 데 능숙하지만 아무도 관련성있는 모든 것을 기억하지 못합니다. 결과적으로, 사람들이 포커 결정을 내리기 위해 사용하는 주정부 표현은 의심의 여지가 아닌 Markov이며 의사 결정 자체는 아마도 불완전 할 것입니다. 그럼에도 불구하고, 사람들은 여전히 ​​그러한 업무에서 매우 훌륭한 결정을 내립니다. 완벽한 Markov 상태 표현에 접근 할 수 없다는 것은 강화 학습 에이전트에서 심각한 문제가 아닐 가능성이 크다.

  


연습 3.6 : 망가진 시력 시스템 당신이 시력 시스템이라고 상상해보십시오. 하루를 처음 켜면 카메라로 이미지가 넘칩니다. 당신은 많은 것을 볼 수 있지만 모든 것을 볼 수는 없습니다. 폐색 된 물체는 볼 수 없으며, 뒤에있는 물체는 볼 수 없습니다. 첫 번째 장면을보고 나면 환경의 마르코프 상태에 액세스 할 수 있습니까? 하루 종일 카메라가 고장났다가 하루 종일 아무런 이미지도받지 못했다고 가정 해보십시오. 그럼 Markov 상태에 액세스 할 수 있습니까?

  


  


제 3 장 최종 마르코프 결정 과정 3.6 마르코프 결정 과정

  


Markov 속성을 만족시키는 강화학습 과제를 Markov decision process 또는 MDP라고합니다. 주 공간과 작업 공간이 유한 한 경우이를 유한 Markov 결정 프로세스 \(유한 MDP\)라고합니다. 유한 MDP는 강화 학습 이론에 특히 중요합니다. 우리는이 책 전체에서 광범위하게 취급합니다. 그들은 현대 강화학습의 90 %를 이해하는 데 필요한 모든 것입니다.

  


특정 유한 MDP는 상태 및 동작 집합과 환경의 한 단계 동적 특성에 의해 정의됩니다. 어떤 상태와 행동 s와 a가 주어지면, 다음 상태와 보상의 가능한 쌍의 확률 s ', r은

  


p \(s', r \| s, a\) =로 표시됩니다. Pr St + 1 = s ', Rt + 1 = r \| St = s, At = a. \(3.6\)이 양은 유한 MDP의 동역학을 완전하게 나타냅니다. 대부분의 이론

  


우리는이 책의 나머지 부분에서 환경이 유한 한 MDP라고 암묵적으로 가정합니다.

  


\(3.6\)에 의해 규정 된 동력학을 감안할 때, 국가 - 행동 쌍

  


r \(s, a\) =에 대한 기대 보상과 같이 환경에 대해 알고 자하는 다른 것을 계산할 수있다 . \(s ', s \| s\),

  


상태 전이 확률,

  


p \(s'\| s, a\) 는 다음 과 같이 정의된다 . E \[Rt + 1 \| St = s, At = a\] = rp . 홍보 세인트 + 1 = s의 '\| 세인트 = s의에서 = A = P \(들', R \| S, A\),

  


r∈R

  


및 국가 행동 다음 상태 트리플에 대한 기대 보상

  


R \(S, A , s '\) =. E Rt + 1 St = s, At = a, St + 1 = s '= r∈Rrp \(s', r \| s, a\). p \(s '\| s, a\)

  


\(3.7\)

  


\(3.8\)

  


\(3.9\)

  


이 책의 초판에서 동역학은 Pass와 Rass로 각각 표시된 두 개의 양으로 표현되었다. 그 표기법의 약점 중 하나는 여전히 보상의 역 동성을 충분히 특성화하지 못하여 기대치 만 제공한다는 것입니다. 또 다른 약점은 아래 첨자와 위 첨자의 초과입니다. 이 판에서는 \(3.6\)의 명시적인 표기법을 주로 사용하고, 때로는 전이 확률 \(3.8\)을 직접 참조합니다.

  


예제 3.7 : 재활용 로봇 MDP 재활용 로봇 \(Example 3.3\)은 MDP의 단순화 된 예와 좀 더 자세한 내용을 제공함으로써 MDP의 간단한 예가 될 수 있습니다. \(우리의 목표는 특별히 현실적인 것이 아닌 간단한 예를 만드는 것입니다.\) 에이전트가 외부 이벤트 \(또는 로봇 제어 시스템의 다른 부분\)에 의해 결정될 때 결정을 내리는 것을 상기하십시오. 이 때마다 로봇은 \(1\) 적극적으로 캔을 검색해야하는지, \(2\) 정지 상태를 유지하고 누군가가 캔을 가져올 때까지 기다릴 것인지 또는 \(3\) 배터리를 다시 충전하기 위해 홈베이스로 돌아갈 것인지를 결정합니다. 환경이 다음과 같이 작동한다고 가정합니다. 캔을 찾는 가장 좋은 방법은 적극적으로하는 것

  


입니다. 3.6. MARKOV DECISION PROCESSES 61

  


그들을 검색하지만, 로봇의 배터리가 떨어지는 반면 대기는하지 않습니다. 로봇이 검색 할 때마다 배터리가 고갈 될 가능성이 있습니다. 이 경우 로봇을 종료하고 구조 요청을 기다려야합니다 \(낮은 보상 제공\).

  


에이전트는 배터리의 에너지 수준에 따라 결정합니다. 그것은 높고 낮은 두 가지 수준을 구별 할 수 있으므로 상태 집합은 S = {높음, 낮음}입니다. 상담원의 행동 인 대기, 검색 및 재충전과 같은 가능한 결정을 호출하겠습니다. 에너지 레벨이 높으면 재충전은 항상 어리석은 일 이니, 우리는이 상태를 위해 설정된 행동에 포함시키지 않습니다. 상담원의 작업 세트는

  


A \(높음\) =입니다. {검색, 대기}

  


A \(낮음\) =. {검색, 대기, 재충전}.

  


에너지 수준이 높으면 배터리를 소모 할 위험없이 항상 능동 검색 기간을 완료 할 수 있습니다. 고 에너지 레벨로 시작하는 탐색 기간은 확률 α로 에너지 레벨을 높게 유지하고 확률 1 - α로이를 낮게 감소시킵니다. 반면, 에너지 수준이 낮을 때 수행되는 검색 기간은 확률 β로 낮게 유지하고 확률 1 - β로 배터리를 소모합니다. 후자의 경우 로봇을 구조해야하고 배터리를 다시 높은 상태로 충전해야합니다. 각 로봇은 단위 보상으로 계산할 수 있지만 보상을받을 경우 로봇을 구조해야 할 때마다 -3이 계산됩니다. rsearch

&gt;

 rwait 인 rsearch와 rwait은 각각 검색 할 때와 기다리는 동안 로봇이 수집 할 예상 캔 수 \(따라서 예상되는 보상액\)를 나타냅니다. 마지막으로, 물건을 간단하게 유지하기 위해, 재충전을 위해 집에 집을 채울 수있는 캔이없고, 배터리가 고갈 된 단계에서 캔을 수집 할 수 없다고 가정하십시오. 이 시스템은 유한 MDP이며 표 3.1과 같이 전환 확률과 예상 보상을 기록 할 수 있습니다.

  


s s

  


'높음 높음 높음 낮음 낮음 높음 낮음 낮음 높음 높음 낮음 낮음 높음 높음 낮음 낮음 높음 높음 낮음 p \(s'\| s, a\)

  


r \(s, a, s '\)

  


a

  


α 검색 검색 1-α 검색 검색 1-β-3 검색 β rsearch wait 1 rwait wait 0 rwait wait 0 rwait wait 1 rwait recharge 1 0 recharge 0 0.

  


표 3.1 : 재활용 로봇 예제의 유한 MDP에 대한 전환 확률 및 예상 보상. 현재 상태, s, 다음 상태, s '및 현재 상태에서 가능한 동작의 가능한 조합에 대한 행이 있으며, a ∈ A \(s\)입니다.

  


전이 그래프는 유한 MDP의 동력을 요약하는 데 유용한 방법입니다. 그림 3.3은 재활용 로봇 예제의 전환 그래프입니다.

  


  


rt

  


  


  


bg

  


mw 가 2 개 있습니다.

  


s

  


rh

  


o

  


  


o

  


r

  


t

  


  


\|

  


m

  


  


as

  


tn

  


nb

  


\) aa

  


d

  


  


  


검색

  


1 1 

  


검색 1

  


0

  


0

  


  


  


  


  


à 

  


5

  


  


0 0 대기 대기

  


62

  


검색 제 3 장 FINITE MARKOV 결정 프로세스

  


대기 1,

  


대기

  


대기 대기

  


0 1-!, -3

  


  


  


\(0 \|\)! ,

  


\(0\) 검색

  


대기 à à

  


1 

  


0 0.

  


  


대기

  


검색

  


  


1 

  


검색 높은 0 low10 0 기다려

  


\(\) 0 대기 \| \(\)는

  


0 

  


1 0

  


재충전

  


염 둥이 MP에 대한 D 보상

  


1 

  


검색 대기

  


1~5 

  


5 대기

  


각각의 가능한 조합에 대한

  


검색

  


, 현재의 상태를 가능

  


  


1 0

  


됨 0

  


대기

  


  


  


1

  


"

, R 검색 

  


1

"

,

  


  


0

  


0 0

  


\(\|\) \(\) 

  


0

  


0\) \(\)

  


  


  


1 wa1it 0

  


검색

  


R 검색 검색

  


R 검색 검색

  


R 대기 대기

  


검색

  


1 검색

  


검색

  


obabilities 및 e pecte

  


à à à 

  


. 행 상태 0과 동작

  


0 \(\| 

  


  


  


1 1 0

  


5

  


  


1 

  


1\) 

  


은이 상태에 대해 설정된 작업에 포함하지 않습니다.

  


  


1 1 대기

  


  


0 0 대기 대기

  


검색

  


대기

  


0 0

  


표 .1 전환 확률 및

  


우리는 MP 0에 대한 보상을받습니다. 우리는 에이전트의 행동이

  


기다리는 가능한 결정을합니다.

  


1 개의 클램핑 로봇이 충분합니다. 가능한 조합 1에 대한 행이 있습니다 

  


. 재활용 로봇을위한 ecThraarngisnitgiownouglurdaph는 다음과 같습니다.

  


현재 상태의

  


0 검색 , 현재 상태 및 가능한 현재 상태에서 검색 가능한 작업, 1 0

  


2 A \(\).

  


노드의 종류 : 상태 노드와 동작 노드 각 가능한 상태에 대한 노드가

  


}

  


상태 즉 ciasniodnsa의 nthaectaigoentn의 soadcetiofonrs을 \(큰 openiscSirc 르 labeledb} {Y는. Ltheteunsacmalel는 tohfetphoesssibtaleted\)

  


},, 및. 암탉 energ 르 엘은 각 상태 액션 쌍합니다 \(작업의 이름으로 레이블이 붙은 작은 고체 원 것이다 충전된다

  


염 둥이 MP 1 개 대기 대기에 대한 0.1 Transit1ion probab의 ilities 및 전자 pected 보상 할 수 T

  


촬영 \(CL\)의 0 g ro1bot E 형 amp0le있다. 바보 각각의 가능한 조합 ALWA s에 대한 행이 있으므로이 상태에 대해 설정된 액션을 0include 않는다.

  


1 개

  


a0l .IN O EF ctuor RTE HN 등 일 TA TE 의해 ne0cted 콘 , n0o, dne\) .t Ssta0tre.t, g, aindstacteiosn apnodssitbalekiingthaectciuorereantmsotavtes,

  


에이전트의 액션 세트는 다음과 같은 기간의 행위가 될 수 있습니다.

  


상태 노드 s에서 동작 노드 \(s, a\)까지 줄을 따라 가면됩니다. 그런 다음 배터를 고갈시키는 환경.

  


화살표 중 하나를 통해 다음 상태의 노드로의 전환으로 응답하는 검색 기간

  


표 .1 전환 가능성 \(상태\) 및 영향 \(문제\) nite MP 능력 및 nite에 대한 보상 MP

  


르 엘 레아 probabilit와 energ 르 엘 말이지 것은

  


S는 {}. 우리가 가능한 결정을 '대리인의 행위'

  


\(lsin, ga\) r.obEotaceh aamrprolew라고 부르 자. 청각 장애인 \(isb, lesc, oam\) b, iwnahteiorne s는 다음 상태입니다. 가능한 조합마다 행이 있습니다.

  


A 가능성 \(확률\) 1. 다른 한편으로는

  


, 0, 그리고. 현재 에너지

  


상태, 상태, 그리고 현재 상태에서 가능한 활동을 재충전 할 수있는 에너지가 있다면, 설정 상태, 상태 확률, p \(s \| s, a\) 및 기대

  


되는 에너지 엘은 probabilit

  


2 A \(\)를 가지고있다. 어리 석다. 그래서 우리는이 상태를 위해 설정된 행동에 그것을 포함하지 않는다.

  


그 천이 R \(S, A, S\)에 대한 보상. probabilit 1로 라벨링하는 전이 확률이 있음에 유의하십시오. 후자의 경우, 로봇

  


에너지가 있다면, 활동 검색 기간은 에이전트가 할 수있는 행동 집합이다.

  


이 배타적 인 실수는 배타적이다. 1.

  


배터가 고갈 될 위험없이 완료되었습니다. 검색 기간

  


은 S {}입니다. 우리가 할 수있는 결정은 상담원의 행동에 따라 달라지며, 소파에 앉아있는 사람들은 자신의 능력에

  


따라 에너지를 소모

  


합니다. 에너지가 부족하다면 재충전 할 수 있습니다. 전자 레인지, 레이져, 레인지,

  


검색 및 적색은 가능성이 있음 1. n 한편, A \(\) {}의 기간

  


우리는 어리 석다. 그래서 우리는이 상태를 위해 설정된 행동에 그것을 포함시키지 않는다. 그 밖의 다른 사람들과의 협조가 필요합니다. TT \(haend는

  


르 EL energ가 레아 probabilit 3.7 가치 함수와 함께 말이지 때 수행들을 검색

  


에이전트의 작업 세트가

  


검색 동안 기다리는 동안 d\). inall,

  


probabilit 1로 배터 를 지키고 고갈시키기 위하여. 후자의 경우, 로봇

  


이 에너지를 발휘한다면 활동 검색 기간 동안 집을 구할 수있는 캔을 모을 수 없으므로

  


구출해야하며 배터는 다시 충전됩니다. 각 수

  


배터가

  


수집 한 단계에서 수집 한 단계에서

  


거의 보상을받지 못하면서 거의 보상을받지 못한다. 결과의 보상은

  


에너지 보상으로 시작 하는 반면, \(\) {} 캔트에서 수집 한 결과 는 거의 보상되지 않는다.

  


로봇이 구출되어야 할 때

  


그가 얼마나 좋은지를 추정하는 국가 \(또는 국가 - 행동 쌍\)의 probabilit 기능을 가진 다면 \(즉, MP\) . 검색 대기 및 검색 대기,

  


그럴 가능성을 줄이자. 다른 한편으로는, gdivTeanblset a.t1e와 같이, 어프로치 트로스트 바우어의 기간을 지정합니다. \(또는 주어진 영역에서 주어진 행동을 수행하는 것이 얼마나 좋은지

  


는 로봇이 수집 할 수있는 깡통 수를 나타냅니다 \(그리고 에너지가

  


잠재적 인 상태로 그것을 찾을 때 검색이 수행됩니다 \). Thenotionhoefn 

"

cehothweeg opoedct

"

 에드레마이저스\) 에피소드를 검색 할 수 있고 검색을

  


통해 배터를 죽일 수 있습니다 1. 후자의 경우, 로봇

  


단순한 일이지만, 집에서 달리기를 할 때 깡통을 모을 수는 없다고 가정 해 봅시다.이 깡통은

  


배터를 고갈시키는 데 쓰 입니다.

  


구출해야하는 검색 기간이 끝나면 배터는 다시 충전됩니다. 각각은

  


재충전 할 수 있으며, 타자가 에너지를 사용하여 에너지를 얻을 수있는 단계에서 캔을 수집 할 수

  


없다는 점을 고려해야합니다. 수집 된 로봇은 단위 보상으로 계산되지만, 결과의 보상

  


소진되었습니다. 이 줄기는 그 때 nite MP이며, 우리는 그것을 적어서 probabilit 1로 줄입니다. 반면에,

  


가능성이 있는 기간 1. n 다른 한편으로는, 기간은 다음

  


과 같습니다. 귀중한 것, 귀중한 것, 귀중한 것, 평화로운 것, 평범한 것. ,

  


전이 확률 및 보상 된 보상을 표 1에 제시한다. 에너지 원

  


이 로봇이 수집 할 수있는 깡통 수를 나타낼

  


수있는 에너지 가 있는지를 조사 할 때 수행되는 검색 \(그리고

  


정책, π은 각각의 상태, s∈S, 그리고 행동, a ∈ A \(s\),

  


가능성이있는 배터를 고갈시킨다. 후자의 경우, 가능성이있는 로봇 1. 후자의 경우, 로봇은

  


그로부터 영향을받습니다.\) awhuislefuselawrcah inta asnudmwmhairlei we atihtiengd. n ainmailcl s, otfo

  


는 상태 s에서 행동을 취할 확률 π \(a \| s\)를 유지한다. 비공식적으로,의 가치는 구출되어야하고, 배터는 그 때 등을 맞댄 재충전된다. 각

  


배터는 다시 충전 할 수 있습니다 . 각각은

  


일을 간단하게 할 수 있습니다. 단 한

  


번의 보상으로 시작될 때 동료들에 대한 집에서 집을 채취 할 수있는 캔이 없다고 가정하면 ,

  


결과 보상에 대한 보상으로 π를 보상하는 반면에 \(\)

  


재충전 할 수 있고, 배터가

  


ertherobothastberescued 할 단계에 깡통을 모을 수 없다 . 하자 erescueda.nLdetfo가 llowinangdπ, er.Fo rM으로 seDarP을 rweiathft, CHS는 wewcaaitndefinseeavrch \(들\) fwoairtmallyas

  


와, 

  


검색 허리 그의 WSA의 isttem는 다음 귀염둥이 Mπ P이다 depletesdea.rcTh, 우리는 아래로 쓸 수있는

  


전자를 나타내는 respecti 엘

  


검색 할 때와 기다리는 동안 로봇이 수집 할 수있는 깡통 수 \(그리고

  


표 1에서와 같이 전환 확률과 보상 된 보상, 그러므로 보상 된 보상\)을 수집합니다. 기다리는 동안 탐색하면서

  


.

  


것들 simpvle \(SU\) p = poEse \[tGhat \| NSO C = anss\] C = anEbe colleγcteRd 동안 aSru = nsho, tnocanscanbeπcollectedπdurtingatrunhomeπfor t + K + 1 t 저

  


  


∞

  


A가 E에게 무한의 D namics을 summari하는 akuseful WA는

  


\( 3.10\) transwithioenreprEoba \[· b\]은 \(는\) 다음과 같은 제품을 사용하고 있음을 안다. 변수는 에이전트가

  


재충전되고, batterk = 0

  


이 고갈 된 단계에서 타자를 수집 할 수있는 단계에서 캔을 수집 할 수 없다는 점에서 주어진다 . 이 줄기는 그때 nite MP이며, 그 다음에 nite MP를 적어 놓고

  


표 1과 같이 설명 된 줄거리를 기록 할 수 있습니다 .

  


정책 π를 따르고, t는 임의의 시간 간격이다. 터미널 상태의 값은 if

  


A는 E에게 무한의 D namics summari 전자 진드기에게 유용한 WA의 D namics summari을 위해 유용하다 WA

  


검색 대기 검색 대기

  


,

  


3.7. any는 항상 0입니다. 우리는 함수 π를 정책 π에 대한 상태 값 함수라고 부른다.

  


유사하게 우리는 정책 π \(정책 π\) 하에서 행동 s를 시작으로 정책 π를 따르는 예상 수익으로 q π \(s, a\)로 표현 된 정책 π 하에서 행동 s를 취하는 값을 정의한다.

  


∞ 

  


γkRt + k + 1 St = s, At = a. \(3.11\) 우리는 정책 π에 대한 행동 가치 함수를 qπ라고 부른다.

  


가치 함수 vπ와 qπ는 경험을 통해 추정 할 수 있습니다. 예를 들어, 에이전트가 정책 π를 수행하고 발생한 각 상태에 대해 해당 상태를 지난 실제 수익의 평균을 유지하면 평균은 상태의 값 vπ \(s\)로 수렴합니다. 상태는 무한대에 접근한다. 한 상태에서 취한 각 행동에 대해 별도의 평균이 유지되면 이러한 평균은 마찬가지로 행동 값 qπ \(s, a\)에 수렴합니다. 우리는 이러한 종류의 몬테카를로 추정 방법을 실제 회수의 많은 무작위 표본에 대해 평균화하기 때문에 호출합니다. 이러한 종류의 방법은 5 장에 제시되어있다. 물론, 매우 많은 주들이 있다면, 각 주마다 개별적인 평균을 유지하는 것이 현실적이지 않을 수도있다. 대신, 에이전트는 vπ 및 qπ를 매개 변수화 된 함수 \(상태보다 적은 수의 매개 변수\)로 유지하고 관측 된 반환 값과보다 잘 일치하도록 매개 변수를 조정해야합니다. 매개 변수화 된 함수 근사자의 특성에 따라 크게 달라 지지만 정확한 추정도 가능합니다. 이러한 가능성은이 책의 두 번째 부분에서 논의됩니다.

  


강화학습과 동적 프로그래밍을 통해 사용되는 가치 기능의 근본적인 특성은 특정한 재귀 관계를 만족 시킨다는 것입니다. 임의의 정책 π 및 임의의 상태 s에 대해, s의 값과 그것의 가능한 후속 상태의 값 사이에 다음의 일관성 조건이 유지된다 :

  


qπ \(s, a\) =. Eπ \[GT는 \| 세인트가 S에서, = A = Eπ에서

  


K = 0

  


\(V\) \(S\) = E \[G \| S = S\]. 

  


π π TT 

  


= E ∞γkR S의가 S의 π의 t + K + 1 t의

  


K = 0 

  


= Eπ Rt +1 + γ ∞ γkRt + k + 2 St = s 

  


k = 0 ∞ 

  


= π \(a \| s\) p \(s ', r \| s, a\) r + γEπγkRt + k + 2 St + 1 = s 'as = r = 0

  


= π \(a \| s\) p \(s', r \| s, a\) r + γvπ \(s '\), ∀s∈S, \(3.12\)

  


여기서 a, s, s는 집합 S \(또는 에피소드 문제의 경우 S +\)에서 취해지고 보상은 집합 A \(s\)에서 취해지는 암시 적이며, r은 집합 R에서 취해진 다. 또한 마지막 방정식에서 s '의 모든 값과 r의 모든 값에 대해 하나씩 두 합계를 어떻게 병합

  


했는지 주목하라.

  


\(v ⇡\)의

  


\(q⇡\) S, A

  


아칸소

  


S0

  


r에

  


S0의

  


A0의

  


그림 3.4 : vπ 및 qπ에 대한 백업도.

  


가능한 모든 값에 대해 하나의 합계로 나타냅니다. 수식을 단순화하기 위해 이러한 종류의 병합 된 합계를 자주 사용합니다. 최종 표현식이 예상 값으로 매우 쉽게 읽을 수있는 방법에 유의하십시오. 그것은 정말로 3 개의 변수 인 a, s '및 r의 모든 값에 대한 합계입니다. 각 트리플에 대해 우리는 확률 π \(a \| s\) p \(s ', r \| s, a\)를 계산하고 그 확률로 괄호 안의 수량을 가중 한 다음 모든 가능성을 합하여 기대 값을 얻습니다.

  


식 \(3.12\)는 vπ에 대한 벨만 방정식이다. 국가의 가치와 후계 국가의 가치 사이의 관계를 표현한다. 그림 3.4 \(왼쪽\)에 제시된대로 한 국가에서 가능한 후계 국가를 미리 생각해보십시오. 각각의 열린 원은 상태를 나타내고 각 원은 상태 - 액션 쌍을 나타냅니다. 상단의 루트 노드 인 상태 s에서 시작하여 에이전트는 몇 가지 동작 중 하나를 취할 수 있습니다. 세 가지가 그림 3.4 \(왼쪽\)에 나와 있습니다. 이들 각각으로부터, 환경은 보상과 함께 몇 가지 다음 상태 중 하나 s '로 응답 할 수 있습니다. Bellman 방정식 \(3.12\)은 모든 가능성에 대해 평균을 내고 발생 가능성에 따라 가중치를 부여합니다. 시작 상태의 값은 예상되는 다음 상태의 \(할인 된\) 값과 함께 예상되는 보상을 더한 값과 같아야합니다.

  


값 함수 vπ는 Bellman 방정식에 대한 고유 한 해법입니다. 다음 장에서 Bellman 방정식이 vπ를 계산하고, 근사하고, 학습하는 여러 가지 방법의 기초를 형성하는 방법을 보여줍니다. 그림 3.4 백업 다이어그램에 표시된 다이어그램은 강화 학습 방법의 핵심에있는 업데이트 또는 백업 작업의 기초를 형성하는 관계를 그려주기 때문에 호출합니다. 이러한 동작은 값 정보를 후속 상태 \(또는 상태 - 동작 쌍\)에서 상태 \(또는 상태 - 동작 쌍\)로 다시 전송합니다. 이 책 전체에서 백업 다이어그램을 사용하여 우리가 논의하는 알고리즘의 그래픽 요약을 제공합니다. \(전환 그래프와 달리, 백업 다이어그램의 상태 노드는 반드시 고유 한 상태를 나타내는 것은 아니며, 예를 들어 상태가 자체 후계자 일 수 있습니다.

  


Example 3.8 : Gridworld 그림 3.5 \(왼쪽\)는 간단한 유한 MDP의 직사각형 그리드 월드 표현을 보여줍니다. 그리드의 셀은 환경 상태에 해당합니다. 각 셀에서 북쪽, 남쪽, 동쪽 및 서쪽의 네 가지 동작이 가능합니다.이 동작은 에이전트가 그리드의 각 방향으로 한 셀을 이동하게합니다. 에이전트를 그리드에서 벗어나게하는 작업은 위치를 변경하지 않고 -1의 보상을줍니다. 다른 행동은 특별한 상태 A와 B에서 에이전트를 이동시키는 것을 제외하고는 0의 보상을 낳습니다. 상태 A에서,

  


가장자리로 달려가는 가능성에 대해 부과 된 penalt \(부정적인 보상\)은

  


3.7 이상 입니다. 가치 기능

  


65

  


3.3

  


8.8

  


4.4

  


5.3

  


1.5

  


1.5

  


3.0

  


2.3

  


1.9

  


0.5

  


0.1

  


0.7

  


0.7

  


0.4

  


-0.4

  


-1.0

  


-0.4

  


-0.4

  


-0.6

  


-1.2

  


-1.9

  


-1.3

  


-1.2

  


-1.4

  


-2.0

  


A

  


B

  


+5

  


+10

  


\(a\)

  


\(b\)

  


B '

  


동작

  


A'

  


그림 3.5 : 그리드 월드 예 : 예외적 인 보상 역학 \(왼쪽\)과 동일 확률 무작위 정책 \(오른쪽\)의 상태 값 함수.

  


igure .5 그리드 전자 \(a\) 전자 상거래 보상 \(b\) 전자 채권 확률 임의 확률에 대한 상태 계산 기능.

  


네 가지 행동 모두 +10의 보상을 받고 에이전트를 A '로 데려갑니다. B 상태에서 모든 행동은 +5의 보상을 얻고 에이전트를 B '에게 가져갑니다.

  


에이전트가 모든 상태에서 동일한 확률로 4 개의 모든 동작을 선택한다고 가정합니다. 그림 3.5 \(오른쪽\)는 γ = 0.9 인 할인 보상 사례에 대한이 정책의 가치 함수 vπ를 보여준다. 이 값 함수는 선형 방정식 시스템 \(3.12\)을 계산하여 계산되었습니다. 하단 가장자리 근처의 음수 값을 확인하십시오. 이것들은 무작위 정책 하에서 그리드의 가장자리를 강타 할 확률이 높기 때문이다. 주 A는이 정책 하에서 가장 좋은 주이지만, A에서 에이전트가 A '로 이동하여 그리드의 가장자리로 들어가기 쉽기 때문에 예상 수익은 즉각적인 보상 인 10 미만입니다 . 반면에 B는 에이전트가 B '로 이동하기 때문에 즉각적인 보상 인 5보다 큰 값을 갖습니다.이 값은 양수 값을가집니다.

  


예제 3.9 : 골프 강화학습 과제로 골프 홀을 만들려는 경우, 우리는 볼을 홀에 넣을 때까지 각 스트로크에 대해 -1의 패널티 \(부정적인 보상\)를 계산합니다. 상태는 볼의 위치입니다. 상태의 값은 해당 위치에서 홀에 대한 스트로크 수의 음수입니다. 우리의 행동은 공을 어떻게 목표로하고 스윙하는지, 그리고 우리가 어떤 클럽을 선택하는지입니다. 주어진 포뮬러를 가져 와서 우리가 퍼터 나 운전자라고 가정하는 클럽의 선택 만 고려해 봅시다. 그림 3.6의 위쪽 부분은 항상 퍼터를 사용하는 정책에 대해 가능한 상태 값 함수 vputt \(s\)를 보여줍니다. 구멍 안에있는 터미널 상태는 0의 값을 가지고 있습니다. 녹색의 어디에서나 우리는 퍼팅을 할 수 있다고 가정합니다. 이러한 상태 값은 -1입니다. 녹색에서 우리는 퍼팅으로 구멍에 닿을 수 없으며 값은 더 큽니다. 퍼팅을 통해 주에서 녹색에 도달 할 수 있다면 그 상태는 녹색의 값인 -2보다 작은 값 1을 가져야합니다. 간단히하기 위해, 우리는 매우 정확하고 결정 론적으로 퍼팅 할 수 있지만 제한된 범위로 퍼트 할 수 있다고 가정 해 봅시다. 이것은 그림에서 -2로 표시된 선명한 등고선을 제공합니다. 그 선과 녹색 사이의 모든 위치는 구멍을 완성하기 위해 정확히 2 스트로크가 필요합니다. 마찬가지로, -2 등고선의 퍼팅 범위 내의 모든 위치는 그림에 표시된 모든 등고선을 얻으려면 -3의 값을 가져야합니다. 퍼팅은 우리를 모래 함정에서 빠지게하지 않으므로 -∞의 값을 갖습니다. 전반적으로, 퍼팅으로 티에서 홀로 가려면 6 스트로크가 필요합니다. 아주 정밀하고 결정 론적으로 퍼팅 할 수 있다고 가정 해 봅시다. 이것은 그림에서 -2로 표시된 선명한 등고선을 제공합니다. 그 선과 녹색 사이의 모든 위치는 구멍을 완성하기 위해 정확히 2 스트로크가 필요합니다. 마찬가지로, -2 등고선의 퍼팅 범위 내의 모든 위치는 그림에 표시된 모든 등고선을 얻으려면 -3의 값을 가져야합니다. 퍼팅은 우리를 모래 함정에서 빠지게하지 않으므로 -∞의 값을 갖습니다. 전반적으로, 퍼팅으로 티에서 홀로 가려면 6 스트로크가 필요합니다. 아주 정밀하고 결정 론적으로 퍼팅 할 수 있다고 가정 해 봅시다. 이것은 그림에서 -2로 표시된 선명한 등고선을 제공합니다. 그 선과 녹색 사이의 모든 위치는 구멍을 완성하기 위해 정확히 2 스트로크가 필요합니다. 마찬가지로, -2 등고선의 퍼팅 범위 내의 모든 위치는 그림에 표시된 모든 등고선을 얻으려면 -3의 값을 가져야합니다. 퍼팅은 우리를 모래 함정에서 빠지게하지 않으므로 -∞의 값을 갖습니다. 전반적으로, 퍼팅으로 티에서 홀로 가려면 6 스트로크가 필요합니다. -2 등고선의 퍼팅 범위 내의 모든 위치는 그림에 표시된 모든 등고선을 얻으려면 -3의 값을 가져야합니다. 퍼팅은 우리를 모래 함정에서 빠지게하지 않으므로 -∞의 값을 갖습니다. 전반적으로, 퍼팅으로 티에서 홀로 가려면 6 스트로크가 필요합니다. -2 등고선의 퍼팅 범위 내의 모든 위치는 그림에 표시된 모든 등고선을 얻으려면 -3의 값을 가져야합니다. 퍼팅은 우리를 모래 함정에서 빠지게하지 않으므로 -∞의 값을 갖습니다. 전반적으로, 퍼팅으로 티에서 홀로 가려면 6 스트로크가 필요합니다.

  


  


제 3 장 최종 마르코프 결정 과정

  


v

  


퍼트

  


! 4! 3

  


모래! 

"

  


! 1

  


퍼팅

  


V

  


! 2! 3

  


! 1

  


! 2

  


! 4

  


! 5 샌프란

  


0

  


! 6

  


\* qQ \(s \(, sd, drriivveerr\)\)

  


sa! 3 n

  


d

  


! 3

  


! 2

  


녹색! 4 d

  


! 

"

  


⇤

  


모래

  


2!

  


0 1 2 녹색!

  


도 3.6 골프 예 : 위 퍼팅 상태 값 함수 및 드라이버 \(아래\)를 사용하기위한 최적의 동작 값 함수.

  


실습 3.7 동작 값, 즉 qπ에 대한 벨만 방정식은 무엇입니까? 그것은 상태 - 행동 쌍 \(s, a\)에 대한 가능한 후임자의 행동 값 qπ \(s ', a'\)의 관점에서 행동 값 qπ \(s, a\)를 제공해야한다. 힌트로,이 방정식에 해당하는 백업 다이어그램은 그림 3.4 \(오른쪽\)에 나와 있습니다. \(3.12\)와 유사하지만 동작 값에 대한 방정식의 시퀀스를 보여줍니다.

  


운동 3.8 Bellman 방정식 \(3.12\)은 그림 3.5 \(오른쪽\)에 표시된 값 함수 vπ에 대한 각 상태에 대해 유지되어야합니다. 예를 들어,이 방정식이 +2.7, +0.4, -0.4 및 +0.7의 4 개의 이웃 상태에 대해 +0.7의 값을 갖는 중심 상태에 대해 유지된다는 것을 수치로 표시합니다. \(이 숫자는 소수 첫째 자리까지만 정확합니다.\)

  


연습 3.9 그리드 월드 예제에서, 보상은 목표에 긍정적이며, 세계의 경계선에 부딪 치는 것에는 부정적이며 나머지 시간은 0입니다. 이 보상의 표시가 중요합니까, 아니면 단지 그 간격 만 있습니까? \(3.2\)를 사용하여 모든 보상에 상수 c를 추가하면 모든 상태의 값에 상수 vc가 추가되므로 모든 정책 하에서 모든 상태의 상대 값에는 영향을 미치지 않는다는 것을 증명하십시오. 무슨 일이야?

  


Exercise 3.10 에피소드

  


3.8의 모든 보상에 상수 c를 추가하는 것을 고려해보십시오 . OPTIMAL VALUE FUNCTIONS 67 미로 실행과 같은 작업. 이것이 어떤 효과가

  


있습니까, 아니면 위의 계속되는 과제에서와 같이 과제를 변경하지 않을 것 입니까? 그 이유는 무엇? 예를 든다.

  


연습 문제 3.11 국가의 가치는 그 국가에서 가능한 행동의 가치와 현재의 정책에 따라 각 행동이 취해질 가능성에 달려있다. 우리는 국가에 뿌리를 둔 작은 백업 다이어그램과 각각의 가능한 행동을 고려하여 생각할 수 있습니다.

  


이 직관과 방정식에 해당하는 방정식을 루트 노드의 값 vπ \(s\)에 대한 값으로 환산하십시오. 예상 리프 노드, qπ \(s, a\), 주어진 St = s. 이 방정식에는 방침 π에 대한 조건이 포함되어야한다. 그런 다음 방정식에 기대 값 표기법이 나타나지 않도록 예상 값을 π \(a \| s\)로 명시 적으로 작성하는 두 번째 방정식을 제공하십시오.

  


실습 3.12 행동의 가치, qπ \(s, a\)는 기대되는 다음 보상과 남은 보상의 예상 합계에 따라 달라집니다. 다시 우리는 작은 백업 다이어그램, 즉 행동 \(상태 - 행동 쌍\)에 뿌리를두고 가능한 다음 상태로 분기하는 관점에서 생각할 수 있습니다.

  


이 직관에 해당하는 방정식과 동작 값 qπ에 대한 다이어그램을 제공하십시오 St = s 및 At = a가 주어질 때 예상되는 다음 보상 Rt + 1 및 예상 된 다음 상태 값 vπ \(St + 1\)의 관점에서, 이 방정식에는 기대치가 포함되어야하지만 방침에 따라 조건이 부여 된 것은 아닙니다. 그런 다음 방정식에 기대 값 표기법이 나타나지 않도록 \(3.6\)에 의해 정의 된 p \(s ', r \| s, a\)의 관점에서 예상 값을 명시 적으로 작성하는 두 번째 방정식을 제공하십시오.

  


3.8 최적 가치 함수

  


강화학습 과제를 해결한다는 것은 대충 장기적으로 많은 보상을받는 정책을 찾는 것을 의미합니다. 유한 MDP의 경우 다음과 같은 방법으로 최적의 정책을 정확하게 정의 할 수 있습니다. 값 함수는 정책에 부분적인 순서를 정의합니다. 정책 π는 모든 국가에서 기대 수익률이 π '보다 크거나 같으면 정책 π'보다 낫거나 같다고 정의된다. 다시 말하면, 모든 s ∈ S에 대해 π ≥ π '이면 v π \(s\) ≥ v π \(s\) 일 경우에만. 항상 다른 모든 정책보다 우수하거나 더 나은 정책이 하나 이상있다. 이것은 최적의 정책입니다. 둘 이상이있을 수 있지만 모든 최적 정책을 π \*로 나타냅니다. 그들은 같은 상태 값 공유

  


\(\| S가\) 확률로 촬영 ⇡

  


의

  


A1 A2

  


v⇡ \(들\) q⇡ \(S, A\)

  


예상 보상을

  


R1

  


, S, A, R2의

  


S02의

  


q⇡ \(S, A\) v⇡은

  


S01의

  


R

  


S0

  


68 제 3 유한 마르코프 결정 기능을 처리는, 최적의 상태 값의 함수로 표시 브이 \*이라하고, 정의

  


\(V \* s의 \) =. maxvπ \(들\), \(3.13\) π

  


모두가 S. ∈ s의

  


최적 정책 같은 최적의 동작 값의 함수로 표시 Q \*를 공유

  


로 정의

  


의 Q \* \(S, A\) =있다.

  


모든 s ∈ S와 a ∈ A \(s\)에 대해 maxqπ \(s, a\), \(3.14\)

  


π . 국가 - 행동 쌍 \(s, a\)에 대해,이 함수는 상태 s에서 행동 a를 취한 후 최적의 정책을 따르는 기대 수익률을 제공한다. 따라서 다음과 같이

  


q \* 를 v \*로 쓸 수 있습니다. q \* \(s, a\) = E \[Rt + 1 + γv \* \(St + 1\) \| St = s, At = a\]. \(3.15\)

  


예제 3.10 : 골프를위한 최적 값 함수 그림 3.6의 아래 부분은 가능한 최적 동작 값 함수 q \* \(s, driver\)의 윤곽을 보여줍니다. 드라이버를 사용하여 처음으로 스트로크를 연주 한 후 드라이버 또는 퍼터 중 더 좋은 것이 선택되면 각 상태의 값입니다. 드라이버를 사용하면 공을 더 멀리 칠 수 있지만 정확도는 떨어집니다. 우리가 이미 아주 가까이에있는 경우에만 드라이버를 사용하여 한 번에 구멍에 도달 할 수 있습니다. 따라서 q \* \(s, driver\)에 대한 -1 등고선은 초록의 작은 부분만을 다룹니다. 그러나 우리가 두 개의 스트로크를 가졌다면 -2 등고선에 표시된 것처럼 훨씬 더 멀리에서 홀에 도달 할 수 있습니다. 이 경우 우리는 작은 -1 컨투어 내에서 녹색으로 만 이동할 수 있습니다. 거기에서 우리는 퍼터를 사용할 수 있습니다. 최적의 동작 값 함수는 특정 첫 번째 동작 \(이 경우에는 드라이버\)에 커밋 한 후에 값을 제공하지만 나중에 가장 적합한 동작을 사용하여 값을 제공합니다. -3 등고선은 여전히 ​​멀리 떨어져 있으며 시작 티를 포함합니다. 티에서 가장 좋은 동작 순서는 2 개의 드라이브와 1 개의 퍼트로 3 개의 스트로크로 공을 가라 앉힌다.

  


v \*는 정책에 대한 가치 함수이기 때문에 State 값 \(3.12\)에 대한 Bellman 방정식에 의해 주어진 자체 일관성 조건을 충족해야합니다. 그러나 최적 값 함수이기 때문에 특정 정책을 참조하지 않고 v \*의 일관성 조건을 특수 형식으로 작성할 수 있습니다. 이것은 v \*에 대한 Bellman 방정식 또는 Bellman 최적 방정식입니다. 직관적으로 Bellman 최적 방정식은 최적 정책 하에서 국가의 가치가

  


  


3.8 과 같아야한다는 사실을 표현합니다 . 최적 값은 기능

  


69

  


그 상태에서 최선의 조치에 대한 기대 수익률 :

  


브이 \* \(들\)

  


= 최대 qπ의 \* \(S, A\) a∈A \(들\)

  


= maxEπ \* \[GT는 \| =에서 세인트 = s의 A\]

  


∞ k 

  


= maxE \* γR S = s, A = a a t + k + 1 tt

  


k = 0 = maxE \* R + γ∞γkR S = s, A = a

  


a + t + k + 2 ttk = 0

  


= maxE \[Rt + 1 + γv \* \(St + 1\) \| St = s, At = a\]

  


\(3.16\) \(3.17\)

  


a = max p \(s ', r \| s, a\) r + γv \* \(s'\).

  


a∈A \(s\) s ', r

  


마지막 두 방정식은 v \*에 대한 Bellman 최적 방정식의 두 가지 형태입니다.

  


Q위한 벨만 최적 방정식 \*입니다

  


' 

  


의 Q \* \(S, A\) = E있는 걸 + 1 + γmaxq \* \(세인트 + 1, a\) 세인트가 S = AA에서'

  


P = \(S ', R \| S, A \) r + γmax q \* \(s ', a'\)이다. s ', ra'

  


그림 3.7의 백업 다이어그램은 v \* 및 q \*에 대한 벨만 최적 방정식에서 고려한 미래 상태와 동작의 범위를 그래픽으로 보여줍니다. 이는 vπ 및 qπ에 대한 백업 다이어그램과 동일합니다. 단, 정책에 따라 예상 값보다 최대 값을 취하도록 에이전트의 선택 지점에 호가 추가되었습니다. 그림 3.7 \(왼쪽\)은 Bellman 최적 성 방정식 \(3.17\)을 그래픽으로 나타냅니다.

  


유한 MDP의 경우 Bellman 최적 방정식 \(3.17\)은 정책과 독립적 인 고유 한 솔루션을 제공합니다. Bellman 최적 방정식은 실제로 각 상태에 대한 방정식 시스템입니다. 따라서 N 상태가있는 경우 N 개의 미지수에 N 방정식이 있습니다. 환경의 역 동성이 알려진다면 \(p \(s ', r \| s, a\)\), 원칙적으로 비선형 방정식의 시스템을 푸는 다양한 방법 중 하나를 사용하여 v \*에 대한이 방정식 시스템을 풀 수 있습니다. q \*에 대한 관련 방정식 세트를 풀 수 있습니다.

  


\(v⇤\)의 최대

  


\(q⇤\) S, A

  


, R의

  


최대

  


AR

  


S0의

  


S0의

  


A0의

  


그림 3.7 : V \*와 Q \*에 대한 백업도

  


70 제 3 장 유한 마르코프 의사 결정 프로세스

  


v \*가 있으면 최적의 정책을 결정하는 것이 상대적으로 쉽습니다. 각각의 상태 s에 대해 벨맨 최적 방정식에서 최대가 얻어지는 하나 이상의 동작이있을 것이다. 이러한 조치에만 0이 아닌 확률을 지정하는 정책은 최적의 정책입니다. 이것을 한 단계 검색으로 생각할 수 있습니다. 최적 값 함수 인 v \*를 사용하면 원 스톱 검색 후에 가장 잘 나타나는 동작이 최적의 동작이됩니다. 이것을 말하는 또 다른 방법은 최적의 평가 함수 v \*에 대해 욕심이있는 정책이 최적의 정책이라는 것입니다. 욕심이라는 용어는 컴퓨터 과학에서 로컬 또는 즉각적인 고려 사항을 기반으로하는 대안을 선택하는 검색 또는 의사 결정 절차를 설명하는 데 사용됩니다. 그러한 선택이 미래의 더 나은 대안으로의 접근을 막을 수있는 가능성을 고려하지 않는다. 결과적으로, 단기적인 결과에만 근거하여 조치를 선택하는 정책을 설명합니다. v \*의 아름다움은 행동의 단기 결과 즉, 한 걸음 씩의 결과를 평가하기 위해이를 사용하면 욕심 많은 정책은 실제로 우리가 관심을 갖는 장기적인 관점에서 최적이라는 것입니다. 왜냐하면 v \* 가능한 모든 미래 행동의 보상 결과를 고려합니다. v \*를 사용하여 최적의 예상 장기 수익률은 각 주마다 지역 및 즉각적으로 사용할 수있는 수량으로 변환됩니다. 따라서 원 스텝 어드밴스 검색은 장기적인 최적의 동작을 산출합니다. v \*의 아름다움은 행동의 단기 결과 즉, 한 걸음 씩의 결과를 평가하기 위해이를 사용하면 욕심 많은 정책은 실제로 우리가 관심을 갖는 장기적인 관점에서 최적이라는 것입니다. 왜냐하면 v \* 가능한 모든 미래 행동의 보상 결과를 고려합니다. v \*를 사용하여 최적의 예상 장기 수익률은 각 주마다 지역 및 즉각적으로 사용할 수있는 수량으로 변환됩니다. 따라서 원 스텝 어드밴스 검색은 장기적인 최적의 동작을 산출합니다. v \*의 아름다움은 행동의 단기 결과 즉, 한 걸음 씩의 결과를 평가하기 위해이를 사용하면 욕심 많은 정책은 실제로 우리가 관심을 갖는 장기적인 관점에서 최적이라는 것입니다. 왜냐하면 v \* 가능한 모든 미래 행동의 보상 결과를 고려합니다. v \*를 사용하여 최적의 예상 장기 수익률은 각 주마다 지역 및 즉각적으로 사용할 수있는 수량으로 변환됩니다. 따라서 원 스텝 어드밴스 검색은 장기적인 최적의 동작을 산출합니다. 최적의 기대 장기 수익률은 각 주마다 지역 및 즉각적으로 사용할 수있는 수량으로 변환됩니다. 따라서 원 스텝 어드밴스 검색은 장기적인 최적의 동작을 산출합니다. 최적의 기대 장기 수익률은 각 주마다 지역 및 즉각적으로 사용할 수있는 수량으로 변환됩니다. 따라서 원 스텝 어드밴스 검색은 장기적인 최적의 동작을 산출합니다.

  


q \*를 사용하면 최적의 작업을 더 쉽게 선택할 수 있습니다. q \*를 사용하면 상담원이 한 걸음 앞선 검색을 수행 할 필요조차 없습니다. 모든 주에 대해 q \* \(s, a\)를 최대화하는 작업을 간단히 찾을 수 있습니다. action-value 함수는 모든 one-step-ahead 검색 결과를 효율적으로 캐시합니다. 최적의 예상 장기 수익을 각 주 - 행동 쌍에 대해 지역적으로 즉시 이용할 수있는 가치로 제공합니다. 따라서 국가의 행동 대신에 국가 - 행동 쌍의 기능을 나타내는 대가로 최적의 행동 가치 함수는 가능한 후계자 상태와 그 가치에 대해 알 필요없이 최적의 행동을 선택할 수있게한다. 환경의 역 동성에 대해 알 필요가 있습니다.

  


예제 3.11 : 재활용 로봇에 대한 Bellman 최적 방정식 \(3.17\)을 사용하여, 재활용 로봇 예제에 대한 Bellman 최적 방정식을 명시 적으로 제공 할 수 있습니다. 상황을 좀 더 간결하게 만들기 위해 상태를 고저로 축약하고 h, l, s, w 및 re로 각각 검색, 대기 및 재충전합니다. 두 가지 상태 만 있기 때문에 Bellman 최적 방정식은 두 방정식으로 구성됩니다. v \* \(h\)에 대한 방정식은 다음과 같이 쓸 수있다.

  


1 \[rw + γv \* \(h\)\] + 0 \[rw + γv \* \(l\)\] 

  


max p \(h \| h, s\) \[r \(h, s \(h, s, l\) + γv \* \(l\)\], p \(h \| h, w\) \[r \(h, w\) \(h, w, l\) + γv \* \(l\)\]

  


v \* \(h\) =

  


= 최대 α \[rs + γv \* \( h\) \(1 

  


\), \(2\), \(3\), \(4\), \(5\)와 같이 표현할 수있다. rw + γv \* \(h\)

  


3.8. v \* \(l\)에 대해 동일한 과정을 수행하면 다음과 같은 방정식을 얻을 수있다.

  


β -3 -3 \* \(l\) = 최대 rw + γv \* \(l\),.

  


γv \* \(H\)

  


모든 RS의 선택, β, α, RW 및 γ를 들어, 함께 0 ≤ γ 

&lt;

1, 0 ≤ α는 ≤ 1 β, 숫자 정확히 한 쌍 브이 \* \(H\) 및 V가 \* \(l\),이 두 비선형 방정식을 동시에 만족시킨다.

  


Example 3.12 : Gridworld 해석 Example 3.8에서 소개하고 그림 3.8 \(왼쪽\)에서 다시 볼 수있는 간단한 그리드 작업에 대한 Bell \* 방정식을 풀 었다고 가정 해 봅시다. 상태 A는 +10의 보상과 상태 A '로의 전환이 뒤 따르는 반면, 상태 B는 +5의 보상과 상태 B'로의 전환이 뒤 따른다는 것을 상기하십시오. 그림 3.8 \(중간\)은 최적 값 기능을 나타내고, 그림 3.8 \(오른쪽\)은 해당 최적 정책을 보여줍니다. 한 셀에 여러 개의 화살표가있는 경우 해당 액션 중 하나가 최적입니다.

  


A

  


A '

  


+10

  


B

  


B'

  


+5

  


22.0

  


19.8

  


17.8

  


16.0

  


24.4

  


22.0

  


19.8

  


17.8

  


22.0

  


19.8

  


17.8

  


16.0

  


19.4

  


17.8

  


16.0

  


14.4

  


17.5

  


16.0

  


14.4

  


13.0

  


14.4

  


16.0

  


14.4

  


13.0

  


11.7

  


a\) Ggrirdidwoorrldld ⇡ b\) vV⇤ \* c\)! ⇤⇤ \*

  


\*

  


\*

  


그림 3.8 : 그리드 월드 예제에 대한 최적 해.

  


Bellman 최적 방정식을 명백하게 해결하면 최적의 정책을 찾아 강화 학습 문제를 해결할 수있는 한 가지 방법이 제공됩니다. 그러나이 솔루션은 거의 직접 유용하지 않습니다. 철저한 검색과 유사하며, 모든 가능성을 앞서고, 예상되는 보상의 관점에서 발생 가능성 및 그 바람직한 가능성을 계산합니다. 이 해법은 실용 상 거의 사실이 아닌 최소한 세 가지 가정에 의존한다. \(1\) 우리는 환경의 역 동성을 정확하게 알고있다. \(2\) 우리는 솔루션의 계산을 완료하기에 충분한 계산 자원을 가지고 있습니다. \(3\) 마르코프 재산. 우리가 관심을 가지고있는 업무의 경우, 일반적으로 이러한 가정의 다양한 조합을 침해하기 때문에이 솔루션을 정확하게 구현할 수 없습니다. 예를 들어, 첫 번째와 세 번째 가정은 주사위 놀이 게임에 문제가 없지만 두 번째 가정은 큰 장애물입니다. 게임은 약 1020 개의 주를 가지고 있기 때문에, v \*에 대한 Bellman 방정식을 풀기 위해 오늘날 가장 빠른 컴퓨터에서 수천 년이 걸릴 것이며, q \*를 찾는 데에도 마찬가지입니다. 강화학습에서는 일반적으로 근사 해를 구해야합니다.

  


  


72 제 3 장 결언 마르코프 결정 과정

  


다양한 의사 결정 방법은 Bellman 최적 방정식을 근사 적으로 해결하는 방법으로 볼 수 있습니다. 예를 들어, 휴리스틱 검색 방법은 \(3.17\)의 오른쪽을 여러 번 깊이있게 확장하여 가능성의 

"

나무

"

를 형성 한 다음 경험적 평가 함수를 사용하여 

"

 잎 

"

노드. \(A \*와 같은 휴리스틱 검색 방법은 거의 항상 에피소드의 경우를 기반으로합니다.\) 동적 프로그래밍의 방법은 Bellman 최적 방정식과 훨씬 더 밀접하게 관련 될 수 있습니다. 많은 강화학습 방법은 Bellman 최적 방정식을 근사 적으로 해결하는 것으로 명확히 이해할 수 있습니다. 예상되는 전이 지식 대신 ​​실제 경험이있는 전이를 사용합니다. 다음 장에서는 다양한 방법을 고려합니다.

  


연습 3.13 골프 예를위한 최적 상태 값 함수를 그리거나 기술하십시오.

  


연습 3.14 골프 예를 위해 q \* \(s, putter\)를 넣기위한 최적의 action-value 함수의 윤곽을 그리거나 기술하십시오.

  


연습 3.15 재활용 로봇에 대한 q \*의 Bellman 방정식을 준다.

  


실습 3.16 그림 3.8은 그리드 월드의 최적 상태의 최적 값을 24.4로 소수점 첫째 자리까지 제공합니다. 최적의 정책에 대한 지식과 \(3.2\)를 사용하여이 값을 상징적으로 표현한 다음 소수 세 자리까지 계산하십시오.

  


3.9 최적 성과 근사

  


우리는 최적의 가치 기능과 최적의 정책을 정의했습니다. 분명히 최적의 정책을 배우는 에이전트는 매우 성공적 이었지만 실제로는 거의 발생하지 않습니다. 우리가 관심을 가지고있는 업무의 종류에 따라 극한의 계산 비용으로 최적의 정책을 생성 할 수 있습니다. 잘 정의 된 최적 성 개념은이 책에서 설명하는 학습 방법을 구성하고 다양한 학습 알고리즘의 이론적 특성을 이해하는 방법을 제공하지만 상담원이 다양한 각도로만 근사 할 수 있다는 것이 이상적입니다. 위에서 논의했듯이, 우리가 환경 역학의 완전하고 정확한 모델을 가지고 있다고하더라도 Bellman 최적 방정식을 풀어 최적의 정책을 계산하는 것은 일반적으로 불가능합니다. 예를 들어, 체스와 같은 보드 게임은 인간 경험의 극히 일부분이지만, 맞춤 설계된 컴퓨터는 여전히 최적의 동작을 계산할 수 없습니다. 에이전트가 직면하는 문제의 중요한 측면은 항상 사용할 수있는 계산 능력, 특히 단일 시간 단계에서 수행 할 수있는 계산량입니다.

  


사용 가능한 메모리 또한 중요한 제약 사항입니다. 가치 기능, 정책 및 모델의 근사치를 구축하려면 많은 양의 메모리가 필요합니다. 유한 상태 집합이 작은 작업의 경우 각 상태 \(또는 상태 - 작업 쌍\)에 대해 하나의 항목이있는 배열 또는 표를 사용하여 이러한 근사를 작성할 수 있습니다. 이를 표 형식의 사례라고 부르고 표기법이라고하는 해당 메서드를 호출합니다. 그러나 실용적인 많은 경우에 테이블에있을 수있는 것보다 훨씬 많은 상태가 있습니다. 이 경우 함수는 대략

  


3.10을 사용하여 근사화되어야합니다 . 요약 73 매개 변수화 된보다 간단한 함수 표현의 일종.

  


강화학습 문제에 대한 우리의 프레임은 우리가 근사치를 해결하도록 강요합니다. 그러나 유용한 근사를 달성 할 수있는 몇 가지 독특한 기회를 제공합니다. 예를 들어, 최적의 행동을 근사화하기 위해 에이전트가 직면하는 여러 가지 상태가있을 수 있으며, 에이전트에 대한 하위 작업을 선택하면 에이전트가받는 보상의 양에 거의 영향을 미치지 않을 가능성이 적습니다. 예를 들어 Tesauro의 주사위 놀이 플레이어는 전문가와의 게임에서 결코 발생하지 않는 보드 구성에서 매우 나쁜 결정을 내릴지라도 뛰어난 기술로 활약합니다. 실제로 TD-Gammon은 게임의 상태 집합의 상당 부분에 대해 나쁜 결정을 내릴 수 있습니다. 강화학습의 온라인 성격은 가끔 발생하는 주에 대한 노력을 줄이면서 자주 접하게되는 주를 위해 올바른 결정을 내리는 데 더 많은 노력을 기울이는 방식으로 최적의 정책을 근사 할 수있게합니다. 이것은 강화학습을 다른 접근법에서 대략적으로 MDP를 해결하는 것과 구분하는 핵심 속성 중 하나입니다.

  


3.10 요약

  


이 장에서 제시 한 강화 학습 문제의 요소를 요약 해 보겠습니다. 강화 학습은 목표를 달성하기 위해 상호 작용에서 학습하는 방법입니다. 강화학습 에이전트와 그 환경은 일련의 이산 시간 단계에 걸쳐 상호 작용합니다. 인터페이스의 스펙은 특정 태스크를 정의합니다. 조치는 에이전트가 선택한 사항입니다. 주정부는 선택을하기위한 기초입니다. 보상은 선택을 평가하는 기초입니다. 에이전트 내부의 모든 것은 에이전트에 의해 완전히 알려지고 제어 가능합니다. 외부 모든 것은 불완전하게 제어 가능하지만 완전히 알려지지 않았을 수도 있습니다. 정책은 에이전트가 상태의 함수로 동작을 선택하는 확률 적 규칙입니다.

  


수익은 에이전트가 최대화하고자하는 미래 보상의 함수입니다. 작업의 성격과 지연된 보상을 할인할지 여부에 따라 여러 가지 정의가 있습니다. 할인되지 않은 배합은 에이전트 - 환경 상호 작용이 자연적으로 에피소드로 분리되는 일회성 작업에 적합합니다. 할인 된 공식은 상호 작용이 자연스럽게 에피소드로 침입하지 않고 제한없이 계속되는 연속 작업에 적합합니다.

  


상태 신호가 미래를 예측하는 능력을 저하시키지 않으면 서 과거를 압축 적으로 요약하면 환경은 Markov 속성을 충족시킵니다. 이것은 거의 사실이 아니지만 종종 그렇습니다. 상태 신호는 Markov 속성이 가능한 한 가깝게 유지되도록 선택되거나 구성되어야합니다. 이 책에서 우리는 이것이 이미 수행되었다고 가정하고 의사 결정 문제에 초점을 맞 춥니 다. 즉, 어떤 상태 신호가 사용 가능 하든지 상관없이 무엇을 해야할지 결정하는 것입니다. Markov 속성이 유지되면 환경을 Markov decision process \(MDP\)라고합니다. 유한 MDP는 유한 상태 및 동작 세트가있는 MDP입니다.

  


강화학습 의 현재 이론의 대부분은 유한 MDPs로 제한되어 있지만, 방법과 아이디어가

  


더 일반적으로 적용됩니다 .

  


정책 값 기능은 에이전트가 정책을 사용하는 경우 각 상태 또는 상태 - 조치 쌍, 해당 상태에서 예상되는 리턴 또는 상태 - 조치 쌍을 할당합니다. 최적 가치 함수는 모든 정책에 의해 달성 가능한 가장 큰 기대 수익을 각 주 또는 상태 - 조치 쌍에 할당합니다. 가치 기능이 최적 인 정책이 최적의 정책입니다. 주 및 국가 - 행동 쌍에 대한 최적 가치 기능이 주어진 MDP에 대해 고유 한 반면에, 많은 최적 정책이있을 수 있습니다. 최적 가치 기능과 관련하여 욕심을 가진 정책은 최적의 정책이어야합니다. Bellman 최적 방정식은 최적의 가치 기능이 충족시켜야하며 최적의 가치 기능을 위해 원칙적으로 해결할 수있는 특수한 일관성 조건이며 최적의 정책을 상대적으로 쉽게 결정할 수 있습니다.

  


강화학습 문제는 초기에 상담원이 사용할 수있는 지식 수준에 대한 가정에 따라 다양한 방법으로 제기 될 수 있습니다. 완전한 지식의 문제에서 에이전트는 환경의 역 동성에 대한 완전하고 정확한 모델을 가지고 있습니다. 환경이 MDP 인 경우, 그러한 모델은 모든 상태와 허용 가능한 조치에 대해 한 단계 전이 확률과 예상 보상으로 구성됩니다. 불완전한 지식의 문제에서 환경의 완벽하고 완벽한 모델을 사용할 수 없습니다.

  


에이전트가 완전하고 정확한 환경 모델을 가지고 있더라도 에이전트는 일반적으로 시간 단계별로 충분한 계산을 수행하여이를 완전히 사용하지 못합니다. 사용 가능한 메모리 또한 중요한 제약 사항입니다. 가치 기능, 정책 및 모델에 대한 정확한 근사치를 구축하려면 메모리가 필요할 수 있습니다. 실용적인 관심의 대부분의 경우에 테이블에 항목이 될 수있는 것보다 훨씬 많은 상태가 있으며 근사값을 만들어야합니다.

  


잘 정의 된 최적 성의 개념은이 책에서 설명하는 학습 방법을 구성하고 다양한 학습 알고리즘의 이론적 특성을 이해하는 방법을 제공하지만 강화학습 에이전트가 다양한 정도에만 근접 할 수 있다는 것이 이상적입니다. 강화 학습에서 우리는 최적 해를 찾을 수 없지만 어떤 방법으로 근사해야만하는 경우에 매우 관심이 있습니다.

  


서지 및 역사적 비고

  


강화학습 문제는 최적 제어 분야에서 마르코프 결정 과정 \(MDPs\)의 아이디어에 깊이 관 심을 기울이고있다. 이러한 역사적 영향과 심리학의 다른 주요 영향은 1 장에서 주어진 간단한 역사에 기술되어있다. 강화학습은 MDPs에 현실적으로 큰 문제에 대한 근사 및 불완전한 정보에 초점을 맞춘다. MDPs와 강화 학습 문제는 인공 지능의 전통적인 학습 및 의사 결정 문제와 약한 연관이 있습니다. 그러나 인공 지능은 다양한 관점에서 기획 및 의사 결정을위한 MDP 공식을 적극적으로 연구하고 있습니다. MDP는 인위적으로 사용 된 이전의 제형보다 일반적입니다

  


3.10. 그들은 더 일반적인 종류의 목표와 불확실성을 허용한다는 점에서 지능을 요약한다.

  


강화학습 문제에 대한 우리의 발표는 Watkins \(1989\)의 영향을 받았다.

  


3.1 생물 반응기의 예는 Ungar \(1990\)와 Miller and Williams \(1992\)의 연구에 기반을두고있다. 재활용 로봇의 예는 조나단 코넬 \(Jonathan Connell, 1989\)이 제작 한 캔 수집 로봇에서 영감을 얻은 것입니다.

  


3.3-4 에피소드 및 계속 작업의 용어는 MDP 문헌에서 일반적으로 사용되는 용어와 다르다. 그 문헌에서 다음과 같은 세 가지 유형의 과제를 구별하는 것이 일반적이다. \(1\) 일정한 고정 된 수의 시간 단계 후에 상호 작용이 종결되는 유한 지평선 과제. \(2\) 상호 작용이 임의로 오래 지속될 수 있지만 결국 종결되어야하는 무기한 지평선 과제; \(3\) 상호 작용이 종결되지 않는 무한 수평선 과제. 우리의 일시적인 그리고 계속되는 임무는 각각 무기한 수평선과 무한 수평선 과제와 유사하지만, 우리는 상호 작용의 성격의 차이점을 강조하는 것을 선호한다. 이 차이는 일반적인 용어로 강조된 객관적인 기능의 차이보다 더 근본적인 것처럼 보입니다.

  


막대 균형 조정의 예는 Michie and Chambers \(1968\)와 Barto, Sutton 및 Anderson \(1983\)에서 나온 것입니다.

  


3.5 국가 개념에 대한 더 자세한 설명은 Minsky \(1967\)를 참조하라.

  


3.6 MDP의 이론은 Bertsekas \(2005\), Ross \(1983\), White \(1969\), Whittle \(1982, 1983\) 등으로 다루어진다. 이 이론은 또한 적응 최적 제어 방법이 강화학습과 가장 밀접한 관계가있는 확률 적 최적 제어 \(예 : Kumar, 1985; Kumar and Varaiya, 1986\)의 제목하에 연구된다.

  


MDP 이론은 불확실성 하에서 일련의 의사 결정 문제를 이해하기위한 노력에서 진화되었으며, 각 결정은 이전의 결정과 결과에 달려있다. 때로는 다단계 의사 결정 이론 또는 순차적 결정 과정의 이론으로 불리며 연속적 샘플링에 관한 통계 자료에 톰슨 \(1933, 1934\)과 로빈스 \(1952\)가 챕터 2에서 언급 한 논문 \(여러 상황 문제로 공식화 된 경우 프로토 타입 MDP 임\).

  


강화학습이 MDP 형식주의를 사용하여 논의 된 최초의 사례는 Andreae \(1969b\)의 학습 기계에 대한 통합 된 관점에 대한 설명입니다. Witten and Corbin \(1973\)은 나중에 Witten \(1977\)이 MDP 형식론을 사용하여 분석 한 강화 학습 시스템을 실험했다. 그가 명시 적으로 MDPS, Werbos를 언급하지 않았지만

  


76

  


제 3 장 유한 마르코프 결정 프로세스

  


3.7-8를

  


\(1977\)은 현대 강화학습 방법과 관련된 확률 적 최적 제어 문제에 대한 근사 해법을 제안했다. \(Werbos, 1982, 1987, 1988, 1989, 1992\). Werbos의 아이디어는 그 당시 널리 알려지지 않았지만 인공 지능을 비롯한 다양한 분야에서 최적의 제어 문제를 근사 적으로 해결하는 것이 중요하다는 점에서 선견지명을 갖고있었습니다. 강화학습과 MDP의 가장 영향력있는 통합은 Watkins \(1989\) 때문이다. MDP 형식주의를 이용한 강화학습에 대한 그의 대우는 널리 채택되었습니다.

  


p \(s ', r \| s, a\)의 관점에서 MDP의 동역학에 대한 우리의 특성은 약간 특이하다. MDP 문헌에서 상태 전이 확률 p \(s '\| s, a\)와 예상되는 다음 보상 r \(s, a\)의 관점에서 역학을 설명하는 것이 더 일반적입니다. 그러나 강화학습에서 우리는 실제 기대치 또는 표본 보상 \(예상 값보다는\)을 참조해야하는 경우가 더 많습니다. 우리의 표기법은 또한 St와 Rt가 일반적으로 공동으로 결정되고, 따라서 동일한 시간 지수를 가져야 함을 명백히한다. 강화 학습을 가르 칠 때, 우리는 더 간단하고 이해하기 쉬운 표기법을 발견했습니다.

  


기생하는 뿌리에서 계속되는 좋은 것의 근거가되는 가치를 배정하십시오. 제어 이론에서 상태를 제어 결정의 장기적인 결과를 나타내는 수치로 매핑하는 것은 1950 년대 고전 역학의 19 세기 국가 기능 이론을 확장함으로써 개발 된 최적 제어 이론의 핵심 부분입니다 \(예 : Schultz and Melsa, 1967\). Shannon \(1950\)은 체스 게임을 위해 컴퓨터를 프로그래밍 할 수있는 방법을 설명하면서 체스 위치의 장기적인 장점과 단점을 고려한 평가 함수를 사용할 것을 제안했습니다.

  


왓킨스 \(1989\) q \*를 추정하기위한 Q- 학습 알고리즘 \(제 6 장\)은 행동 가치 함수를 강화학습의 중요한 부분으로 만들었으며, 따라서 이러한 함수를 종종 Q- 함수라고 부른다. 그러나 행동 가치 기능에 대한 아이디어는 이것보다 훨씬 오래되었습니다. 샤논 \(Shannon, 1950\)은 체스 재생 프로그램이 위치 P에서의 이동 M이 탐험할만한 가치가 있는지 결정하기 위해 함수 h \(P, M\)를 사용할 수 있다고 제안했다. Michie \(1961, 1963\) MENACE 체계와 Michie and Chambers \(1968\) BOXES 체계는 행동 가치 함수를 추정하는 것으로 이해할 수있다. 고전 물리학에서 해밀턴의 주요 기능은 행동 가치 기능이다. 뉴턴 역학은이 기능과 관련하여 욕심 많다 \(Goldstein, 1957\). 행동 가치 함수는 또한 수축 매핑의 관점에서 DP의 이론적 처리 인 Denardo \(1967\)에서 중심적인 역할을 수행했다.

  


v \*에 대한 Bellman 방정식은 

"

기본 함수 방정식

"

이라고 불리는 Richard Bellman \(1957a\)에 의해 처음 소개되었습니다. 연속 시간 및 상태 문제에 대한 Bellman 최적 방정식의 항은 Hamilton- Jacobi-Bellman 방정식 \(또는 해밀턴 - 자 코비 방정식\)은 고전 물리학에서의 뿌리를 나타낸다 \(Schultz and Melsa, 1967\).

  


3.10. 요약 77 골프 예제는 Chris Watkins에 의해 제안되었다.

  


78 제 3 장 FINITE MARKOV 결정 과정

