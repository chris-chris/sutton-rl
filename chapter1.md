# **1 장 강화 학습 문제**

우리가 환경과 상호 작용 함으로써 배우는 아이디어는 아마도 우리가 학습의 본질에 대해 생각할 때 가장 먼저 발생하는 것입니다. 유아가 놀 때, 팔을 흔들거나, 외모를 뚜렷이 나타냅니다. 명백한 교사는 없지만 환경에 직접 감각 운동을합니다. 이 연결을 연습하면 원인과 결과, 행동의 결과 및 목표를 달성하기 위해해야 ​​할 일에 대한 풍부한 정보를 얻을 수 있습니다. 우리의 삶 전체에서 그러한 상호 작용은 의심 할 여지없이 우리의 환경과 우리 자신에 대한 지식의 주요 원천입니다. 자동차를 운전하거나 대화를 나누는 것을 배우 든, 우리는 환경이 우리가하는 것에 어떻게 반응 하는지를 정확히 알고 있으며, 우리의 행동을 통해 일어나는 일에 영향을 미치려고 노력합니다. 상호 작용으로부터 배우는 것은 학습과 지능의 거의 모든 이론을 근간으로하는 기본 아이디어입니다.

이 책에서 우리는 상호 작용으로부터 학습하는 전산 접근법을 탐구합다. 사람이나 동물의 학습 방법을 직접 이론화하기보다는 이상적인 학습 상황을 탐구하고 다양한 학습 방법의 효과를 평가합니다. 즉, 우리는 인공 지능 연구원 또는 엔지니어의 관점을 채택합니다. 우리는 과학적 또는 경제적 관심의 학습 문제를 푸는 데 효과적인 기계 설계를 탐구하고 수학적 분석 또는 계산 실험을 통해 설계를 평가합니다. 강화 학습이라고하는 우리가 탐구하는 접근법은 기계 학습에 대한 다른 접근법보다 목표 지향적 인 상호 작용 학습에 훨씬 더 중점을 둡니다.

## **1.1 강화 학습**

강화 학습은 기계 학습 및 등산과 같은 "ing"으로 끝나는 많은 주제와 마찬가지로 문제의 클래스에서 잘 작동하는 솔루션 방법의 한 종류이자 이러한 문제를 연구하는 분야입니다. lems 및 솔루션 방법. 강화 학습 문제는 무엇을 해야할지 배우기 - 상황을 행동에 어떻게 매핑 할 것인가 - 숫자 재사용을 극대화하기 위해 - 와드 신호. 필수적인 방법으로 이것은 학습 시스템의 행동이 이후 입력에 영향을주기 때문에 폐쇄 루프 문제입니다. 더욱이, 학습자는 다양한 형태의 기계 학습에서와 같이 어떤 행동을 취해야하는지에 대해 알려주지 않고 대신 어떤 행동이 가장 효과적인지 알아내야합니다. 가장 흥미롭고 도전적인 경우 행동은 즉각적인 보상뿐만 아니라 다음 상황과 그 이후의 모든 보상에도 영향을 미칠 수 있습니다. 이 세 가지 특징은 필수적인 방식으로 폐회로가되고 취해야 할 조치에 대한 직접적인 지침이 없으며 보상 신호를 포함한 조치의 결과가 장기간에 걸쳐 실행되는 경우 세 가지 가장 중요한 특징은 다음과 같습니다. 강화학습 문제.

마르코프 결정 과정 \(MDPs\)의 최적 제어에 대한 강화학습 문제의 완전한 명세는 3 장까지 기다려야 만하지만, 기본 아이디어는 단순히 학습 문제를 다루는 학습 에이전트가 직면 한 실제 문제의 가장 중요한 측면을 포착하는 것이다. 환경을 목표로합니다. 분명히 그러한 에이전트는 환경의 상태를 어느 정도 감지 할 수 있어야하며 상태에 영향을 미치는 조치를 취할 수 있어야합니다. 또한 에이전트는 환경 상태와 관련된 목표를 가지고 있어야합니다. MDP 공식은 감각, 행동 및 목표의 세 가지 측면 만 포함시키려는 의도가 있습니다. 이러한 문제를 해결하는 데 적합한 방법으로 우리는 강화학습 방법으로 생각합니다.

강화 학습은 감독 학습 \(supervised learning\)과는 다릅니다. 학습의 종류는 기계 학습 분야의 최신 연구에서 연구되었습니다. 감독 학습은 지식이 풍부한 외부 감독자가 제공 한 예제 레이블로 학습합니다. 각각의 예는 상황이 속하는 범주를 식별하기 위해 시스템이 해당 상황에 대해 취해야하는 올바른 조치의 명세 \(레이블\)과 함께 상황에 대한 설명입니다. 이러한 종류의 학습의 목적은 시스템이 훈련 세트에없는 상황에서 올바르게 반응하도록 응답을 추정하거나 일반화하는 것입니다. 이것은 중요한 학습 유형이지만 혼자서는 상호 작용을 통한 학습에 적합하지 않습니다. 대화식 문제에서는 상담원이 행동해야하는 모든 상황을 정확하고 대표적으로 바라는 바람직한 행동의 예를 얻는 것이 비현실적입니다. 학습자가 가장 유익한 학습을 ​​기대하는 미지의 영역에서 상담원은 자신의 경험을 통해 학습 할 수 있어야합니다.

강화 학습은 기계 학습 연구자가 감독되지 않은 학습이라고 부르는 것과는 다르며, 일반적으로 레이블이 지정되지 않은 데이터의 컬렉션에 숨겨진 구조를 찾는 것입니다

강화학습은 기계 학습 연구자가 감독되지 않은 학습이라고 부르는 것과는 다르며, 일반적으로 레이블이없는 데이터 컬렉션에 숨겨진 구조를 찾는 것입니다. 감독 학습 및 감독되지 않은 학습이라는 용어는 기계 학습 패러다임을 철저히 분류하지만, 그렇지 않습니다. 강화학습이 올바른 행동의 예에 의존하지 않으므로 일종의 무 감독 학습으로 생각하는 유혹을받을 수도 있지만 강화학습은 숨겨진 구조를 찾는 대신 보상 신호를 최대화하려고 시도합니다. 상담원의 경험에서 구조를 밝히는 것은 강화학습에서 확실히 유용 할 수 있지만 강화학습 에이전트의 보상 신호 최대화 문제를 해결하지는 못합니다. 그러므로 우리는 강화학습이 감독 학습 및 감독되지 않은 학습과 함께 세 번째 기계 학습 패러다임이라고 생각합니다.

강화학습에서 발생하는 문제 중 하나는 다른 유형의 학습이 아니라 탐사\(Exploration\)와 착취\(Exploit\) 사이의 절충입니다. 보상을 많이 받기 위해서는 강화 학습 에이전트가 과거에 시도한 행동을 선호하여 보상을 산출하는 데 효과적이라고 판단해야합니다. 그러나 그러한 행동을 발견하기 위해서는 이전에 선택하지 않은 행동을 시도해야합니다.

에이전트는 보상을 얻기 위해 이미 알고있는 것을 활용해야 하지만 앞으로 더 나은 조치를 취하기 위해 탐색해야합니다. 딜레마는 탐사 나 착취가 그 일에 실패하지 않고 독점적으로 추구 될 수 없다는 것입니다. 대리인은 다양한 행동을 시도해야하며 점진적으로 최선의 행동을 보이는 사람을 선호해야합니다. 확률 론적 과제에서 예상되는 보상에 대한 신뢰할 수있는 추정치를 얻으려면 각 행동을 여러 번 시도해야합니다. 탐색 탐구 딜레마는 수십 년 동안 수학자들에 의해 집중적으로 연구되어왔다 \(2 장 참조\). 지금 우리는 탐구와 착취의 균형을 맞추는 모든 문제는 최소한 순수한 형태의 감독과 감독되지 않은 학습에서도 발생하지 않는다는 점에 유의해야합니다.

강화학습의 또 다른 주요 특징은 불확실한 환경과 상호 작용하는 목표 지향 에이전트의 모든 문제를 명시 적으로 고려한다는 점입니다. 이것은 하위 문제가 더 큰 그림에 어떻게 들어갈지를 언급하지 않고 하위 문제를 고려하는 많은 접근법과 대조적이다. 예를 들어, 우리는 기계 학습 연구의 많은 부분이 그러한 능력이 최종적으로 얼마나 유용 할지를 명시 적으로 지정하지 않고 감독 학습과 관련이 있다고 언급했습니다. 다른 연구자들은 일반적인 목표를 가지고 계획하는 이론을 개발했지만 실시간 의사 결정에서의 계획의 역할이나 계획을 세우는데 필요한 예측 모델의 출처를 고려하지 않았습니다. 이러한 접근 방식이 많은 유용한 결과를 가져 왔지만 분리 된 하위 문제에 중점을 두는 것은 중요한 제한 사항입니다.

강화 학습은 완전하고 상호 작용하는 목표 추구 에이전트부터 시작하여 반대 방향으로 나아 간다. 모든 강화학습 에이전트는 명확한 목표를 가지고 있으며, 환경의 측면을 감지 할 수 있으며, 환경에 영향을 줄 수있는 조치를 선택할 수 있습니다. 또한 대개 처음부터 에이전트가 직면하는 환경에 대한 심각한 불확실성에도 불구하고 에이전트가 작동해야한다고 가정합니다. 강화 학습이 계획을 수반 할 때는 계획과 실시간 행동 선택 사이의 상호 작용뿐만 아니라 환경 모델이 어떻게 획득되고 개선되는지에 대한 문제를 다루어야합니다. 강화 학습이 감독 학습을 포함 할 때, 어떤 기능이 중요하고 어떤 기능이 그렇지 않은지를 결정하는 특별한 이유에 따라 강화 학습이 수행됩니다. 학습 연구가 진전을 이루기 위해서는 중요한 하위 문제를 격리하고 연구해야하지만 전체 요원의 모든 세부 사항을 아직 채울 수는 없더라도 완전한 쌍방향 목표 추구 요원에서 명확한 역할을하는 하위 문제 여야합니다.

이제 완전하고 상호 작용하는 목표 추구 에이전트가 완전한 유기체 또는 로봇과 같은 것을 의미하는 것은 아닙니다. 이것들은 명확한 예이지만 완전한 쌍방향의 목표 추구 에이전트는보다 큰 행동 시스템의 구성 요소 일 수 있습니다. 이 경우 에이전트는 대용량 시스템의 나머지 부분과 직접 상호 작용하고 대용량 시스템 환경과 직접 상호 작용합니다. 간단한 예제는 로봇의 배터리 충전 수준을 모니터링하고 로봇의 배터리로 명령을 보내는 에이전트입니다.

제어 아키텍처. 이 에이전트의 환경은 로봇의 환경과 함께 나머지 로봇입니다. 강화학습 프레임 워크의 일반성을 이해하기 위해서는 에이전트와 환경에 대한 가장 분명한 예제를 살펴 봐야합니다.

현대 강화학습의 가장 흥미로운 측면 중 하나는 다른 엔지니어링 및 과학과의 방대하고 유익한 상호 작용입니다

현대 강화학습의 가장 흥미로운 측면 중 하나는 다른 엔지니어링 및 과학 분야와의 견고하고 유익한 상호 작용입니다. 강화 학습은 인공 지능과 기계 학습에서 통계, 최적화 및 기타 수학 과목과의보다 긴밀한 통합을위한 수십 년 간의 추세 중 일부입니다. 예를 들어, 매개 변수화 된 근사를 사용하여 학습하는 일부 강화학습 방법의 기능은 운영 연구 및 제어 이론에서 고전적인 "차원의 저주"를 다룹니다. 보다 명확하게, 재교육 학습은 심리학 및 신경과학과도 강력하게 상호 작용하여 실질적인 이점을 얻게됩니다. 모든 형태의 기계 학습 중에서 강화 학습은 인간과 다른 동물이하는 학습에 가장 가깝습니다. 그리고 강화 학습의 핵심 알고리즘 중 많은 부분은 원래 생물학적 학습 시스템에서 영감을 얻었습니다. 또한 강화 학습은 경험적 데이터의 일부와 더 잘 일치하는 동물 학습의 심리 모델과 뇌의 보상 시스템의 영향력있는 모델을 통해 되돌아 왔습니다. 이 책의 본문은 공학 및 인공 지능과 관련된 강화 학습 아이디어를 개발하고 14 장 및 15 장에 요약 된 심리학 및 신경과학과의 연관성을 설명합니다.

마지막으로 강화학습은 단순한 일반 원칙으로 돌아가는 인공 지능의 큰 추세 중 일부이기도 합니다. 1960 년대 후반부터 많은 인공 지능 연구원들은 발견 할 수있는 일반적인 원칙이 없다는 것을 추측했으며, 대신 특수 목적의 트릭, 절차 및 휴리스틱 스를 엄청나게 소유하고 있기 때문에 인텔리전스가 있다는 것을 알았습니다. 때때로 우리가 기계에 충분한 관련 사실, 즉 1 백만 또는 10 억을 말할 수 있다면 지능화 될 것이라고 말한 경우가 있습니다. 검색이나 학습과 같은 일반 원칙을 기반으로 한 방법은 "약한 방법"으로 특징 지어졌지만 특정 지식을 기반으로하는 방법은 "강력한 방법"이라고 불 렸습니다.이 견해는 오늘날 여전히 일반적이지만 훨씬 덜 일반적입니다. 우리의 관점에서 볼 때, 그것은 시기상조였다. 일반적인 원칙에 대한 탐색에 너무 적은 노력으로 아무 것도 없다고 결론 내렸다. 현대의 인공 지능은 학습, 검색 및 의사 결정의 일반적인 원칙을 찾고 광범위한 지식을 통합하려는 많은 연구를 포함합니다. 진자가 얼마나 멀리 뒤로 휘두르는 지 명확하지 않지만 강화 학습 연구는 확실히 인공 지능의 더 간단하고 일반적인 원리에 대한 스윙의 일부입니다.

## **1.2 예제**

강화학습을 이해하는 좋은 방법은 개발을 유도 한 몇 가지 예와 가능한 응용 프로그램을 고려하는 것입니다.

• 마스터 체스 플레이어가 움직입니다.

선택은 계획 -는 가능한 답장과 반론을 예상하고 즉각적이고 직관적으로

특정 위치와 움직임의 바람직 함을 판단합니다.

• 적응 형 컨트롤러는 석유 정제 작업의 매개 변수를 실시간으로 조정합니다. 컨트롤러는 엔지니어가 원래 제안한 설정치에 엄격히 따르지 않고 지정된 한계 비용을 토대로 생산량 / 비용 / 품질 트레이드 오프를 최적화합니다.

• 가젤 송아지는 태어나 자마자 발로 서있기 조차 힘들어합니다. 30 분 후에 그것은 시속 20 마일로 달리고 있습니다.

• 이동 로봇이 배터리를 충전 할 수있는 곳을 찾기 위해 더 많은 쓰레기를 모으거나 새 방에 들어가야할지 여부를 결정합니다. 배터리의 현재 충전 레벨과 과거에 충전기를 얼마나 쉽고 빠르게 찾을 수 있는지에 따라 결정됩니다.

• Phil은 아침 식사를 준비합니다. 자세히 살펴보면, 심지어 명백하게 평범한 활동조차도 조건부 행동과 연동하는 목표 - 부목 관계의 복잡한 웹을 드러내고 있습니다. 찬장을 걷고, 그것을 열고, 시리얼 박스를 선택한 다음, 상자를 잡고, 파악하고, 검색합니다. 그릇, 숟가락, 우유 주전자를 얻으려면 복잡하고 조정 된 상호 작용 순서가 필요합니다. 각 단계에는 정보를 얻고 도달 및 이동을 안내하기 위해 일련의 안구 운동이 필요합니다. 물체를 운반하는 방법이나 다른 사람들을 얻기 전에 식탁에 물건을 올려 놓는 것이 더 나은지에 대한 신속한 판단이 계속됩니다. 각 단계는 숟가락을 잡거나 냉장고에 들어가는 것과 같은 목표에 따라 진행되며 시리얼이 준비되면 궁극적으로 영양분을 얻는 등의 목적으로 스푼을 먹는 것과 같은 다른 목표를 수행합니다. 그가 그것을 알고 있든 안하든 Phil은 그의 영양 상태, 기아 수준 및 음식 선호도를 결정하는 신체 상태에 대한 정보에 액세스하고 있습니다.

이 예제는 너무 간과하기 쉬운 기본 기능을 공유합니다. 모두는 의사 결정자가 환경에 대한 불확실성에도 불구하고 목표를 달성하고자하는 적극적인 의사 결정권자와 그 환경 사이의 상호 작용을 포함합니다. 에이전트의 행동은 환경의 미래 상태 \(예 : 다음 체스 위치, 정유 공장의 저수준 수준, 로봇의 다음 위치 및 배터리의 미래 충전 수준\)에 영향을 미치므로 사용 가능한 옵션 및 기회에 영향을 미칩니다 나중에 에이전트에게. 올바른 선택은 행동의 간접적이고 지연된 결과를 고려해야하므로 예측이나 계획이 필요할 수 있습니다.

동시에이 모든 예에서 행동의 효과는 완전히 예측 될 수 없다. 따라서 에이전트는 환경을 자주 모니터링하고 적절하게 반응해야합니다. 예를 들어, Phil은 넘쳐나 지 않도록 그의 곡물 그릇에 부은 우유를보아야합니다. 이 모든 사례는 에이전트가 직접적으로 감지 할 수있는 것을 토대로 목표를 향한 진행을 판단 할 수 있다는 점에서 명백한 목표를 포함합니다. 체스 플레이어는 그가이기는 지 여부를 알고, 정련소 컨트롤러는 많은 석유가 생산되고 있으며, 이동 로봇은 배터리가 언제 작동 하는지를 안다

아래로, Phil은 그가 아침 식사를 즐기고 있는지 아닌지 알고 있습니다.

이 모든 예에서 에이전트는 경험을 사용하여 시간이 지남에 따라 성능을 향상시킬 수 있습니다. 체스 선수는 자신의 위치를 ​​평가하는 데 사용하는 직감을 개선하여 자신의 플레이를 향상시킵니다. 가젤 종아리는 그것이 할 수있는 효율성을 향상시킵니다; 필은 아침 식사를 합리화하는 법을 배웁니다. 상담원이 이전에 관련 작업을 경험했거나 설계 또는 진화로 구축 한 경험에서 얻은 지식은 유용하거나 배우기 쉬운 것에 영향을 미칩니다. 그러나 환경과의 상호 작용은 특정 행동을 조정하기 위해 행동을 조정하는 데 필수적입니다. 작업의 특징.**      
**

## **1.3 강화 학습 요소**

에이전트와 환경을 넘어, 강화학습 시스템의 4 가지 주요 하위 요소, 즉 정책, 보상 신호, 가치 기능 및 선택적으로 환경 모델을 식별 할 수 있습니다.

정책은 학습 에이전트가 주어진 시간에 행동하는 방식을 정의합니다. 대충 말하면, 정책은 환경의 인식 된 상태에서 해당 주에서 취할 행동으로의 매핑입니다. 그것은 심리학에서 일련의 자극 - 반응 규칙 또는 연관이라고 부르는 것에 해당합니다 \(자극은 동물 내에서 오는 자극을 포함합니다\). 경우에 따라 정책은 단순한 함수 또는 조회 테이블 일 수 있지만 다른 경우에는 검색 프로세스와 같은 광범위한 계산이 필요할 수 있습니다. 이 정책은 행동을 결정하기에 충분하다는 의미에서 강화학습 에이전트의 핵심입니다. 일반적으로 정책은 확률적일 수 있습니다.

보상 신호는 강화학습 문제에서 목표를 정의합니다. 각 시간 단계에서 환경은 강화학습 에이전트에 하나의 숫자 인 보상을 보냅니다. 에이전트의 유일한 목표는 장기적으로받는 총 보상을 최대화하는 것입니다. 따라서 보상 신호는 에이전트에 대한 양호한 이벤트와 나쁜 이벤트를 정의합니다. 생물학적 시스템에서 우리는 보상이 즐거움이나 고통의 경험과 유사하다고 생각할 수 있습니다. 이들은 에이전트가 직면 한 문제의 즉각적이고 명확한 특징입니다. 따라서, 보상 신호를 생성하는 프로세스는 에이전트에 의해 변경 불가능해야합니다. 에이전트는 프로세스가 직접 생성 한 신호를 변경할 수 있으며 보상 신호는 이들 신호에 따라 달라지기 때문에 환경의 상태를 변경하여 간접적으로 신호를 변경할 수 있지만 신호를 생성하는 기능은 변경할 수 없습니다. 다른 말로 표현하자면, 에이전트는 직면 한 문제를 단순히 다른 문제로 변경할 수 없습니다. 보상 신호는 정책을 변경하기위한 기본 기본입니다. 정책에 의해 선택된 행동 뒤에 낮은 보상이있는 경우 정책은 나중에 그 상황에서 다른 행동을 선택하도록 변경 될 수 있습니다. 일반적으로 보상 신호는 환경 상태 및 취해진 조치의 확률 적 기능 일 수 있습니다. 3 장에서 보상 기능이 동물에 의해 변경 될 수 없다는 아이디어가 동물의 뇌 안에서 보상 신호가 생성되는 생물학에서 보는 것과 어떻게 일치하는지 설명합니다.

보상 신호는 즉각적인 의미에서 좋은 것을 나타내는 반면, 가치 기능은 장기적으로 무엇이 유익한지를 지정합니다. 대충 말하면, 주정부의 가치는 해당 주에서부터 시작하여 직원이 미래에 누적 할 것으로 예상 할 수있는 총 보상 금액입니다. 보상이 환경 상태의 즉각적이고 본질적인 바람직 함을 결정하는 반면, 가치는 따르기 쉬운 국가와 해당 주에서 제공되는 보상을 고려한 후 장기간의 바람직 함을 나타냅니다. 예를 들어, 주에서는 항상 낮은 즉각적인 보상을 산출 할 수 있지만 높은 보상을 제공하는 다른 주 다음으로 정기적으로 이어지기 때문에 여전히 높은 가치를 지닙니다. 또는 그 반대가 사실 일 수 있습니다. 인간 유추를 만들기 위해 보상은 쾌락 \(높으면\)과 고통 \(낮 으면\)과 비슷하지만 가치는 우리 환경이 특정 상태에 있다는 것을 얼마나 기쁘고 불만스럽게하는지에 대한보다 세련되고 원시적인 판단에 해당합니다. 이런 식으로 표현하면 가치 기능이 기본적이고 친숙한 아이디어를 공식화하는 것이 분명해지기를 바랍니다.

보상은 일차적 인 의미에서, 보상의 예측으로서의 가치는 이차적입니다. 보상이 없다면 가치가 없을 수 있으며, 가치를 측정하는 유일한 목적은 더 많은 보상을 얻는 것입니다. 그럼에도 불구하고 결정을 내리고 평가할 때 우리가 가장 염려하는 가치입니다. 행동 선택은 가치 판단에 기초하여 이루어진다. 우리는 장기적으로 우리에게 가장 큰 보상을 얻으므로 가장 높은 보상이 아닌 가장 높은 가치를 지닌 국가에 대한 행동을 추구합니다. 불행히도, 보상을 결정하는 것보다 가치를 결정하는 것이 훨씬 더 어렵습니다. 보상은 기본적으로 환경에 의해 직접적으로 주어 지지만, 에이전트가 전 생애에 걸쳐 관찰 한 일련의 관찰 결과로부터 가치를 추정하고 재평가해야합니다. 사실 우리가 고려하는 거의 모든 강화학습 알고리즘의 가장 중요한 구성 요소는 효율적으로 값을 추정하는 방법입니다. 가치 평가의 중심 역할은 틀림없이 지난 수십 년 동안 우리가 강화학습에 대해 배웠던 가장 중요한 것입니다.

일부 강화학습 시스템의 네 번째이자 마지막 요소는 환경 모델입니다. 이것은 환경의 행동을 모방하거나 더 일반적으로 환경이 어떻게 행동 할 것인가에 대한 추론을 가능하게하는 것입니다. 예를 들어, 상태와 액션이 주어지면, 모델은 결과적인 다음 상태와 다음 보상을 예측할 수 있습니다. 모델은 계획을 세우는 데 사용되며,이를 통해 우리는 죄책감에 의한 행동 방침을 결정하는 어떤 방법을 의미합니다

예를 들어, 상태와 액션이 주어지면, 모델은 결과적인 다음 상태와 다음 보상을 예측할 수 있습니다. 모델은 계획에 사용되며 실제 경험을하기 전에 가능한 미래 상황을 고려하여 조치 과정을 결정하는 방법을 의미합니다. 모델과 계획을 사용하는 강화학습 문제를 해결하는 방법을 모델 기반 방법이라고 부릅니다. 모델을 사용하지 않는 간단한 방법 인 명백히 시행 착오적 인 학습자는 계획과 거의 반대입니다. 8 장에서는 시행 착오를 동시에 배우고, 환경 모델을 학습하고, 계획을 위해 모델을 사용하는 강화학습 시스템을 탐구합니다. 현대의 강화 학습은 저레벨의 시행 착오적 학습에서 높은 수준의 심의 적 계획에 이르기까지 다양합니다.

## **1.4 한계와 범위**

이 책에서 우리가 고려하는 강화학습 방법의 대부분은 가치 함수를 추정하는 것으로 구성되어 있지만, 반드시 그렇게 할 필요는 없습니다.

강화 학습 문제를 해결하십시오. 예를 들어 유전 알고리즘, 유전 프로그래밍, 시뮬레이션 어닐링 및 기타 최적화 방법과 같은 방법을 사용하여 가치 기능을 호소하지 않으면 서 강화학습 문제에 접근했습니다. 이 방법은 환경과 상호 작용하기 위해 다른 정책을 사용하는 많은 비 학습 에이전트의 "평생"행동을 평가하고 가장 많은 보상을 얻을 수있는 에이전트를 선택합니다. 우리는 생물학적 진화가 개인의 생애 동안 배우지 않더라도 숙련 된 행동을하는 유기체를 생산하는 방식과 유사하기 때문에 진화론 적 방법이라고 부릅니다. 정책의 공간이 충분히 작거나 좋은 정책이 일반적이거나 찾기 쉽도록 구조화 될 수 있거나 검색에 많은 시간이 소요되는 경우 진화 적 방법이 효과적 일 수 있습니다. 또한 진화 방법은 학습 에이전트가 환경 상태를 정확하게 감지 할 수없는 문제에 이점이 있습니다.

우리의 초점은 환경과 상호 작용하면서 학습과 관련된 강화학습 방법에 초점을두고 있습니다. 진화 방법은하지 않습니다 \(연구 된 접근법의 일부처럼 학습 알고리즘을 발전시키지 않는 한\). 개인적인 행동 상호 작용의 세부 사항을 이용할 수있는 방법은 많은 경우 진화 방법보다 훨씬 효율적일 수 있다는 우리의 믿음입니다. 진화론 적 방법은 강화 학습 문제의 유용한 구조의 많은 부분을 무시한다. 그들은 그들이 찾고있는 정책이 국가에서 행동으로가는 기능이라는 사실을 사용하지 않는다. 그들은 평생 동안 어떤 개인이 통과했는지 또는 어떤 행동을 선택 하는지를 알지 못합니다. 어떤 경우에는이 정보가 오도 될 수 있습니다 \(예 : 상태가 잘못 인식 된 경우\). 그러나 더 자주 검색을 수행해야하는 경우가 있습니다. 진화와 학습은 많은 특징을 공유하고 자연스럽게 함께 작동하지만, 진화론 적 방법 자체가 학습 문제를 강화시키는 데 특히 적합하다고는 생각하지 않습니다. 단순함을 위해이 책에서 "강화 학습 방법"이라는 용어를 사용할 때는 진화 적 방법을 포함하지 않습니다.

그러나 진화론 적 방법과 마찬가지로 가치 기능을 호소하지 않는 몇 가지 방법이 있습니다. 이 메소드는 수치 매개 변수 집합으로 정의 된 정책 공간을 검색합니다. 그들은 정책의 성과를 가장 빠르게 개선하기 위해 매개 변수를 조정해야하는 방향을 예측합니다. 그러나 진화론적인 방법과는 달리, 에이전트가 환경과 상호 작용하는 동안 이러한 견적을 산출하므로 개개인의 행동 상호 작용에 대한 세부 정보를 이용할 수 있습니다. 정책 기울기 방법이라고하는 이와 같은 방법은 많은 문제에서 유용함이 입증되었으며, 가장 간단한 강화학습 방법 중 일부가이 범주에 속합니다. 실제로 이러한 방법 중 일부는 값 함수 추정치를 활용하여 기울기 추정치를 향상시킵니다. 전반적으로, 정책 구배 방법과 강화학습 방법으로 포함하는 다른 방법의 차이는 크게 정의되지 않습니다.

강화학습의 최적화 방법과의 연결은 일반적인 오해의 원인이기 때문에 추가적인 언급이 필요합니다. 강화학습 에이전트의 목표가 수치적인 보상 신호를 최대화하는 것이라고 말할 때, 우리는 물론 에이전트가 실제로 목표를 달성해야한다고 주장하지 않습니다

최대 보상. 수량을 최대화하려고한다고해서 그 수량이 최대화되는 것은 아닙니다. 요점은 강화학습 에이전트가 항상받는 보상의 양을 늘리려고한다는 것입니다. 많은 요소가 있어도 최대 값을 달성하지 못할 수 있습니다. 즉, 최적화는 최적화와 동일하지 않습니다.

최적화 방법이 최적 성을 달성했는지 여부에 관계없이 최적화를 기반으로 한 인공 지능 시스템을 설계하려면 이러한 시스템의 동작이 항상 예측 가능하지 않기 때문에주의가 필요합니다. 강화 학습 에이전트는 환경에 보상을 제공하는 예기치 않은 방법을 때로는 발견합니다. 한 가지 관점에서 이것은 지능의 바람직한 속성입니다. 그것은 일종의 독창성입니다. 진화와 강화학습의 본질 인 변이와 선택에 기반한 과정은 동물 개체군이나 인공 지능이 직면 한 모든 도전에 대한 성공의 새로운 길을 발견 할 수 있습니다. 그러나 이러한 예상치 못한 "해결책"이 의도하지 않은 바람직하지 않은 결과를 초래하지 않도록하는 방법에 대한 중요한 문제를 야기합니다. 이 관심은 강화학습을하는 경우 거의 새로운 것이 아닙니다. 그것은 문학의 주요 테마입니다 \(예 : Goethe의 1797 년시 "The Sorcerer 's Apprentice"\). "당신이 그것을 얻을 수도 있기 때문에 당신이 바라는 것을 조심하십시오!"라는 트로피에 의해 요약됩니다. 최적화 중 제약 조건을 적용하거나 최적화를 위험에 민감하게 만드는 목적 함수를 조정하는 것과 같이이 문제의 심각성은 부분적인 해결책 일뿐입니다. 표준 엔지니어링 관행은 제품, 구조물 또는 안전한 성능을 갖춘 실제 시스템을 구축 할 때 그 결과를 사용하기 전에 오랜 시간 동안 최적화 과정의 결과를 면밀히 검토해야했습니다. 이것은 또한 강화 학습의 엔지니어링 사용에있어 필수적인 실습이며, 강화 학습 시스템이 강화 학습 에이전트 뿐만이 아니라 예기치 못한 결과를 수용 할 수없는 영역에 온라인으로 배포되는 경우 특별한주의가 필요합니다. 에이전트 환경 및 그 안에있는 사람들. 인공 지능의 빠른 속도, 특히 기계 학습 시스템이 특정 영역에서 초인적 인 성능을 가능하게함으로써 이러한 우려를 전면에 내고 있습니다.

최적화의 예측 불가능 성은 강화학습 시스템이 현실 세계에서 책임감있게 전개 될 수있는 방법의 광범위한 주제 중 하나 일뿐입니다. 이는 다시 다른 엔지니어링 기술에 관한 동일한 우려와 크게 다르지 않으며 원하지 않는 결과의 위험을 완화하기위한 많은 방법이 개발되었습니다. 최적의 제어 방법을 적용 할 때 위험을 완화하기위한 접근법이 특히 적합합니다. 그 중 일부는 강화학습에 적용되었습니다. 이것은 입문서에서 다루려고 시도하는 것 이상으로 많은 차원을 가진 크고 복잡한 주제입니다. 그러나 우리는 공학 방법론으로 취급 될 때 학습 및 지능에 관한 이론뿐만 아니라 강화학습이 모든 엔지니어링 방법론의 적용을 안내하는 모든주의 사항의 적용을 받음을 너무나 강하게 강조 할 수는 없습니다.

## ** 1.5 확장 된 예 : Tic-Tac-Toe**

![](https://lh6.googleusercontent.com/NbuHqBnswBxfLUdmR7-7RsdUOWaKRJWfS_a33fML-uDS5MOLrRJL6vx4B_-I2mBK79OBSB7IrHrFgDHBfLqKwmBtoZibAFe5ZoGOzqjQm2FJkfNwMJTO-NUv4gAOvTjNwsPyFWGu)

강화학습의 일반적인 개념을 설명하고 다른 접근법과 대비하기 위해 다음으로 한 가지 예를 자세히 살펴 봅니다.

친숙한 아이들의 tic-tac-toe 게임을 생각해보십시오. 두 명의 플레이어가 3x3 보드에서 번갈아 노는 게임입니다. X 플레이어가 게임의 오른쪽 그림과 같이 가로, 세로 또는 대각선으로 연속적으로 3 개의 마크를 배치하여 한 플레이어가 승리 할 때까지 한 플레이어는 Xs와 다른 Os를 플레이합니다. 보드가 3 명이 연속으로 채워지지 않으면 게임은 무승부입니다. 숙련 된 플레이어는 절대 잃어 버릴 수없는 플레이를 할 수 있기 때문에, 때때로 우리의 플레이가 부정확하고 우승 할 수없는 불완전한 플레이어와 플레이하고 있다고 가정합시다. 사실상 우리는 무승부와 손실이 우리에게 똑같이 나쁘다고 생각합시다. 우리는 상대 선수의 경기에서 불완전 함을 발견하고 우승 확률을 극대화하는 방법을 배우는 플레이어를 어떻게 구성 할 수 있습니까?

이것은 간단한 문제이지만, 고전적 기술을 통해 만족스럽게 해결 될 수는 없습니다. 예를 들어, 게임 이론의 고전적인 "미니 맥스 \(minimax\)"해법은 상대방이 특정한 방법으로 게임을한다는 가정하에 올바르지 않습니다. 예를 들어, 미니 맥스 플레이어는 상대방에 의한 부정확 한 플레이로 인해 실제로 그 상태에서 항상 승리하더라도, 잃을 수있는 게임 상태에 결코 도달하지 못합니다. 동적 프로그래밍과 같은 순차적 결정 문제에 대한 고전적인 최적화 방법은 모든 상대방에게 최적의 솔루션을 계산할 수 있지만 상대방이 각 보드 상태에서 각 이동을 할 확률을 포함하여 해당 상대의 완전한 사양을 입력으로 요구합니다. 실용적인 관심사의 대부분의 문제가 아니므로이 정보는이 문제에 선험적으로 사용할 수 없다고 가정합시다. 반면에, 그러한 정보는 경험을 통해 추측 될 수 있습니다.이 경우 상대방과 많은 경기를합니다. 이 문제에 대해 할 수있는 최선의 방법은 상대방 행동의 모델을 먼저 신뢰 수준까지 배우고 대략적인 상대 모델을 고려한 최적의 솔루션을 계산하기 위해 동적 프로그래밍을 적용하는 것입니다. 결국, 이것은 우리가이 책의 뒷부분에서 다룰 강화학습 방법 들과는 다르지 않습니다.

이 문제에 적용된 진화론 적 방법은 상대방을 상대로 높은 확률로 가능성있는 정책 공간을 직접 탐색합니다. 여기 정책은 플레이어가 3 x 3 보드에서 Xs 및 Os를 구성 할 때마다 게임의 모든 상태에 대해 어떤 움직임을 만들지를 알려주는 규칙입니다. 고려 된 각 정책에 대해, 상대방에 대해 몇 가지 게임을하면 승리 확률을 구할 수 있습니다. 이 평가는 다음에 고려 된 정책을 지시합니다. 전형적인 진화 방법은 정책 공간을 넓히고 증분 개선을 시도하기 위해 정책을 연속적으로 생성 및 평가합니다. 또는 정책 모집단을 유지하고 평가하는 유전자 양식 알고리즘을 사용할 수도 있습니다. 문자 그대로 수백 가지의 다른 최적화 방법을 적용 할 수 있습니다.

다음은 값 함수를 사용하는 방법으로 tic-tac-toe 문제에 접근하는 방법입니다. 먼저 게임의 각 가능한 상태에 대해 하나씩 숫자 표를 설정합니다. 각 숫자는 해당 state에서 우승 할 확률의 최신 예상치입니다. 이 추정치를 state의 값으로 취급하고 전체 표는 학습 된 가치 함수입니다. 상태 A는 상태 B보다 높은 가치를 가지고 있거나 A에서 얻은 확률의 현재 예상치가 B보다 높은 경우 "상태 B"보다 "더 좋음"으로 간주됩니다. 우리는 항상 X를 플레이한다고 가정하고 모든 상태에 대해 우리가 이미 이기기 때문에 3 개의 X가 연속으로이기는 확률은 1입니다. 유사하게, 3 개의 Os가 연속적으로 있거나 또는 "채워지는"모든 상태에 대해 올바른 확률은 0입니다. 우리가 이길 수는 없습니다. 우리는 다른 모든 상태의 초기 값을 0.5로 설정하여 우리가 승리 할 확률이 50 %라고 추측합니다.

우리는 상대방과 많은 경기를합니다. 우리의 움직임을 선택하기 위해 우리는 각각의 가능한 움직임 \(보드의 각 빈 공간에 하나씩\)으로 인한 상태를 검사하고 테이블에서 현재 값을 찾습니다. 대부분 우리는 탐욕스럽게 움직이며, 가치가 가장 큰 상태 즉, 가장 높은 예상 확률로 이끄는 움직임을 선택합니다. 그러나 때로는 다른 동작 중에서 무작위로 선택합니다. 이들은 우리가 결코 볼 수없는 상태를 경험하게하기 때문에 탐험적인 움직임이라고 불립니다. 게임 중에 만들어지고 고려되는 일련의 동작은 그림 1.1과 같이 다이어그램으로 나타낼 수 있습니다.

우리가 노는 동안 우리는 게임 도중 자신을 발견 한 state의 가치를 바꿉니다. 우리는 승리 확률에 대한보다 정확한 추정을 시도합니다. 이렇게하기 위해, 우리는 그림 1.1의 화살표에 의해 제시된 것처럼 움직이기 전의 상태로 각 탐욕스러운 행동을 한 후에 국가의 가치를 "백업"합니다. 보다 정확하게는, 이전 상태의 현재 값은 나중 상태의 값에 더 가깝도록 조정된다. 이것은 이전 상태의 값을 나중에 상태의 값쪽으로 이동시키는 방법으로 수행 할 수 있습니다. 만약 우리가 s를 욕심 거리는 움직임 이전의 상태로, s '를 이동 한 후에 상태로 나타내면 V \(s\)로 표시된 s의 추정 된 값을 갱신하면 다음과 같이 쓸 수있다.

V \(s\) ← V \(s\) + αV \(s '\) - V \(s\),

여기서 α는 학습의 속도에 영향을 미치는 step-size 매개 변수라고하는 작은 양수 소수입니다. 이 업데이트 규칙은 시간차 학습 방법의 한 예이며, 두 번의 서로 다른 시간에 추정치 간의 차이 인 V \(s '\) - V \(s\)를 기반으로 변경되기 때문에 호출됩니다.

위에서 설명한 방법은이 작업을 아주 잘 수행합니다. 예를 들어 step-size 매개 변수가 시간이 지남에 따라 적절하게 감소되면 \(35 페이지 참조\),이 방법은 고정 된 상대방에 대해 플레이어가 최적으로 플레이 한 각 국가에서 승리 할 실제 확률에 수렴합니다. 게다가, 그때 취해진 움직임 \(탐험적인 움직임 제외\)은 사실이 \(불완전한\) 상대방에 대한 최적의 움직임이다. 즉,이 방법은이 상대방과 게임을하기위한 최적의 정책에 수렴합니다. step-size 매개 변수가 시간이 지남에 따라 0으로 줄이지 않으면이 플레이어는 또한 천천히 자신의 플레이 방식을 변경하는 상대방에게 잘 맞 춥니다.

이 예는 진화론 적 방법과 가치 기능을 배우는 방법의 차이점을 설명합니다. 정책을 평가하기 위해 진화론 적 방법은 정책을 고정시키고 상대방에 대해 많은 게임을하거나 상대방의 모델을 사용하여 많은 게임을 시뮬레이션합니다. 승리의 빈도는 해당 정책으로 승리 할 확률에 대한 편견없는 추정치를 제공하며 다음 정책 선택을 지시하는 데 사용될 수 있습니다. 그러나 각 정책 변경은 많은 게임 후에 만 ​​이루어지며 각 게임의 최종 결과 만 사용됩니다. 게임 도중 발생하는 상황은 무시됩니다. 예를 들어, 플레이어가 이기면 특정 동작이 승리에 얼마나 중요한지 독립적으로 게임의 모든 동작에 크레딧이 부여됩니다. 결코 발생하지 않은 움직임에 대해서도 신용을 부여합니다! 반대로 값 함수 방법은 개별 상태를 평가할 수있게합니다. 결국, 진화와 가치 기능 방법은 모두 정책 공간을 탐색하지만, 가치 기능을 학습하는 것은 놀이 과정에서 이용 가능한 정보를 이용한다.

이 간단한 예제는 강화 학습 방법의 주요 기능 중 일부를 보여줍니다. 첫째, 환경과 상호 작용하면서 학습하는 것에 중점을 두는 것입니다 \(이 경우 상대 플레이어와 함께\). 둘째, 분명한 목표가 있으며 올바른 행동을 위해서는 선택의 지연된 영향을 고려한 계획이나 선견지사가 필요합니다. 예를 들어, 단순 강화학습 플레이어는 근시안적 인 상대를 위해 멀티 이동 트랩을 설정하는 법을 배웁니다. 그것은의 눈에 띄는 기능입니다

그림 1.1 : tic-tac-toe가 움직이는 순서. 실선은 게임 중에 취한 동작을 나타냅니다. 점선은 우리 \(우리의 강화 학습 학습자\)가 고려했지만하지 않은 움직임을 나타냅니다. 우리의 두 번째 움직임은 탐구적인 움직임이었습니다. 즉, 다른 형제 동행, e \*로 이끄는 형제가 더 높은 순위에 올랐지 만 취해진 것입니다. 탐험적인 움직임은 어떤 학습도 가져 오지 않지만, 우리의 다른 움직임들 각각은 굽은 화살표에 의해 제안되고 텍스트에서 자세히 설명 된 것처럼 백업을 일으 킵니다.

상대방의 모델을 사용하지 않고 미래의 상태와 행동의 가능한 순서에 대해 명시 적으로 검색하지 않고도 계획 및 미리보기의 효과를 얻을 수있는 강화학습 솔루션입니다.

이 예제는 강화 학습의 주요 기능 중 일부를 보여 주지만 강화 학습이 실제보다 더 제한적인 인상을 줄 수있을만큼 간단합니다. tic-tac-toe는 두 사람 게임이지만 강화 적 학습은 외부 적 대가가없는 경우, 즉 "자연과의 게임"의 경우에도 적용됩니다. 강화 학습은 문제에 국한되지 않습니다 이 게임에서는 각각의 에피소드가 끝날 때마다 보상과 함께 별도의 게임 인 tic-tac-toe와 같은 별도의 에피소드로 구분됩니다. 행동이 무기한으로 지속될 때와 언제든지 다양한 등급의 보상을받을 수있는 것과 마찬가지로 적용 할 수 있습니다. 강화 학습은 tic-tac-toe의 플레이와 같이 이산 시간 단계로 분해되지 않는 문제에도 적용됩니다.

Tic-tac-toe는 상대적으로 작고 유한 상태 집합을 가지지 만 강화학습은 상태 집합이 매우 크거나 심지어 무한 할 때 사용될 수 있습니다. 예를 들어, Gerry Tesauro \(1992, 1995\)는 위에서 설명한 알고리즘을 인공 신경 네트워크와 결합하여 약 1020 개의 상태를 가진 주사위 놀이를하는 법을 배웠습니다. 이 많은 주에서는 그 중 일부를 경험하는 것이 불가능합니다. Tesauro의 프로그램은 이전 프로그램보다 훨씬 뛰어 났으며 이제는 세계 최고의 인간 선수 수준에서 뛰게됩니다 \(16 장 참조\). 신경 네트워크는 프로그램의 경험을 바탕으로 일반화 할 수있는 기능을 제공하므로 새로운 상태에서 네트워크에 의해 결정된 과거의 유사한 상태에서 저장된 정보를 기반으로 이동을 선택합니다. 그러한 큰 주 \(state\) 집합에 대한 문제에서 강화학습 시스템이 얼마나 잘 작동 할 수 있는지는 과거의 경험으로부터 얼마나 적절하게 일반화 될 수 있는지에 밀접하게 관련되어 있습니다. 이 역할에서 우리는 강화 학습을 통해 감독 된 학습 방법을 가장 많이 필요로합니다. 신경망과 깊은 학습 \(9.6 절\)만이 유일하게, 또는 반드시 최선의 방법이 아닙니다.

이 tic-tac-toe 예제에서 학습은 게임의 규칙을 초월한 사전 지식없이 시작되었지만 강화 학습은 결코 학습과 지능에 대한 시각을 필요로하지 않습니다. 반대로 사전 정보는 효과적인 학습을 위해 중요 할 수있는 다양한 방법으로 강화 학습에 통합 될 수 있습니다. 또한 tic-tac-toe 예제에서 실제 상태에 액세스 할 수 있었지만, 상태 학습의 일부가 숨겨져 있거나 다른 상태가 학습자에게 동일하게 나타날 때 강화학습을 적용 할 수도 있습니다. 그러나이 경우는 상당히 어려워이 책에서 다루지 않습니다.

마지막으로, tic-tac-toe 플레이어는 가능한 모든 동작으로 인해 발생할 상태를 미리 파악할 수있었습니다. 이를 위해서는 게임의 환경을 결코 바꿀 수없는 움직임에 대응하여 어떻게 변화 시킬지 "생각할" 수 있는 게임 모델이 있어야했습니다. 많은 문제들이 이와 같지만, 다른 것들에서는 행동의 효과에 대한 단기적인 모델조차도 부족합니다. 강화 학습은 어느 경우에도 적용될 수 있습니다. 모델은 필요하지 않지만 모델을 쉽게 사용할 수 있거나 학습 할 수있는 경우 모델을 쉽게 사용할 수 있습니다.

반면에, 어떤 종류의 환경 모델도 전혀 필요로하지 않는 강화학습 방법이 있습니다. 모델이없는 시스템은 단일 작업에 대응하여 환경이 어떻게 변할지 생각조차 할 수 없습니다. tic-tac-toe 플레이어는 상대방과 관련하여 이런면에서 모델이 없습니다. 상대방에 대한 모델이 없습니다. 모델은 유용하기에 합리적으로 정확해야하기 때문에 모델을 사용하지 않는 방법은 문제를 해결하기위한 실제 병목 현상이 충분히 정확한 환경 모델을 구성하는 것이 어려울 때보다 복잡한 방법보다 장점을 가질 수 있습니다. 모델없는 방법은 모델 기반 방법의 중요한 빌딩 블록이기도합니다. 이 책에서 우리는 더 복잡한 모델 기반 방법의 구성 요소로 어떻게 사용될 수 있는지 토론하기 전에 모델없는 방법에 대해 여러 장을 다룬다.

강화 학습은 시스템에서 높은 수준과 낮은 수준 모두에서 사용할 수 있습니다. tic-tac-toe 플레이어는 게임의 기본 동작에 대해서만 배웠지 만 강화학습이 더 높은 수준에서 작동하는 것을 방지하지 못합니다. 각 "동작" 자체가 가능한 정교한 문제 해결의 응용 프로그램이 될 수 있습니다 방법. 계층 적 학습 시스템에서 강화 학습은 여러 수준에서 동시에 작동 할 수 있습니다.**  
**

연습 1.1 : 셀프 플레이 \(Self-Play\) 무작위로 상대방을 상대하는 대신 위에서 설명한 강화 학습 알고리즘이 양측 학습으로 진행되었다고 가정 해보십시오. 이 경우에 어떻게 될 것이라고 생각하십니까? 그것은 움직임을 선택하는 다른 정책을 배우겠습니까?

연습 1.2 : 대칭 많은 tic-tac-toe 위치는 다르게 나타나지만 대칭 때문에 실제로 동일합니다. 우리는 이것을 활용하기 위해 위에서 설명한 학습 과정을 어떻게 수정할 수 있습니까? 이 변화는 어떤면에서 학습 과정을 개선 할 것입니까? 이제 다시 생각해보십시오. 상대방이 대칭을 이용하지 않는다고 가정 해보십시오. 그렇다면 우리가해야할까요? 대칭 적으로 동등한 지위가 반드시 같은 가치를 가져야한다는 것은 사실입니까?

연습 1.3 : 욕심쟁이 놀이 강화 학습 학습자가 욕심이 많았다 고 가정 해 봅시다. 즉, 항상 최상으로 평가 된 위치로 이동했습니다. 비협조적인 연주자보다 더 잘하거나 더 나을 것을 배울 수 있습니까? 어떤 문제가 발생할 수 있습니까?

연습 1.4 : 탐험 학습 학습 탐구를 포함하여 모든 동작 후에 학습 업데이트가 발생했다고 가정합니다. step-size 매개 변수가 시간이 지남에 따라 적절히 감소되면 \(탐색하려는 경향이 아니라\), 상태 값은 확률 집합으로 수렴합니다. 우리가 할 때 계산되는 두 세트의 확률은 무엇이며, 그렇지 않을 때 탐험 움직임으로부터 배웁니다. 우리가 탐험적인 움직임을 계속한다고 가정하면 어떤 확률 집합을 배우는 것이 더 낫겠습니까? 어느 것이 더 많은 승리를 가져다 줄까요?

운동 1.5 : 기타 개선 사항 강화 학습 학습자를 향상시키는 다른 방법을 생각해 볼 수 있습니까? 틱택 페이스 문제를 해결할 더 좋은 방법을 생각해 낼 수 있습니까?

## **1.6 요약**

강화 학습은 목표 지향 학습 및 의사 결정을 이해하고 자동화하는 전산적인 접근 방식입니다. 모범적 인 감독이나 환경의 완전한 모델에 의존하지 않고 에이전트가 환경과의 직접적인 상호 작용을 학습하는 것에 중점을 두어 다른 계산 방법과 구별됩니다. 우리의 견해로는 강화학습은 장기 목표를 달성하기 위해 환경과의 상호 작용을 통해 학습 할 때 발생하는 계산 문제를 해결하기위한 첫 번째 분야입니다.

강화 학습은 상태, 행동 및 보상 측면에서 학습 에이전트와 그 환경 간의 상호 작용을 정의하는 공식 프레임 워크를 사용합니다. 이 프레임 워크는 인공 지능 문제의 필수 기능을 나타내는 간단한 방법입니다. 이러한 특징에는 원인과 결과의 감각, 불확실성과 비결정론의 의미, 명확한 목표의 존재가 포함됩니다.

가치와 가치 함수의 개념은이 책에서 우리가 고려하는 대부분의 강화학습 방법의 주요 특징입니다. 우리는 정책 공간에서 효율적인 검색을 위해 가치 기능이 중요하다는 입장을 취합니다. 가치 함수의 사용은 강화학습 방법과 전체 정책의 스칼라 평가에 의해 유도 된 정책 공간에서 직접 검색하는 진화 방법을 구별합니다.

## **1.7 강화 학습의 역사**

강화 학습의 역사에는 근본적인 강화 학습에서 얽히게하기 전에 독자적으로 추구 된 두 가지 긴 스레드와 부유 스레드가 있습니다. 한 가지 실마리는 동물 학습의 심리학에서 시작된 시행 착오에 의한 학습에 관한 것이다. 이 스레드는 인공 지능에서 초기 작업 중 일부를 실행하고 1980 년대 초 강화 학습의 부활로 이어졌습니다. 다른 스레드는 최적의 제어 문제와 가치 함수 및 동적 프로그래밍을 사용하는 솔루션에 관한 문제입니다. 대부분이 부분은 학습과 관련이 없습니다. 두 스레드가 크게 독립적 이었지만 예외는이 장의 tic-tac-toe 예제에서 사용 된 것과 같은 시간 차이 방법과 관련하여 덜 분명한 세 번째 스레드를 중심으로 회전합니다.

시행 착오적 학습에 초점을 맞추는 실은 우리가 가장 익숙하고이 간단한 역사에서 가장 많이 이야기하고있는 실습입니다. 그러나이를 수행하기 전에 최적 제어 스레드에 대해 간략히 논의합니다. "최적 제어"라는 용어는 1950 년대 후반에 동적 시스템의 동작을 시간에 따라 최소화하는 컨트롤러를 설계하는 문제를 설명하기 위해 사용되었습니다. 이 문제에 대한 접근법 중 하나는 1950 년대 중반 Richard Bellman과 19 세기 Hamilton과 Jacobi의 이론을 확장하여 다른 사람들에 의해 개발되었습니다. 이 접근법은 동적 시스템 상태의 개념을 사용합니다.      


가치 함수 또는 "최적의 반환 함수"를 사용하여 함수식을 정의 할 수 있습니다.이 방정식은 현재 벨만 방정식이라고도합니다. 이 방정식을 풀어서 최적의 제어 문제를 푸는 방법은 동적 프로그래밍 \(Bellman, 1957a\)으로 알려져 있습니다. Bellman \(1957b\)은 또한 Markovian decision processes \(MDPs\)로 알려진 최적 제어 문제의 이산 확률론적인 버전을 소개했으며, Ronald Howard \(1960\)는 MDPs에 대한 정책 반복 방법을 고안했습니다. 이 모든 것들은 근대 강화 학습의 이론과 알고리즘을 뒷받침하는 필수 요소입니다.

      


동적 프로그래밍은 일반적으로 확률 적 최적 제어 문제를 해결할 수있는 유일한 방법으로 널리 간주됩니다. Bellman이 "차원의 저주 \(delse of dimensionality\)"라고 불렀던 것에서봤을 때, 계산 요구 사항이 상태 변수의 수에 따라 기하 급수적으로 증가하지만 다른 일반적인 방법보다 훨씬 효율적이고 광범위하게 적용될 수 있습니다. 동적 프로그래밍은 부분적으로 관찰 가능한 MDP \(Lovejoy, 1991\), 많은 응용 프로그램 \(White, 1985, 1988, 1993\), 근사 방법 \(Rust, 1996\) 등을 포함하여 1950 년대 후반부터 광범위하게 개발되었습니다. , 비동기 방법 \(Bertsekas, 1982, 1983\). 역동적 인 프로그래밍에 대한 많은 우수한 현대적 치료법이 이용 가능하다 \(예, Bertsekas, 2005, Puterman, 1994, Ross, 1983, Whittle, 1982, 1983\).

      


이 책에서 우리는 최적 제어에서의 모든 작업 또한 강화 학습에서 작동한다는 것을 의미한다고 생각합니다. 우리는 강화 학습 방법을 강화학습 문제를 해결하는 효과적인 방법으로 정의하고 있으며, 이러한 문제가 최적 제어 문제, 특히 MDP로 공식화 된 것과 같은 확률 적 제어 문제와 밀접하게 관련되어 있음이 현재 분명하다. 따라서 우리는 동적 프로그래밍과 같은 최적 제어의 솔루션 방법을 강화학습 방법으로 고려해야합니다. 거의 모든 기존의 방법들은 통제 할 시스템에 대한 완전한 지식을 요구하기 때문에, 그들이 강화 학습의 일부라고 말하는 것은 부 자연스럽게 느껴집니다. 반면에 많은 동적 프로그래밍 알고리즘은 점진적이고 반복적입니다. 학습 방법과 마찬가지로, 그들은 점진적 근사를 통해 점진적으로 정확한 답을 얻습니다. 이 책의 나머지 부분에서 보여 주듯이, 이러한 유사점은 피상적 인 것보다 훨씬 큽니다. 완전하고 불완전한 지식의 경우에 대한 이론과 해법은 너무 밀접하게 관련되어 있기 때문에 같은 주제의 일부로 함께 고려해야한다고 생각합니다.

      


시행 착오 학습이라는 아이디어를 중심으로 한 재교육의 현대 분야로 이끄는 다른 주요 실로 돌아 갑시다. 우리는 여기서 중요한 주제에 관해서 만 다루고 있습니다.이 주제는 14 장에서 자세하게 다루겠습니다. 미국의 심리학자 인 RS Woodworth에 따르면, 시행 착오적 학습에 대한 아이디어는 Alexander Bain의 학습에 대한 1850 년대까지 거슬러 올라갑니다 영국의 생물 학자이자 심리학자 인 콘웨이 로이드 모건 \(Conway Lloyd Morgan\)이 1894 년에 동물 행동에 대한 그의 견해를 묘사하기 위해이 용어를 사용하는 것을 "연구하고 실험 \(groping and experiment\)"했다. 아마도 학습의 원칙으로 시행 착오 학습의 본질을 간결하게 표현한 첫 번째 요인은 Edward Thorndike :

      


동일한 상황에 대한 몇 가지 응답 중에서 동물에 만족하거나 가까이에 따라 만족하는 것들은 다른 것들이 평등하고 상황과보다 밀접하게 연결되어 재발 할 때 더 많이 반응 할 것입니다 되풀이하다; 동물에게 불편 함이 수반되거나 밀접하게 뒤 따르는 동물은 다른 동물과 동등한 상태에있을 것이고, 그러한 상황과의 연관성이 약해 지므로 반복 될 때 발생 가능성이 적습니다. 만족도 또는 불편 함이 클수록 채권의 강화 또는 약화가 커집니다. \(Thorndike, 1911, 244 쪽\)

      


Thorndike는 이것을 "효과의 법칙 \(Law of Effect\)"이라고 부르는데, 그 이유는 행동을 선택하는 경향에 이벤트를 강화하는 효과를 설명하기 때문입니다. Thorndike는 동물 교육에 관한 자료 축적 \(보상과 처벌의 효과 사이의 차이와 같은\)에 대해 더 잘 설명하도록 법률을 수정했으며, 다양한 형태의 법은 학습 이론가들 사이에 상당한 논란을 불러 일으켰다 \(Gallistel, 2005 참조\) Herrnstein, 1970, Kimble, 1961, 1967, Mazur, 1994\). 그럼에도 불구하고 효과의 법칙 \(한 가지 형태 또는 다른 형태\)은 많은 행동의 기초가되는 기본 원칙으로 널리 간주된다. Clark Hull의 영향력있는 학습 이론과 BF Skinner의 실험 방법의 기초입니다 \(예 : Hull, 1943; Skinner, 1938\).

---

동물 학습의 맥락에서 "강화"라는 용어는 Thorndike가 효과 법칙을 표현한 후에 잘 사용되었으며, 조건부 반사에 대한 Pavlov의 모노 그래프의 1927 년 영문 번역본에서이 문맥에서 처음으로 나타나는 우리의 지식 중 가장 좋습니다. 강화 \(reinforcement\) 란 동물이 다른 자극이나 반응으로 적절한 시간적 관계에서 강화제 \(보강제\)를 받음으로써 행동 패턴을 강화시키는 것입니다. 일부 심리학자들은 약화의 과정을 포함하여 그 의미를 확장하여, 강화뿐만 아니라 사건의 생략 또는 종결이 행동을 변화시킬 때 적용합니다. 강화는 강화제가 회수 된 후에도 지속되는 행동 변화를 일으키며, 인공 지능의 가능성에 대한 가장 초기의 생각 중에는 컴퓨터에서 시행 착오를 학습하는 아이디어가 나타났습니다. 1948 년 보고서에서 앨런 튜링 \(Alan Turing\)은 효과 법칙에 따라 작업 한 "즐거움 - 통증 시스템"에 대한 설계를 설명했습니다. 조치가 결정되지 않은 구성에 도달하면 누락 된 데이터에 대한 무작위 선택이 이루어집니다 잠정적 설명에 적절한 입력이 이루어지고 적용됩니다. 통증 자극이 발생하면 모든 잠정 항목이 취소되고 즐거움 자극이 발생하면 모두 일시적입니다. \(Turing, 1948\)

---

많은 독창적 인 전자 기계 기계가 시범 및 오류 학습을 시연했다. 가장 초기의 것은 토마스가 만든 기계 일 것입니다.

Ross \(1933\)는 간단한 미로를 통해 길을 찾고 스위치 설정을 통해 경로를 기억할 수있었습니다. 1951 년 W. Grey Walter는 그의 "기계식 거북이"\(Walter, 1950\)로 이미 알려져 있었고 단순한 형태의 학습이 가능한 버전을 만들었습니다 \(Walter, 1951\). 1952 년 클로드 섀넌 \(Claude Shannon\)은 테우스 \(Theus\)라고 불리는 미로 실행 마우스를 시연했으며, 미로를 통해 그 길을 찾기 위해 시행 착오를 일으켰다. 미로는 자석과 계단을 통해 성공적인 방향을 기억했다 \(Shannon, 1951, 1952\). JA Deutsch \(1954\)는 모델 기반 강화 학습과 공통점이있는 행동 이론 \(Deutsch, 1953\)에 기반한 미로 해결 기계를 설명했습니다 \(8 장\). 박사 과정. 분열 \(Minsky, 1954\),    


전자 기계 학습 기계를 구축하면 디지털 컴퓨터를 프로그래밍하여 다양한 유형의 학습을 수행 할 수 있었으며 그 중 일부는 시행 착오를 통한 학습을 ​​구현했습니다. Farley and Clark \(1954\)는 시행 착오를 통해 배운 신경망 학습 기계의 디지털 시뮬레이션에 대해 설명했다. 그러나 그들의 관심은 시행 착오적 학습에서 일반화와 패턴 인식, 즉 강화학습에서 감독 학습으로 옮겨 갔다 \(Clark and Farley, 1955\). 이것은 이러한 유형의 학습들 사이의 관계에 대한 혼란의 패턴을 만들기 시작했습니다. 많은 연구자들이 실제로 감독 학습을 공부할 때 강화 학습을 연구하고 있다고 생각하는 것 같습니다. 예를 들어, Rosenblatt \(1962\)와 Widrow and Hoff \(1960\)와 같은 신경 네트워크 개척자들은 강화학습에 의해 명확하게 동기 부여되었지만 보상과 처벌의 언어를 사용했으나 그들이 연구 한 시스템은 패턴 인식에 적합한 감독 학습 시스템이었고 지각 학습. 오늘날에도 일부 연구자와 교과서는 이러한 유형의 학습을 구별하거나 모호하게 만듭니다. 예를 들어, 일부 신경망 교과서에서는 학습 예제를 통해 배울 수있는 네트워크를 설명하기 위해 "시행 착오"라는 용어를 사용했습니다. 이러한 네트워크는 연결 정보를 업데이트하기 위해 오류 정보를 사용하기 때문에 이해할 만하다. 그러나 이것은 올바른 정보에 의존하지 않는 평가 피드백을 기반으로 작업을 선택하는 것으로 시행 착오 학습의 본질을 놓치기 때문에 이해하기 쉽다. 행동이 있어야합니다.    


부분적으로는 이러한 혼란의 결과로, 진정한 시행 착오를 배우는 연구는 1960 년대와 1970 년대에는 드물게 나 타 났지만 주목할만한 예외가있었습니다. 1960 년대에 시도와 오류 학습의 공학적 사용을 기술하기 위해 엔지니어링 문헌에서 처음으로 "강화"와 "강화학습"이라는 용어가 사용되었습니다 \(예 : Waltz and Fu, 1965; Mendel, 1966; Mendel and McClaren, 1970\). 특히 영향력있는 것은 Minsky의 논문 "인공 지능을 향한 단계"\(Minsky, 1961\)였으며, 예측, 기대, 복잡한 보강을위한 기본 신용 할당 문제를 포함하여 시행 착오 학습과 관련된 몇 가지 쟁점을 논의했습니다 학습 시스템 : 당신은 어떻게합니까? 그것을 생산하는 데 관련된 많은 결정 중에서 성공을위한 신용을 분배 하는가? 이 책에서 우리가 토론하는 모든 방법은 어떤면에서 이 문제를 해결하기위한 것입니다. Minsky의 논문은 오늘날도 여전히  읽을 가치가있다.    


다음 몇 단락에서 우리는 1960 년대와 1970 년대에 진정한 시행 착오 학습에 대한 계산적이고 이론적 인 연구의 상대적 무시에 대한 몇 가지 다른 예외와 부분적인 예외를 논의합니다.

      


그 중 하나는 John Andreae라는 뉴질랜드 연구원의 연구 결과입니다. Andreae \(1963\)는 환경과의 상호 작용에서 시행 착오를 통해 배운 STeLLA라는 시스템을 개발했다. 이 시스템에는 세계의 내부 모델과 나중에 숨겨진 국가의 문제를 다루는 

"

내부 독백

"

이 포함되어있다 \(Andreae, 1969a\). Andreae의 후기 저작 \(1977\)은 교사의 학습에 중점을 두었지만 시행 착오를 포함했다. 불행히도, 그의 선구자적인 연구는 잘 알려지지 않았으며 이후의 강화학습 연구에 큰 영향을 미치지 않았습니다.

      


더 영향력있는 것은 Donald Michie의 작업이었습니다. 1961 년과 1963 년에 그는 매틱스 \(Matchbox Educable Naughts and Crosses Engine\)에 대해 틱 - 톡 - 발가락 \(또는 콧물과 십자가\)을하는 법을 배우기위한 간단한 시행 착오 학습 시스템을 설명했습니다. 그것은 가능한 각 게임 위치에 대한 성냥갑으로 구성되어 있으며, 각 성냥갑에는 색깔있는 구슬이 여러 개 있고 그 위치에서 가능한 각 이동마다 다른 색이 있습니다. 현재 게임 위치에 해당하는 성냥갑에서 무작위로 비드를 그려서 MENACE의 이동을 결정할 수 있습니다. 게임이 끝나면 경기 중에 사용 된 상자에 비즈를 추가하거나 제거하여 MENACE의 결정을 강화하거나 처벌했습니다. Michie와 Chambers \(1968\)는 GLEE \(Game Learning Expectimaxing Engine\)이라는 또 다른 tic-tac-toe 강화학습자와 BOXES라고하는 강화학습 컨트롤러를 설명했습니다. 그들은 극이 떨어지거나 카트가 궤도의 끝에 도달했을 때만 발생하는 고장 신호를 기반으로 이동식 카트에 연결된 폴의 균형을 잡는 법을 배우는 작업에 BOXES를 적용했습니다. 이 과제는 Widrow와 Smith \(1964\)의 초기 연구에서 채택되었는데, 교사가 이미 극의 균형을 잡을 수 있다고 가정하면 감독 학습 방법을 사용했습니다. Michie와 Chambers의 pole-balancing 버전은 불완전한 지식 상태에서 강화학습 과제의 가장 좋은 예 중 하나입니다. 그것은 우리 자신의 연구 \(Barto, Sutton, and Anderson, 1983; Sutton, 1984\)의 일부로 시작하여 강화학습에서 많은 후반 작업에 영향을 미쳤습니다. Michie는 시행 착오와 학습이 인공 지능의 필수적인 측면으로서 일관되게 강조했다 \(Michie, 1974\).

      


Widrow, Gupta, Maitra \(1973\)는 Widrow와 Hoff \(1960\)의 LMS 알고리즘을 수정하여 학습 예제가 아닌 성공과 실패 신호를 통해 학습 할 수있는 강화학습 규칙을 만들었다. 그들은 "선택적인 부트 스트랩 적응"을 배우는이 형태를 "교사와 함께 배우는" 대신 "비평가와 함께 배우는"것으로 묘사했습니다. 그들은이 규칙을 분석하고 블랙 잭을 배우는 법을 보여주었습니다. 이것은 감독 학습에 대한 기여도가 훨씬 더 큰 영향을 미친 Widrow의 강화 학습으로의 독립된 진출이었습니다. 우리의 "평론가"라는 용어의 사용은 위드 로우 \(Widrow\), 굽타 \(Gupta\), 마이 트라 \(Maitra\)의 논문에서 비롯된 것입니다. Buchanan, Mitchell, Smith, Johnson \(1978\)      


기계 학습 \(Dietterich and Buchanan, 1984\)의 맥락에서 용어 비평가를 사용 하였지만 비평가는 성과를 평가하는 것 이상을 할 수있는 전문가 시스템입니다.

      


학습 오토 마 타 연구는 현대적인 강화학습 연구로 이어지는 시행 착오적 인 문제에 직접적인 영향을 미쳤다. 이것은 k 어 레버 \(2 장 참조\)를 제외하고는 슬롯 머신이나 "한쪽 무장 한 적기"와 유추하여 k- 무장 한 적기로 알려진 비논리적 인 순전히 선택적인 학습 문제를 푸는 방법입니다. 학습 오토 마톤은 이러한 문제에서 보상의 확률을 향상시키기위한 단순하고 메모리가 적은 기계입니다. 학습 오토 마타는 러시아의 수학자이자 물리학자인 ML Tsetlin과 동료 \(1973 년 Tsetlin에서 사후에 출간 됨\)의 1960 년 작업에서 기인했으며 그 이후 엔지니어링 분야에서 광범위하게 발전해 왔습니다 \(Narendra and Thathachar, 1974, 1989 참조\). 이러한 개발에는 확률 론적 학습 오토 마 타 연구, 이는 보상 신호에 기초하여 행동 확률을 갱신하는 방법이다. Stochastic learning au tomata는 William Estes의 1950 년 학습 이론 \(Estes, 1950\)을 시작으로 심리학자 인 Robert Bush와 통계 학자 인 Frederick Mosteller \(Bush와 Mostller, 1955\).

      


심리학에서 개발 된 통계적 학습 이론은 경제학 연구자들이 채택하여 강화학습에 전념 한 연구 분야로 이어졌습니다. 이 연구는 1973 년 부시와 모스 텔러의 학습 이론을 고전 경제 모델 모음에 적용한 것에서 시작되었다 \(Cross, 1973\). 이 연구의 한 가지 목표는 전통적인 이상주의 경제 요원보다 실제 사람들처럼 행동하는 인공 작용제를 연구하는 것이었다 \(Arthur, 1991\). 이 접근법은 게임 이론의 맥락에서 강화 학습의 연구로 확대되었습니다. 경제학에서의 강화 학습은 인공 지능의 초기 연구와는 크게 독립적으로 발전했지만, 강화 학습과 게임 이론은 두 분야에서 현재 관심을 끄는 주제이지만이 책의 범위를 벗어나는 주제입니다. Camerer \(2003\)는 경제학에서 강화학습 학습 전통에 대해 논의하고 Now et al. \(2012\)는이 책에서 소개하는 접근 방식에 대한 다중 에이전트 확장의 관점에서 주제에 대한 개요를 제공합니다. 보상 강도 학습과 게임 이론은 tic-tac-toe, checker 및 다른 레크리에이션 게임을하는 프로그램에서 사용되는 강화 학습과는 매우 다른 주제입니다. 강화학습 및 게임에 대한 개요는 Szita \(2012\)를 참조하십시오. 및 다른 레크리에이션 게임. 강화학습 및 게임에 대한 개요는 Szita \(2012\)를 참조하십시오. 및 다른 레크리에이션 게임. 강화학습 및 게임에 대한 개요는 Szita \(2012\)를 참조하십시오.

      


John Holland \(1975\)는 선택 원칙에 입각 한 적응 형 시스템에 대한 일반적인 이론을 개괄했다. 그의 초기 연구는 진화 적 방법과 k- 무장 한 산적과 같이 주로 비 동시 적 형태의 시행 착오와 관련이있다. 1976 년과 1986 년에 완전히 개설 된 그는 분류기 시스템, 협회 및 가치 기능을 포함한 진정한 강화학습 시스템을 도입했습니다. Holland의 분류 시스템의 핵심 구성 요소는 항상 유전 알고리즘이었으며 진화 적 방법으로 유용한 표현을 발전시키는 역할을 수행했습니다. 분류 자 시스템은 많은 연구자들에 의해 강화학습 학습 \(Urbanowicz and Moore, 2009에 의해 재검토 됨\)의 주요 부분을 형성하기 위해 광범위하게 개발되었지만, 우리는 이 아닌 유전자 알고리즘을 사용했다 . 

      


\(Fogel, Owens and Walsh, 1966, Koza, 1992\)와 같이 진화론 적 계산에 대한 다른 접근법을 가지고 있기 때문에 훨씬 더 많은 주목을 받았다.

      


시행 착오를 인공 지능 내에서 강화 학습으로 되 돌리는 가장 큰 책임자는 Harry Klopf \(1972, 1975, 1982\)였다. Klopf는 배우는 학습자가 거의 감독 학습에만 집중할 때 적응 행동의 필수적인 측면을 잃어 버리고 있음을 인식했습니다. Klopf에 따르면, Klopf에 따르면, 환경의 일부 쾌거를 달성하고, 원하는 목표를 향해 그리고 원하지 않는 목표를 벗어나 환경을 제어하기위한 행동의 쾌락적인 측면이었습니다. 이것은 시행 착오 학습의 핵심 아이디어입니다. Klopf의 아이디어는 저자들에게 특히 영향을주었습니다. Barto와 Sutton \(1981a\)는 감독 학습과 강화학습의 구분과 강화 학습에 중점을 두었 기 때문에 감사합니다. 우리와 동료가 수행 한 초기 연구의 대부분은 강화학습과 감독 학습이 실제로 다르다는 것을 보여주었습니다 \(Barto, Sutton, and Brouwer, 1981; Barto and Sutton, 1981b; Barto and Anandan, 1985\). 다른 연구들은 강화학습이 신경 회로망 학습에서 중요한 문제를 어떻게 다루는지를 보여 주었고, 특히 다층 네트워크 학습 알고리즘을 어떻게 만들어 낼 수 있는지를 보여 주었다 \(Barto, Anderson and Sutton, 1982; Barto and Anderson, 1985; Barto and Anandan, 1985; , 1985, 1986, Barto and Jordan, 1987\). 15 장에서는 강화학습과 신경 회로망에 대해 더 자세히 말합니다. Barto and Anandan, 1985\). 다른 연구들은 강화학습이 신경 회로망 학습에서 중요한 문제를 어떻게 다루는지를 보여 주었고, 특히 다층 네트워크 학습 알고리즘을 어떻게 만들어 낼 수 있는지를 보여 주었다 \(Barto, Anderson and Sutton, 1982; Barto and Anderson, 1985; Barto and Anandan, 1985; , 1985, 1986, Barto and Jordan, 1987\). 15 장에서는 강화학습과 신경 회로망에 대해 더 자세히 말합니다. Barto and Anandan, 1985\). 다른 연구들은 강화학습이 신경 회로망 학습에서 중요한 문제를 어떻게 다루는지를 보여 주었고, 특히 다층 네트워크 학습 알고리즘을 어떻게 만들어 낼 수 있는지를 보여 주었다 \(Barto, Anderson and Sutton, 1982; Barto and Anderson, 1985; Barto and Anandan, 1985; , 1985, 1986, Barto and Jordan, 1987\). 15 장에서는 강화학습과 신경 회로망에 대해 더 자세히 말합니다.

      


우리는 강화 학습의 역사에 대한 세 번째 실마리, 즉 시간차 학습에 관한 이야기로 전환합니다. 시간차 학습 방법은 같은 양의 시간에 따라 연속적으로 추정하는 것 \(예 : 틱택 토 \(tic-tac-toe\) 예에서 승리 할 확률\)의 차이에 따라 달라 지므로 특유합니다. 이 스레드는 다른 두 개보다 작고 덜 분명한 반면, 부분적으로는 시간차 방법이 강화학습에 새롭고 고유 한 것으로 보이는 부분에서 특히 현장에서 특히 중요한 역할을 수행했습니다.

      


시간차 학습의 기원은 특히 2 차 보강자의 개념에서 동물 교육 심리학에 부분적으로있다. 보조 보강제는 음식이나 통증과 같은 주요 보강제와 짝을 이루는 자극제이며, 결과적으로 유사한 보강 성질을 갖게되었습니다. 민스크 \(Minsky, 1954\)는 이러한 심리학 적 원리가 인공 학습 시스템에 중요 할 수 있음을 깨닫는 최초의 인물 일 수있다. Arthur Samuel \(1959\)은 유명한 체커 플레이 프로그램의 일환으로 일시적 차이 아이디어를 포함하는 학습 방법을 제안하고 구현 한 최초의 사람이었습니다.

      


Samuel은 Minsky의 연구 또는 동물 교육과의 관련성에 대해 언급하지 않았습니다. 그의 영감은 클로드 샤논 \(Claude Shannon, 1950\)의 제안에 따르면 컴퓨터가 체스 게임을하기 위해 평가 기능을 사용하도록 프로그래밍 될 수 있으며이 기능을 온라인으로 수정하여 게임을 향상시킬 수 있다고 제안했다. \(섀넌의 이러한 아이디어는 벨맨 영향을 가능성이있다, 그러나 우리는 이에 대한 증거로 알고있다.\) \(1961\) 민스키 광범위하게 그의 "단계"논문에서 사무엘의 일을 논의 차 보강 이론에 대한 연결을 제안, 모두 자연

\*\*

\*\*

우리가 논의한 것처럼, 민스크와 사무엘의 작업을 따르는 10 년 동안, 시행 착오 학습에 대해서는 거의 계산 작업이 수행되지 않았으며, 일시적 차이 학습에 대한 계산 작업은 전혀 수행되지 않았다. 1972 년 Klopf는 시차 조정 학습의 중요한 구성 요소와 함께 시행 착오 학습을 가져 왔습니다. Klopf는 대규모 시스템에서 학습에 확장 ​​할 수있는 원리에 관심이 있었기 때문에 전체 학습 시스템의 하위 구성 요소가 서로를 강화할 수있는 지역 강화라는 개념에 흥미를 느꼈습니다. 그는 모든 구성 요소 \(명목상 모든 뉴런\)가 모든 입력을 보강 측면에서 바라 보는 "보편적 강화"에 대한 아이디어를 개발했습니다. 흥분성 입력은 보상으로, 보상 입력은 처벌로 간주됩니다. 이것은 우리가 지금 시차 차이 학습으로 알고있는 것과 같은 생각이 아니며, 회개 할 때 사무엘의 작업보다 더 멀리 떨어져 있습니다. 다른 한편, Klopf는이 아이디어를 시행 착오적 학습으로 연결하고 동물 학습 심리학의 거대한 경험적 데이터베이스와 관련시켰다.

---

Sutton \(1978a, 1978b, 1978c\)은 Klopf의 아이디어, 특히 동물 학습 이론에 대한 링크를 개발하여, 연속적으로 일어나는 예측의 변화에 ​​의한 학습 규칙을 설명했다. Barto와 Barto는 이러한 아이디어를 세련하고 시간차 학습에 기반한 고전적 조건화의 심리적 모델을 개발했다 \(Sutton and Barto, 1981a, Barto and Sutton, 1982\). 시간차 학습에 기초한 고전적 조건화에 대한 몇 가지 다른 중요하지 않은 심리학 적 모델이 있었다 \(Klopf, 1988; Moore et al., 1986; Sutton and Barto, 1987, 1990\). 이시기에 개발 된 일부 신경 과학 모델은 시간차 학습이라는 측면에서 잘 해석된다 \(Hawkins and Kandel, 1984; Byrne, Gingrich, and Baxter, 1990, Gelperin, Hopfield, and Tank, 1985; Tesauro, 1986; Friston et al. , 1994\),

---

시간적 차이 학습에 대한 우리의 초기 작업은 동물 학습 이론과 Klopf의 연구에 크게 영향을 받았습니다. 민스크의 "계단"과 사무엘의 체커 선수들과의 관계는 이후에야 인식 된 것으로 보인다. 그러나 1981 년까지 우리는 일시적 차이와 시행 착오적 인 실마리의 일부로서 위에서 언급 한 모든 이전 연구를 완전히 인식했다. 이 시점에서 우리는 actor-critic 구조로 알려진 시행 착오 학습에서 시간차 학습을 사용하는 방법을 개발하였고,이 방법을 Michie와 Chambers의 극 균형 문제에 적용했습니다 \(Barto, Sutton, and Anderson, 1983 \). 이 방법은 Sutton \(1984\) 박사에서 광범위하게 연구되었습니다. 앤더슨 \(Anderson, 1986\) 박사 학위 논문에서 백 프로 모션 신경망을 사용하도록 확장. 논문. 이 무렵, 네덜란드 \(1986\)는 일시적 차이 개념을 자신의 분류 자 ​​시스템에 명시 적으로 통합했다. 주요 단계는 Sutton이 1988 년에 시간차 학습을 통제와 분리하여 일반적인 예측 방법으로 간주함으로써 수행되었습니다. 그 논문은 또한 TD \(λ\) 알고리즘을 도입하고 그 수렴 특성을 입증했다.      


1981 년 배우 - 비평가 건축에 대한 작업을 마무리하면서, 우리는 시간차 학습 규칙의 가장 초기에 알려진 출판물을 포함하는 Ian Witten \(1977\)의 논문을 발견했다. 그는 MDP를 해결하기위한 적응 컨트롤러의 일부로 사용하기 위해 표 형식 TD \(0\)라고 부르는 방법을 제안했습니다. 위튼의 연구는 Andreae의 초기 실험 인 STeLLA와 다른 시행 착오 학습 시스템의 후손이었습니다. 따라서 위튼 \(Witten\)의 1977 년 논문은 강화 학습 학습의 주요한 부분 인 시행 착오적 학습과 최적의 통제에 이르는 동안 시간적 차이를 학습하는 데 일찍 기여했다.

---

시간차 및 최적 제어 스레드는 Chris Watkins의 Q-러닝 개발과 함께 1989 년에 완전히 통합되었습니다. 이 연구는 강화학습 연구의 세 가지 스레드에서 이전 연구를 확장하고 통합했습니다. Paul Werbos \(1987\)는 1977 년 이래 시행 착오적 학습과 동적 프로그래밍의 수렴을 주장하면서 이러한 통합에 공헌했다. Watkins의 작업이 있었을 때 주로 기계 학습에서 강화 학습 연구가 엄청나게 성장했다 인공 지능의 하위 분야뿐만 아니라 신경망 및 인공 지능 분야에서도 광범위하게 사용됩니다. 1992 년 Gerry Tesauro의 주사위 놀이 프로그램 인 TD-Gammon의 놀라운 성공은이 분야에 더 많은 관심을 불러 일으켰습니다.

     


이 책의 초판 발행 이후, 강화 학습 알고리즘과 신경계에서의 강화 학습 사이의 관계에 초점을 맞춘 신경 과학의 번성 한 하위 필드가 개발되었습니다. 이것에 가장 책임이있는 것은 많은 연구자들에 의해 지적 된 바와 같이 시간차 알고리즘의 행동과 뇌의 도파민 생성 뉴런의 활동 사이의 기괴한 유사성이다 \(Friston et al., 1994; Barto, 1995a; Houk , Adams, and Barto, 1995, Montague, Dayan, Sejnowski, 1996, Schultz, Dayan, Montague, 1997\). 15 장은 강화 학습의 흥미 진진한 측면을 소개합니다.

      


최근의 강화학습 역사에서 작성된 다른 중요한 기여는이 간단한 설명에서 언급하기에는 너무 많습니다. 우리는 그들이 발생하는 개별 장의 끝에이 중 많은 것을 인용합니다.

      


강화학습에 대한 일반적인 내용은 Szepesv ari \(2010\), Bertsekas and Tsitsiklis \(1996\), Kaelbling \(1993a\) 및 Masashi Sugiyama 등의 저서를 참조하십시오. \(2013\). 제어 또는 운영 연구 관점을 취하는 책은 Si 등의 책입니다. \(2004\), Powell \(2011\), Lewis and Liu \(2012\), Bertsekas \(2012\) 등이있다. 저널 Machine Learning의 세 가지 특수 쟁점은 강화 학습에 초점을두고 있습니다 : Sutton \(1992\), Kaelbling \(1996\) 및 Singh \(2002\). Barto \(1995b\)는 유용한 설문 조사를 제공합니다. Kaelbling, Littman, Moore \(1996\); Keerthi and Ravindran \(1997\) 등이있다. Weiring과 van Otterlo \(2012\)가 편집 한 책은 최근 개발에 대한 훌륭한 개요를 제공합니다.

      


이 장에서 Phil의 아침 식사 예제는 Agre \(1988\)에서 영감을 받았습니다. 독자는 tic-tac-toe 예제에서 사용한 일시적 차이 방법에 대한 언급을 위해 6 장으로 안내합니다.

      


우리는 제 3 장에서 보게 되겠지만, 강화 학습의 이론은 우리가이 책에서 취급으로는 보상의 양의 예상 가치 극대화를 기반으로 에이전트는 미래에 누적 될 수 있습니다. 이것은 von Neumann과 Morgenstern \(1944\)의 고전적 원리와 합리적인 결정은 합리적인 결정이 예상 된 유용성을 극대화하는 것임을 보여준다. 그러나 무작위 수의 기대 값을 극대화하는 것은 위험을 감수해야하는 수량의 편차를 무시하기 때문에 종종 올바른 일이 아닙니다. 위험에 민감한 최적화는 재무 및 최적 제어와 같이 과도한 위험이 파손될 수있는 분야에서 고도로 발달되어 왔습니다. 위험은 강화 학습에도 중요하지만 여기서는 범위를 벗어납니다. Heger \(1994\), Geibel \(2001\), Mihatsch and Neuneier \(2002\), Borkar \(2002\)는 강화 학습에서 위험을 고려하고 여기에 제시된 알고리즘 중 일부의 위험에 민감한 버전을 개발하는 사례이다. Coraluppi와 Marcus \(1999\)는 강화 학습에 대한 우리의 접근법의 기초를 형성하는 이산 시간, 유한 상태의 마르코프 결정 과정의 맥락에서 위험을 논의한다. 우리는 또한 사람들의 욕구를 측정하는 것과 관련된 효용 이론을 완전히 회피합니다. 우리가 인간의 경제적 행동 이론으로 강화학습을 다루고 있다면, 실용 이론은 관련이있을 것이며, 강화학습에 관한 설명 중 일부는 유용성과 상응한다고 \(Russell and Norvig, 2010\). 그러나 이것이 우리의 목표가 아니기 때문에, 우리는 유틸리티 이론과의 연결을 다른 사람들에게 남깁니다. 우리가 인간의 경제적 행동 이론으로 강화학습을 다루고 있다면, 실용 이론은 관련이있을 것이며, 강화학습에 관한 설명 중 일부는 유용성과 상응한다고 \(Russell and Norvig, 2010\). 그러나 이것이 우리의 목표가 아니기 때문에, 우리는 유틸리티 이론과의 연결을 다른 사람들에게 남깁니다. 우리가 인간의 경제적 행동 이론으로 강화학습을 다루고 있다면, 실용 이론은 관련이있을 것이며, 강화학습에 관한 설명 중 일부는 유용성과 상응한다고 \(Russell and Norvig, 2010\). 그러나 이것이 우리의 목표가 아니기 때문에, 우리는 유틸리티 이론과의 연결을 다른 사람들에게 남깁니다.

    **  
**

**파트 I : 테이블 형식의 솔루션 방법**

**      
**

**이 책의 부분에서는 강화학습 알고리즘의 가장 핵심적인 아이디어를 가장 간단한 형태로 설명합니다 : 상태 공간과 작업 공간이 근사값 함수가 배열로 표시 될 정도로 작 으면, 또는 테이블. 이 경우, 메소드는 정확한 솔루션을 찾을 수 있습니다. 즉, 최적의 값 기능과 최적의 정책을 정확히 찾을 수 있습니다. 이것은 책의 다음 부분에 설명 된 근사 방법과 대조되는데,이 방법은 근사 솔루션을 찾지 만 더 큰 문제에 효과적으로 적용 할 수 있습니다.**

**      
**

**책의이 부분의 첫 번째 장에서는 단일 상태 만 존재하는 강화학습 문제의 특수한 경우에 대한 해결책 방법을 기술합니다.이 방법은 적기 문제라고합니다. 두 번째 장은 나머지 모든 유한 마르코프 결정 과정에서 다루는 일반적인 문제 공식과 ​​벨만 방정식 및 가치 함수를 포함한 주요 아이디어를 설명합니다.**

**      
**

**다음 3 장에서는 유한 마르코프 결정 문제를 해결하기위한 세 가지 기본 방법 인 동적 프로그래밍, 몬테 카를로 방법 및 시간차 학습을 설명합니다. 각 클래스의 메소드에는 강점과 약점이 있습니다. 동적 프로그래밍 방법은 수학적으로 잘 발달되어 있지만 완전하고 정확한 환경 모델을 필요로합니다. Monte Carlo 방법은 모델을 필요로하지 않고 개념적으로 단순하지만 단계별 점진적 계산에는 적합하지 않습니다. 마지막으로, 시간차 방법은 모델을 필요로하지 않으며 완전히 증분되지만 분석하기가 더 복잡합니다. 이 방법은 효율성과 수렴 속도면에서 여러면에서 차이가 있습니다.**

**      
**

**나머지 두 장에서는이 세 가지 클래스의 방법을 결합하여 각각의 최상의 기능을 얻는 방법에 대해 설명합니다. 한 장에서 우리는 몬테카를로 방법의 강점과 적성 추적의 사용을 통한 시간차 방법의 강점을 결합하는 방법을 설명합니다. 이 부분의 마지막 장에서는 표본 강화 학습 문제에 대한 완전하고 통합 된 솔루션을 위해 시간차 학습 방법을 모델 학습 및 계획 방법 \(예 : 동적 프로그래밍\)과 어떻게 조합 할 수 있는지 보여줍니다.**

**      
**

**      
**

