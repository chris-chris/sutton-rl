## 16.5 Human-level Video Game Play

실제 문제에 강화 학습을 적용하는 가장 큰 과제 중 하나는 가치 기능 및 / 또는 정책을 표현하고 저장하는 방법을 결정하는 것입니다. 상태 세트가 한정적이며 룩업 테이블에 의한 철저한 표현을 허용 할만큼 충분히 작지 않다면 (예를 들어 많은 예시에서와 같이), 매개 변수화 된 함수 근사 체계를 사용해야한다. 함수 근사는 선형인지 비선형인지에 관계없이 학습 시스템에 쉽게 액세스 할 수 있어야하고 숙련 된 성능에 필요한 정보를 전달할 수 있어야합니다. 강화 학습의 가장 성공적인 응용 프로그램은 다루어야 할 특정 문제에 대한 인간의 지식과 직관에 기반하여 신중하게 수작업으로 만들어진 일련의 기능을 많이 필요로합니다.

Google DeepMind의 연구원 팀은 깊이있는 다중 레이어 인공 신경망 (ANN)이 기능 설계 프로세스를 자동화 할 수 있다는 인상적인 데모를 개발했습니다 (Mnih 외, 2015, 2013). 다층 ANN은 내부 표현을 학습하는 방법으로 역 전파 알고리즘을 1986 년에 보급 한 이래로 강화 학습에서 함수 근사에 사용되었습니다 (Rumelhart, Hinton, Williams, 1986, 9.6 절 참조). 현저한 결과는 보강 학습과 역 전파를 결합함으로써 얻어졌다. 위에서 논의한 Tesauro와 TD-Gammon 및 Watson의 결과는 주목할만한 사례입니다. 이러한 응용 프로그램 및 기타 응용 프로그램은 다중 계층 ANN이 작업 관련 기능을 익히는 데 도움이되었습니다. 그러나 우리가 알고있는 모든 사례에서 가장 인상적인 데모에서는 네트워크의 입력이 주어진 문제에 대한 전문화 된 기능으로 표현되어야했습니다. 이것은 TD-Gammon 결과에서 생생하게 드러납니다. 누구의 네트워크 입력이 주사위 놀이의 아주 작은 지식을 포함 즉, 본질적으로 그는 주사위 보드의 "원시"표현이었다 TD-훈제 햄 0.0, 최적의 이전 주사위 놀이 컴퓨터 프로그램으로 약뿐만 아니라 재생을 배웠습니다. 특수 주사위 놀이 기능을 추가하면 이전의 주사위 놀이 프로그램보다 훨씬 우수한 TD-Gammon 1.0이 만들어졌으며 인간 전문가와 잘 어울렸다.
Mnih et al. 이미지와 같은 데이터의 공간적 배열을 처리하기 위해 전문화 된 다층 또는 심층 ANN (deep-convolutional ANN)과 Q-learning을 결합한 DQN (deep Q-network)이라고하는 보강 학습 에이전트를 개발했습니다. 우리는 9.6 절에서 깊은 길쌈 ANN을 설명합니다. DQN와 Mnih 외.의 일의 시간으로 깊은 길쌈 앤스 포함 깊은 앤스은 많은 응용 프로그램에서 인상적인 결과를 생산했다, 그러나 그들은 널리 강화 학습에 사용되지 않았다.

Mnih et al. DQN을 사용하여 단일 강화 학습 에이전트가 서로 다른 문제 별 기능 세트에 의존하지 않고 다양한 문제에서 높은 수준의 성능을 달성 할 수있는 방법을 보여주었습니다. 이를 증명하기 위해 DQN은 게임 에뮬레이터와 상호 작용하여 49 가지 Atari 2600 비디오 게임을하는 법을 배웠습니다. 각 게임을 배우기 위해 DQN은 동일한 원시 입력, 동일한 네트워크 아키텍처 및 동일한 매개 변수 값 (예 : 단계 크기, 할인율, 탐색 매개 변수 및 구현과 관련하여 더 많은 것)을 사용했습니다. DQN은이 게임의 상당 부분에서 인간 수준 또는 그 이상의 수준의 플레이를 달성했습니다. 게임은 비디오 이미지의 스트림을 보면서 재생되는 것과 비슷했지만, 다른면에서는 다양했습니다. 그들의 행동에는 다른 효과가 있었고, 다른 국가 전환 역학이 있었으며, 높은 점수를 얻기 위해 다른 정책이 필요했습니다. 깊은 컨볼 루션 ANN은 모든 게임에 대한 일반 입력을 공통적 인 게임을 게임의 대부분에 대해 달성 된 높은 수준의 DQN에서 재생할 때 필요한 동작 값을 나타내는 데 특화된 기능으로 변환하는 방법을 배웠습니다.

Atari 2600은 1977 년부터 1992 년까지 Atari Inc.에서 다양한 버전으로 판매 한 가정용 비디오 게임 콘솔입니다. 이제는 Pong, Breakout, Space Invaders 및 Asteroids와 같이 고전으로 간주되는 많은 아케이드 비디오 게임을 소개하거나 대중화했습니다. 현대 비디오 게임보다 훨씬 간단하지만 아타리 (Atari) 2600 게임은 여전히 ​​인간 선수들에게는 즐겁고 도전적이며 강화 학습 방법을 개발하고 평가하기위한 시험대로서 매력적입니다 (Diuk, Cohen, Littman, 2008; Naddaf, 2010; Cobo, Zang , Isbell, Thomaz, 2011, Bellemare, Veness, and Bowling, 2013). Bellemare, Naddaf, Veness 및 Bowling (2012)은 학습 및 계획 알고리즘을 학습하기 위해 Atari 2600 게임을 사용하도록 장려하고 단순화하기 위해 공개적으로 사용할 수있는 Arcade Learning Environment (ALE)를 개발했습니다.

이러한 기존의 연구와 ALE의 가용성은 TD-훈제 햄은 주사위 놀이에 달성 할 수 있었다 인상적인 인간 수준의 성능에 영향을 받았다 Mnih 외.의 시연을 위해 아타리 2600 게임 컬렉션 좋은 선택을했다. DQN은 역 전파 알고리즘에 의해 계산 된 그라디언트를 사용하여 TD 알고리즘의 준 그라디언트 형식에 대한 함수 근사법으로 다층 ANN을 사용하는 경우 TD-Gammon과 유사합니다. 그러나 TD-Gammon과 같이 TD (λ)를 사용하는 대신 DQN은 Q- 학습의 준 그라디언트 형식을 사용했습니다. TD-Gammon은 주사위 놀이 움직임을 만들기위한 규칙에서 쉽게 얻은 애프터 스테이트의 가치를 추정했습니다. Atari 게임에 대해 동일한 알고리즘을 사용하려면 각 가능한 액션에 대해 다음 상태를 생성해야합니다 (이 경우에는 후행하지 않았을 것입니다). 이것은 게임 에뮬레이터를 사용하여 가능한 모든 동작 (ALE가 가능)에 대해 단일 단계 시뮬레이션을 실행하여 수행 할 수 있습니다. 또는 각 게임의 상태 전이 함수의 모델을 학습하고 다음 상태를 예측하는 데 사용할 수있었습니다 (Oh, Guo, Lee, Lewis 및 Singh, 2015). 이러한 방법으로 DQN에 필적하는 결과를 얻었을지라도 구현하기가 더 어려워졌고 학습에 필요한 시간이 크게 늘어났습니다. Q- 러닝을 사용하는 또 다른 동기는 DQN이 오프 - 정책 알고리즘을 필요로하는 아래에 설명 된 경험 재생 방법을 사용한다는 것입니다. 모델이 없으며 오프 정책이 됨으로써 Q- 러닝이 자연스러운 선택이되었습니다.

DQN의 세부 사항과 실험 수행 방법을 설명하기 전에 DQN이 달성 할 수있는 기술 수준을 살펴 봅니다. Mnih et al. DQN의 점수를 당시의 문헌에서 가장 좋은 학습 시스템 점수, 전문 인간 게임 테스터의 점수, 무작위로 행동을 선택한 요원의 점수와 비교했습니다. 문헌에서 나온 최상의 시스템은 Atari 2600 게임 (Bellemare, Naddaf, Veness, and Bowling, 2013)에 대한 지식을 사용하여 설계된 손으로 선형 함수 근사를 사용했습니다. DQN은 게임에 대한 경험이 약 38 일인 5 천만 개의 프레임에 대해 게임 에뮬레이터와 상호 작용하여 각 게임에 대해 학습했습니다. 각 게임을 배우기 시작하자 DQN 네트워크의 가중치가 임의의 값으로 재설정되었습니다. 학습 후 DQN의 기술 수준을 평가하기 위해 각 게임에서 평균 30 회의 세션으로 점수가 매겨졌으며 각 세션은 5 분 동안 지속되었으며 초기 게임 상태는 무작위로 시작되었습니다. 전문 인간 테스터는 동일한 에뮬레이터를 사용하여 연주했습니다 (오디오를 처리하지 않은 DQN보다 가능한 이점을 제거하기 위해 사운드가 꺼진 상태에서). 2 시간의 연습 후에 인간은 각각 최대 5 분 동안 각 게임에서 약 20 회 에피소드를 연주했으며이 시간 동안 휴식을 취할 수 없었습니다. DQN은 게임 중 6 개를 제외하고는 가장 우수한 이전의 보강 학습 시스템보다 더 잘 플레이하는 것을 배웠고, 22 개 게임에서 인간 플레이어보다 더 잘 플레이했습니다. 인간 수준의 75 % 또는 그 이상의 점수가 인간 수준의 놀이와 비교되거나 더 나은 것으로 간주함으로써, Mnih et al. 46 개의 게임 중 29 개에서 DQN의 학습 수준이 인간 수준에 도달하거나 초과했다고 결론지었습니다. Mnih et al. (2015) 이러한 결과에 대한 자세한 설명은

이러한 수준의 놀이를 달성하기위한 인공 학습 시스템이 충분히 인상적 일지 모르지만 이러한 결과를 현저하게 만드는 것은 무엇인가? 인공 지능의 획기적인 결과로 여겨지 던 당시 많은 사람들은 똑같은 학습 시스템이 이러한 수준의 놀이를 달성했다는 것입니다 게임에 특정한 수정을하지 않고도 광범위하게 다양한 게임을 즐길 수 있습니다.

이 49 가지 아타리 게임 중 하나를 실행하는 사람은 60Hz에서 128 색의 210x160 픽셀 이미지 프레임을 봅니다. 원칙적으로 이러한 이미지는 DQN에 원시 입력을 형성 할 수 있지만 메모리 및 처리 요구 사항을 줄이기 위해 Mnih 외. 84 × 84의 휘도 값 어레이를 생성하기 위해 각 프레임을 전처리한다. 많은 Atari 게임의 전체 상태가 이미지 프레임에서 완전히 관찰 가능하지 않기 때문에, Mnih et al. 네트워크에 대한 입력이 84 × 84 × 4 차원을 갖도록 4 개의 최신 프레임을 "쌓아"놓습니다. 이것은 모든 게임에 대한 부분적인 관찰 가능성을 제거하지 못했지만, 많은 사람들이 더 많은 Markovian을 만드는 데 도움이되었습니다.

여기에서 필수적인 점은 이러한 전처리 단계가 46 개 게임 모두 정확히 동일하다는 것입니다. 전반적인 이해를 뛰어 넘는 특정 게임 관련 사전 지식이 없었으므로이 축소 된 차원에서 좋은 정책을 배우는 것이 가능해야하며 인접한 프레임을 스태킹하면 일부 게임의 부분 관찰 가능성에 도움이됩니다. 이 최소량을 넘어서는 게임 특정 사전 지식이 이미지 프레임 전처리에 사용되지 않았으므로 84 × 84 × 4 입력 벡터를 DQN의 "원시"입력으로 생각할 수 있습니다.


DQN의 기본 아키텍처는 그림 9.15에 설명 된 것과 같은 깊은 컨볼 루션 ANN과 유사하지만 (DQN에서의 서브 샘플링은 각 컨볼 루션 계층의 일부로 처리되며 가능 맵핑 필드를 선택할 수있는 단위로 구성된 기능 맵으로 처리됩니다) . DQN에는 3 개의 숨겨진 길쌈 레이어가 있으며 그 뒤에 하나의 완전히 연결된 숨겨진 레이어가 이어지고 출력 레이어가 뒤 따른다. DQN의 3 개의 연속적인 숨겨진 컨볼 루션 레이어는 32 개의 20 × 20 피쳐 맵, 64 개의 9 × 9 피쳐 맵, 64 개의 7 × 7 피쳐 맵을 생성합니다. 각 기능 맵의 단위 활성화 기능은 정류기 비선형 성 (max (0, x))입니다. 이 세 번째 컨볼 루션 계층의 3,136 (64 × 7 × 7) 단위는 완전히 연결된 숨겨진 계층의 512 단위 각각에 연결되며, 각 계층은 출력 계층의 모든 18 단위에 연결됩니다.이 단위는 Atari 경기.

DQN의 출력 단위의 활성화 수준은 네트워크의 입력에 의해 대표되는 상태에 대해 상응하는 상태 - 행동 쌍의 추정 된 최적 동작 값 (최적의 Q 값)이었다. 출력 단위를 게임의 액션에 할당하는 것은 게임마다 다르며 게임의 유효 액션 수가 4에서 18까지 다양하기 때문에 모든 출력 단위가 모든 게임에서 기능적 역할을 수행하는 것은 아닙니다. 네트워크가 마치 18 개의 개별 네트워크 인 것처럼 생각하면 각각의 가능한 작업의 최적 작업 값을 예측할 수 있습니다. 실제로 이러한 네트워크는 초기 레이어를 공유했지만 출력 단위는이 레이어에서 추출한 기능을 다양한 방식으로 사용하는 방법을 배웠습니다.

DQN의 보상 신호는 게임의 스코어가 한 단계에서 다음 단계로 어떻게 바뀌 었는지를 나타냅니다. 증가 할 때마다 +1, 감소 할 때마다 -1, 그렇지 않으면 0으로 나타납니다. 이것은 게임 전반에 걸쳐 보상 신호를 표준화하고 점수 범위가 다양 함에도 불구하고 모든 게임에서 단일 단계 크기 매개 변수가 잘 작동하도록했습니다. DQN은 ε-greedy 정책을 사용했으며, ε은 첫 번째 백만 개 프레임에서 선형 적으로 감소했으며 나머지 학습 세션에서는 낮은 값을 유지했습니다. 학습 단계 크기, 할인율 및 구현과 관련된 기타 다양한 매개 변수의 값은 비공식 검색을 수행하여 작은 값의 게임에 가장 적합한 값을 확인함으로써 선택되었습니다. 이 값들은 모든 게임에서 고정되어있었습니다.

DQN이 액션을 선택한 후, 액션은 게임 에뮬레이터에 의해 실행되었으며, 보상과 다음 비디오 프레임이 반환되었습니다. 프레임이 사전 처리되어 네트워크에 대한 다음 입력이 된 네 개의 프레임 스택에 추가되었습니다. Mnih 등이 작성한 기본적인 Q-learning 절차가 변경됨에 따라 DQN은 네트워크의 가중치를 업데이트하기 위해 Q-learning의 다음과 같은 반 구배 형태를 사용했습니다.

(St, At, wt) ∇wt q (St, At, wt), (16.3) a (wt + 1) = wt + α Rt + 1 +

에이
여기서, wt는 네트워크 웨이트의 벡터이고, At는 시간 단계 t에서 선택된 동작이고, St 및 St + 1은 각각 시간 단계 t 및 t + 1에서 네트워크에 입력되는 사전 처리 된 이미지 스택이다.

(16.3)의 기울기는 역 전파로 계산했습니다. 각 동작에 대해 별도의 네트워크가 있다는 것을 다시 상상해보십시오. 시간 단계 t에서의 업데이트의 경우, Backpropagation은 At에 해당하는 네트워크에만 적용됩니다. Mnih et al. 대규모 네트워크에 적용 할 때 기본 백 프로덕션 알고리즘을 개선하기 위해 표시된 기술을 활용했습니다. 그들은 작은 배치 (32 개 이미지 이후)에 그라디언트 정보를 축적 한 후에 만 ​​가중치를 업데이트하는 미니 배치 기법을 사용했습니다. 이로써 각 동작 후에 가중치를 업데이트하는 일반적인 절차에 비해 더 부드러운 샘플 그라디언트가 생성되었습니다. 또한 RMSProp (Tieleman and Hinton, 2012)라는 그래디언트 - 상승 알고리즘을 사용하여 각 가중치의 단계 크기 매개 변수를 조정하여 학습을 가속화했으며,이 가중치에 대한 최근의 그래디언트 크기의 실행 평균을 기준으로 계산했습니다.

Mnih et al. 세 가지 방법으로 기본 Q- 학습 절차를 수정했습니다. 첫째, 그들은 Lin (1992)이 처음 경험 한 경험 재생이라는 방법을 사용했습니다. 이 방법은 가중치 업데이트를 수행하기 위해 액세스되는 재생 메모리의 각 시간 단계에서 상담원의 경험을 저장합니다. DQN에서 이와 같이 작동했습니다. 게임 에뮬레이터가 이미지 스택 St로 표현 된 상태에서 액션 At를 실행하고 보상 Rt + 1 및 이미지 스택 St + 1을 반환 한 후 재생에 튜플 (St, At, Rt + 1, St + 1)을 추가 한 후 기억. 이 기억은 동일한 게임의 많은 연극에 걸쳐 경험을 쌓았습니다. 각 시간 단계마다 재생 메모리에서 무작위로 샘플링 된 경험을 기반으로 여러 번의 Q- 학습 업데이트 (미니 배치)가 수행되었습니다. Q-learning의 일반적인 형태에서와 같이 St + 1이 다음 업데이트를위한 새로운 St가되는 대신, 다음 업데이트를 위해 데이터를 제공하기 위해 재생 메모리에서 새로운 연결되지 않은 경험이 추출되었습니다. Q-learning은 off-policy 알고리즘이기 때문에 연결된 궤도를 따라 적용 할 필요가 없습니다.

Mnih et al. 안정성을 향상시키기위한 두 번째 방법으로 표준 Q- 학습을 수정했습니다. 부트 스트랩 (bootstrap)과 같은 다른 방법과 마찬가지로 Q- 학습 업데이트의 대상은 현재 동작 값 함수 추정에 따라 달라집니다.

매개 변수화 된 함수 근사법을 사용하여 작업 값을 나타내는 경우 대상은 업데이트되는 것과 동일한 매개 변수의 함수입니다. 예를 들어, (16.3)에 의해 주어진 갱신의 표적은 γ maxa q (St + 1, a, wt)이다. wt에 대한 의존성은 대상이 업데이트되는 매개 변수에 의존하지 않는보다 단순한 감독 학습 상황에 비해 프로세스를 복잡하게 만듭니다. 11 장에서 논의 되었 듯이 이것은 진동과 발산을 야기 할 수있다.

이 문제를 해결하기 위해, Mnih et al. Q- 러닝을 단순한 감독 학습 케이스에 가깝게하면서 동시에 부트 스트랩을 허용하는 기술을 사용했습니다. 행동 값 네트워크의 가중치에 일정한 수의 업데이트가 수행 될 때마다 네트워크의 현재 가중치를 다른 네트워크에 삽입하고 이러한 중복 가중치를 다음 W 개의 업데이트에 대해 고정했습니다. W의 다음 C 갱신에 대한이 중복 네트워크의 출력이 Q 학습 목표로 사용되었습니다. q가이 중복 네트워크의 출력을 나타내는 경우, (16.3) 대신에 업데이트 규칙은 다음과 같습니다.

(St, At, wt) ∇wt q (St, At, wt)에 의해 결정된다. 에이

안정성을 향상시키기 위해 표준 Q-learning의 최종 수정본도 발견되었습니다. 그들은 오차항 Rt + 1 + γ maxa q (St + 1, a, wt) - q (St, At, wt)를 클리핑하여 구간 [-1, 1]에 남아있게 하였다.

Mnih et al. DQN의 다양한 디자인 특징이 성능에 미친 영향에 대한 통찰력을 얻으려는 5 가지 게임에 대해 많은 학습을 실시했습니다. DQN은 경험 재생 및 중복 대상 네트워크가 포함되거나 포함되지 않은 네 가지 조합으로 실행되었습니다. 게임마다 게임마다 결과가 다양했지만 이러한 기능만으로는 성능이 크게 향상되었으며 함께 사용하면 성능이 크게 향상되었습니다. Mnih et al. 또한 DQN의 깊은 컨볼 루션 버전을 단 하나의 선형 레이어 네트워크가있는 버전과 비교하여 동일한 스택 전처리 된 비디오 프레임을 수신함으로써 DQN의 학습 능력에서 깊은 컨볼 루션 ANN이 수행 한 역할을 연구했습니다. 여기서 선형 버전에 비해 깊은 컨볼 루션 버전의 개선은 특히 5 가지 테스트 게임 모두에서 두드러졌습니다.

다양한 도전 과제를 능가하는 인공 에이전트를 만드는 것은 인공 지능의 지속적인 목표였습니다. 이를 달성하기위한 수단으로서의 기계 학습의 약속은 문제 특정 표현을 만들 필요성에 의해 좌절되었습니다. DeepMind의 DQN은 단일 에이전트가 문제 별 기능을 학습하여 다양한 작업에 대해 인간과 경쟁 할 수있는 기술을 습득 할 수 있음을 보여줌으로써 중요한 발전을 의미합니다. 그러나 Mnih et al. DQN은 작업 독립 학습 문제에 대한 완전한 해결책이 아니라는 점을 지적합니다. 아타리 게임에서 탁월한 능력을 발휘하는 데 필요한 기술은 매우 다양했지만 모든 게임은 비디오 이미지를 관찰하여 수행되었으므로 깊은 굴곡 진 ANN이이 작업 컬렉션을위한 자연스러운 선택이되었습니다. 또한 DQN의 일부 Atari 2600 게임 성능은이 게임에서 인간의 기술 수준에 비해 크게 떨어졌습니다. DQN이 가장 어려운 게임들 - 특히 DQN이 임의의 플레이어뿐만 아니라 무언가를 수행하는 것을 배웠던 Montezuma 's Revenge -는 DQN이 설계된 것 이상으로 깊은 계획을 필요로합니다. 또한 DQN과 같은 광범위한 연습을 통해 학습 제어 기술을 습득하는 것은 인간이 일상적으로 성취하는 학습 유형 중 하나 일뿐입니다. 이러한 한계에도 불구하고, DQN은 보강 학습과 현대의 심층 학습 방법을 결합한다는 약속을 인상적으로 시연함으로써 기계 학습의 최첨단을 발전 시켰습니다.


One of the greatest challenges in applying reinforcement learning to real-world problems is deciding how to represent and store value functions and/or policies. Unless the state set is finite and small enough to allow exhaustive representation by a lookup table—as in many of our illustrative examples— one must use a parameterized function approximation scheme. Whether linear or non-linear, function approximation relies on features that have to be readily accessible to the learning system and able to convey the information necessary for skilled performance. Most successful applications of reinforcement learning owe much to sets of features carefully handcrafted based on human knowledge and intuition about the specific problem to be tackled.

A team of researchers at Google DeepMind developed an impressive demonstration that a deep multi- layer artificial neural network (ANN) can automate the feature design process (Mnih et al., 2015, 2013). Multi-layer ANNs have been used for function approximation in reinforcement learning ever since the 1986 popularization of the backpropagation algorithm as a method for learning internal representations (Rumelhart, Hinton, and Williams, 1986; see Section 9.6). Striking results have been obtained by coupling reinforcement learning with backpropagation. The results obtained by Tesauro and colleages with TD-Gammon and Watson discussed above are notable examples. These and other applications benefited from the ability of multi-layer ANNs to learn task-relevant features. However, in all the examples of which we are aware, the most impressive demonstrations required the network’s input to be represented in terms of specialized features handcrafted for the given problem. This is vividly apparent in the TD-Gammon results. TD-Gammon 0.0, whose network input was essentially a “raw” representation of he backgammon board, meaning that it involved very little knowledge of backgammon, learned to play approximately as well as the best previous backgammon computer programs. Adding specialized backgammon features produced TD-Gammon 1.0 which was substantially better than all previous backgammon programs and competed well against human experts.
Mnih et al. developed a reinforcement learning agent called deep Q-network (DQN) that combined Q-learning with a deep convolutional ANN, a many-layered, or deep, ANN specialized for processing spatial arrays of data such as images. We describe deep convolutional ANNs in Section 9.6. By the time of Mnih et al.’s work with DQN, deep ANNs, including deep convolutional ANNs, had produced impressive results in many applications, but they had not been widely used in reinforcement learning.

Mnih et al. used DQN to show how a single reinforcement learning agent can achieve high levels of performance in many different problems without relying on different problem-specific feature sets. To demonstrate this, they let DQN learn to play 49 different Atari 2600 video games by interacting with a game emulator. For learning each game, DQN used the same raw input, the same network architecture, and the same parameter values (e.g., step-size, discount rate, exploration parameters, and many more specific to the implementation). DQN achieved levels of play at or beyond human level on a large fraction of these games. Although the games were alike in being played by watching streams of video images, they varied widely in other respects. Their actions had different effects, they had different state-transition dynamics, and they needed different policies for earning high scores. The deep convolutional ANN learned to transform the raw input common to all the games into features specialized for representing the action values required for playing at the high level DQN achieved for most of the games.

The Atari 2600 is a home video game console that was sold in various versions by Atari Inc. from 1977 to 1992. It introduced or popularized many arcade video games that are now considered classics, such as Pong, Breakout, Space Invaders, and Asteroids. Although much simpler than modern video games, Atari 2600 games are still entertaining and challenging for human players, and they have been attractive as testbeds for developing and evaluating reinforcement learning methods (Diuk, Cohen, Littman, 2008; Naddaf, 2010; Cobo, Zang, Isbell, and Thomaz, 2011; Bellemare, Veness, and Bowling, 2013). Bellemare, Naddaf, Veness, and Bowling (2012) developed the publicly available Arcade Learning Environment (ALE) to encourage and simplify using Atari 2600 games to study learning and planning algorithms.

These previous studies and the availability of ALE made the Atari 2600 game collection a good choice for Mnih et al.’s demonstration, which was also influenced by the impressive human-level performance that TD-Gammon was able to achieve in backgammon. DQN is similar to TD-Gammon in using a multi-layer ANN as the function approximation method for a semi-gradient form of a TD algorithm, with the gradients computed by the backpropagation algorithm. However, instead of using TD(λ) as TD-Gammon did, DQN used the semi-gradient form of Q-learning. TD-Gammon estimated the values of afterstates, which were easily obtained from the rules for making backgammon moves. To use the same algorithm for the Atari games would have required generating the next states for each possible action (which would not have been afterstates in that case). This could have been done by using the game emulator to run single-step simulations for all the possible actions (which ALE makes possible). Or a model of each game’s state-transition function could have been learned and used to predict next states (Oh, Guo, Lee, Lewis, and Singh, 2015). While these methods might have produced results comparable to DQN’s, they would have been more complicated to implement and would have significantly increased the time needed for learning. Another motivation for using Q-learning was that DQN used the experience replay method, described below, which requires an off-policy algorithm. Being model-free and off-policy made Q-learning a natural choice.

Before describing the details of DQN and how the experiments were conducted, we look at the skill levels DQN was able to achieve. Mnih et al. compared the scores of DQN with the scores of the best performing learning system in the literature at the time, the scores of a professional human games tester, and the scores of an agent that selected actions at random. The best system from the literature used linear function approximation with features hand designed using some knowledge about Atari 2600 games (Bellemare, Naddaf, Veness, and Bowling, 2013). DQN learned on each game by interacting with the game emulator for 50 million frames, which corresponds to about 38 days of experience with the game. At the start of learning on each game, the weights of DQN’s network were reset to random values. To evaluate DQN’s skill level after learning, its score was averaged over 30 sessions on each game, each lasting up to 5 minutes and beginning with a random initial game state. The professional human tester played using the same emulator (with the sound turned off to remove any possible advantage over DQN which did not process audio). After 2 hours of practice, the human played about 20 episodes of each game for up to 5 minutes each and was not allowed to take any break during this time. DQN learned to play better than the best previous reinforcement learning systems on all but 6 of the games, and played better than the human player on 22 of the games. By considering any performance that scored at or above 75% of the human score to be comparable to, or better than, human-level play, Mnih et al. concluded that the levels of play DQN learned reached or exceeded human level on 29 of the 46 games. See Mnih et al. (2015) for a more detailed account of these results.

For an artificial learning system to achieve these levels of play would be impressive enough, but what makes these results remarkable—and what many at the time considered to be breakthrough results for artificial intelligence—is that the very same learning system achieved these levels of play on widely varying games without relying on any game-specific modifications.

A human playing any of these 49 Atari games sees 210×160 pixel image frames with 128 colors at 60Hz. In principle, exactly these images could have formed the raw input to DQN, but to reduce memory and processing requirements, Mnih et al. preprocessed each frame to produce an 84×84 array of luminance values. Since the full states of many of the Atari games are not completely observable from the image frames, Mnih et al. “stacked” the four most recent frames so that the inputs to the network had dimension 84×84×4. This did not eliminate partial observability for all of the games, but it was helpful in making many of them more Markovian.

An essential point here is that these preprocessing steps were exactly the same for all 46 games. No game-specific prior knowledge was involved beyond the general understanding that it should still be possible to learn good policies with this reduced dimension and that stacking adjacent frames should help with the partial observability of some of the games. Since no game-specific prior knowledge beyond this minimal amount was used in preprocessing the image frames, we can think of the 84×84×4 input vectors as being “raw” input to DQN.

The basic architecture of DQN is similar to the deep convolutional ANN illustrated in Figure 9.15 (though unlike that network, subsampling in DQN is treated as part of each convolutional layer, with feature maps consisting of units having only a selection of the possible receptive fields). DQN has three hidden convolutional layers, followed by one fully connected hidden layer, followed by the output layer. The three successive hidden convolutional layers of DQN produce 32 20×20 feature maps, 64 9×9 feature maps, and 64 7×7 feature maps. The activation function of the units of each feature map is a rectifier nonlinearity (max(0, x)). The 3,136 (64×7×7) units in this third convolutional layer all connect to each of 512 units in the fully connected hidden layer, which then each connect to all 18 units in the output layer, one for each possible action in an Atari game.

The activation levels of DQN’s output units were the estimated optimal action values (optimal Q- values) of the corresponding state–action pairs, for the state represented by the network’s input. The assignment of output units to a game’s actions varied from game to game, and since the number of valid actions varied between 4 and 18 for the games, not all output units had functional roles in all of the games. It helps to think of the network as if it were 18 separate networks, one for estimating the optimal action value of each possible action. In reality, these networks shared their initial layers, but the output units learned to use the features extracted by these layers in different ways.

DQN’s reward signal indicated how a games’s score changed from one time step to the next: +1 whenever it increased, −1 whenever it decreased, and 0 otherwise. This standardized the reward signal across the games and made a single step-size parameter work well for all the games despite their varying ranges of scores. DQN used an ε-greedy policy, with ε decreasing linearly over the first million frames and remaining at a low value for the rest of the learning session. The values of the various other parameters, such as the learning step-size, discount rate, and others specific to the implementation, were selected by performing informal searches to see which values worked best for a small selection of the games. These values were then held fixed for all of the games.

After DQN selected an action, the action was executed by the game emulator, which returned a reward and the next video frame. The frame was preprocessed and added to the four-frame stack that became the next input to the network. Skipping for the moment the changes to the basic Q-learning procedure made by Mnih et al., DQN used the following semi-gradient form of Q-learning to update the network’s weights:

wt+1 = wt + α Rt+1 + γ max qˆ(St+1, a, wt) − qˆ(St, At, wt) ∇wt qˆ(St, At, wt), (16.3) a

a
where wt is the vector of the network’s weights, At is the action selected at time step t, and St and St+1 are respectively the preprocessed image stacks input to the network at time steps t and t + 1.

The gradient in (16.3) was computed by backpropagation. Imagining again that there was a separate network for each action, for the update at time step t, backpropagation was applied only to the network corresponding to At. Mnih et al. took advantage of techniques shown to improve the basic backpropa- gation algorithm when applied to large networks. They used a mini-batch method that updated weights only after accumulating gradient information over a small batch of images (here after 32 images). This yielded smoother sample gradients compared to the usual procedure that updates weights after each action. They also used a gradient-ascent algorithm called RMSProp (Tieleman and Hinton, 2012) that accelerates learning by adjusting the step-size parameter for each weight based on a running average of the magnitudes of recent gradients for that weight.

Mnih et al. modified the basic Q-learning procedure in three ways. First, they used a method called experience replay first studied by Lin (1992). This method stores the agent’s experience at each time step in a replay memory that is accessed to perform the weight updates. It worked like this in DQN. After the game emulator executed action At in a state represented by the image stack St, and returned reward Rt+1 and image stack St+1, it added the tuple (St, At, Rt+1, St+1) to the replay memory. This memory accumulated experiences over many plays of the same game. At each time step multiple Q- learning updates—a mini-batch—were performed based on experiences sampled uniformly at random from the replay memory. Instead of St+1 becoming the new St for the next update as it would in the usual form of Q-learning, a new unconnected experience was drawn from the replay memory to supply data for the next update. Since Q-learning is an off-policy algorithm, it does not need to be applied along connected trajectories.
Q-learning with experience replay provided several advantages over the usual form of Q-learning. The ability to use each stored experience for many updates allowed DQN to learn more efficiently from its experiences. Experience replay reduced the variance of the updates because successive updates were not correlated with one another as they would be with standard Q-learning. And by removing the dependence of successive experiences on the current weights, experience replay eliminated one source of instability.

Mnih et al. modified standard Q-learning in a second way to improve its stability. As in other methods that bootstrap, the target for a Q-learning update depends on the current action-value function estimate.

When a parameterized function approximation method is used to represent action values, the target is a function of the same parameters that are being updated. For example, the target in the update given by (16.3) is γ maxa qˆ(St+1, a, wt). Its dependence on wt complicates the process compared to the simpler supervised-learning situation in which the targets do not depend on the parameters being updated. As discussed in Chapter 11 this can lead to oscillations and/or divergence.

To address this problem Mnih et al. used a technique that brought Q-learning closer to the simpler supervised-learning case while still allowing it to bootstrap. Whenever a certain number, C, of updates had been done to the weights w of the action-value network, they inserted the network’s current weights into another network and held these duplicate weights fixed for the next C updates of w. The outputs of this duplicate network over the next C updates of w were used as the Q-learning targets. Letting q ̃ denote the output of this duplicate network, then instead of (16.3) the update rule was:

wt+1 = wt + α Rt+1 + γ max q ̃(St+1, a, wt) − qˆ(St, At, wt) ∇wt qˆ(St, At, wt). a

A final modification of standard Q-learning was also found to improve stability. They clipped the error term Rt+1 + γ maxa q ̃(St+1, a, wt) − qˆ(St, At, wt) so that it remained in the interval [−1, 1].

Mnih et al. conducted a large number of learning runs on 5 of the games to gain insight into the effect that various of DQN’s design features had on its performance. They ran DQN with the four combinations of experience replay and the duplicate target network being included or not included. Although the results varied from game to game, each of these features alone significantly improved performance, and very dramatically improved performance when used together. Mnih et al. also studied the role played by the deep convolutional ANN in DQN’s learning ability by comparing the deep convolutional version of DQN with a version having a network of just one linear layer, both receiving the same stacked preprocessed video frames. Here, the improvement of the deep convolutional version over the linear version was particularly striking across all 5 of the test games.

Creating artificial agents that excel over a diverse collection of challenging tasks has been an enduring goal of artificial intelligence. The promise of machine learning as a means for achieving this has been frustrated by the need to craft problem-specific representations. DeepMind’s DQN stands as a major step forward by demonstrating that a single agent can learn problem-specific features enabling it to acquire human-competitive skills over a range of tasks. But as Mnih et al. point out, DQN is not a complete solution to the problem of task-independent learning. Although the skills needed to excel on the Atari games were markedly diverse, all the games were played by observing video images, which made a deep convolutional ANN a natural choice for this collection of tasks. In addition, DQN’s performance on some of the Atari 2600 games fell considerably short of human skill levels on these games. The games most difficult for DQN—especially Montezuma’s Revenge on which DQN learned to perform about as well as the random player—require deep planning beyond what DQN was designed to do. Further, learning control skills through extensive practice, like DQN learned how to play the Atari games, is just one of the types of learning humans routinely accomplish. Despite these limitations, DQN advanced the state-of-the-art in machine learning by impressively demonstrating the promise of combining reinforcement learning with modern methods of deep learning.